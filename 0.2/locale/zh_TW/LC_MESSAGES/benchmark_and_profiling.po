# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2024, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.2\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:47+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../benchmark_and_profiling.md:1
msgid "Benchmark and Profiling"
msgstr ""

#: ../../../benchmark_and_profiling.md:3
msgid "Benchmark"
msgstr ""

#: ../../../benchmark_and_profiling.md:4
msgid ""
"Benchmark a single static batch by running the following command without "
"launching a server. The arguments are the same as for `launch_server.py`. "
"Note that this is not a dynamic batching server, so it may run out of memory "
"for a batch size that a real server can handle. A real server truncates the "
"prefill into several batches, while this unit test does not. For accurate "
"large batch testing, consider using `sglang.bench_serving`."
msgstr ""

#: ../../../benchmark_and_profiling.md:5
msgid ""
"python -m sglang.bench_latency --model-path meta-llama/Meta-Llama-3-8B-"
"Instruct --batch 32 --input-len 256 --output-len 32\n"
msgstr ""

#: ../../../benchmark_and_profiling.md:8
msgid ""
"Benchmark online serving. Launch a server first and run the following "
"command."
msgstr ""

#: ../../../benchmark_and_profiling.md:9
msgid "python3 -m sglang.bench_serving --backend sglang --num-prompt 10\n"
msgstr ""

#: ../../../benchmark_and_profiling.md:13
msgid "Profile with Nsight"
msgstr ""

#: ../../../benchmark_and_profiling.md:14
msgid "Prerequisite"
msgstr ""

#: ../../../benchmark_and_profiling.md:15
msgid ""
"# install nsys\n"
"# https://docs.nvidia.com/nsight-systems/InstallationGuide/index.html\n"
"apt update\n"
"apt install -y --no-install-recommends gnupg\n"
"echo \"deb http://developer.download.nvidia.com/devtools/repos/"
"ubuntu$(source /etc/lsb-release; echo \"$DISTRIB_RELEASE\" | tr -d .)/$(dpkg "
"--print-architecture) /\" | tee /etc/apt/sources.list.d/nvidia-devtools."
"list\n"
"apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/"
"repos/ubuntu1804/x86_64/7fa2af80.pub\n"
"apt update\n"
"apt install nsight-systems-cli\n"
msgstr ""

#: ../../../benchmark_and_profiling.md:26
msgid ""
"To profile a single batch, use `nsys profile --trace-fork-before-exec=true --"
"cuda-graph-trace=node python3 -m sglang.bench_latency --model meta-llama/"
"Meta-Llama-3-8B --batch-size 64 --input-len 512`"
msgstr ""

#: ../../../benchmark_and_profiling.md:28
msgid "To profile a server, e.g."
msgstr ""

#: ../../../benchmark_and_profiling.md:30
msgid ""
"# server\n"
"# set the delay and duration times according to needs\n"
"nsys profile --trace-fork-before-exec=true --cuda-graph-trace=node -o sglang."
"out --delay 60 --duration 70 python3 -m sglang.launch_server --model-path "
"meta-llama/Meta-Llama-3.1-8B-Instruct --disable-radix-cache\n"
"\n"
"# client\n"
"python3 -m sglang.bench_serving --backend sglang --num-prompts 6000 --"
"dataset-name random --random-input 4096 --random-output 2048\n"
msgstr ""

#: ../../../benchmark_and_profiling.md:39
msgid "Use NVTX, e.g."
msgstr ""

#: ../../../benchmark_and_profiling.md:41
msgid ""
"# install nvtx\n"
"pip install nvtx\n"
"\n"
"# code snippets\n"
"import nvtx\n"
"with nvtx.annotate(\"description\", color=\"color\"):\n"
"    # some critical code\n"
msgstr ""
