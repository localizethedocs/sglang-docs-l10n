# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.4\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../references/benchmark_and_profiling.md:1
msgid "Benchmark and Profiling"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:3
msgid "Benchmark"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:5
msgid ""
"Benchmark the latency of running a single static batch without a server. The "
"arguments are the same as for `launch_server.py`. Note that this is a "
"simplified test script without a dynamic batching server, so it may run out "
"of memory for a batch size that a real server can handle. A real server "
"truncates the prefill into several batches, while this simplified script "
"does not."
msgstr ""

#: ../../../references/benchmark_and_profiling.md:7
msgid "Without a server (do not need to launch a server)"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:8
msgid ""
"python -m sglang.bench_one_batch --model-path meta-llama/Meta-Llama-3.1-8B-"
"Instruct --batch 32 --input-len 256 --output-len 32\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:11
msgid ""
"With a server (please use `sglang.launch_server` to launch a server first "
"and run the following command.)"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:12
msgid ""
"python -m sglang.bench_one_batch_server --base-url http://127.0.0.1:30000 --"
"model-path meta-llama/Meta-Llama-3.1-8B-Instruct --batch-size 32 --input-len "
"256 --output-len 32\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:17
msgid ""
"Benchmark offline processing. This script will start an offline engine and "
"run the benchmark."
msgstr ""

#: ../../../references/benchmark_and_profiling.md:19
msgid ""
"python3 -m sglang.bench_offline_throughput --model-path meta-llama/Meta-"
"Llama-3.1-8B-Instruct --num-prompts 10\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:23
msgid ""
"Benchmark online serving. Please use `sglang.launch_server` to launch a "
"server first and run the following command."
msgstr ""

#: ../../../references/benchmark_and_profiling.md:25
msgid "python3 -m sglang.bench_serving --backend sglang --num-prompt 10\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:29
msgid "Profile with PyTorch Profiler"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:31
msgid ""
"[Pytorch Profiler](https://pytorch.org/tutorials/recipes/recipes/"
"profiler_recipe.html) is a convenient basic tool to inspect kernel execution "
"time, call stack, and kernel overlap and occupancy."
msgstr ""

#: ../../../references/benchmark_and_profiling.md:33
msgid "To profile a server"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:35
msgid ""
"# set trace path\n"
"export SGLANG_TORCH_PROFILER_DIR=/root/sglang/profile_log\n"
"\n"
"# start server\n"
"python -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-"
"Instruct\n"
"\n"
"# send profiling request from client\n"
"python -m sglang.bench_serving --backend sglang --model meta-llama/"
"Llama-3.1-8B-Instruct --num-prompts 10 --sharegpt-output-len 100 --profile\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:46
msgid ""
"Please make sure that the `SGLANG_TORCH_PROFILER_DIR` should be set at both "
"server and client side, otherwise the trace file cannot be generated "
"correctly . A secure way will be setting `SGLANG_TORCH_PROFILER_DIR` in the "
"`.*rc` file of shell (e.g. `~/.bashrc` for bash shells)."
msgstr ""

#: ../../../references/benchmark_and_profiling.md:48
msgid "To profile offline"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:49
msgid ""
"export SGLANG_TORCH_PROFILER_DIR=/root/sglang/profile_log\n"
"\n"
"# profile one batch with bench_one_batch.py\n"
"# batch size can be controlled with --batch argument\n"
"python3 -m sglang.bench_one_batch --model-path meta-llama/Llama-3.1-8B-"
"Instruct --batch 32 --input-len 1024 --output-len 10 --profile\n"
"\n"
"# profile multiple batches with bench_offline_throughput.py\n"
"python -m sglang.bench_offline_throughput --model-path meta-llama/"
"Llama-3.1-8B-Instruct --dataset-name random --num-prompts 10 --profile --mem-"
"frac=0.8\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:60
msgid ""
"Possible PyTorch Bug If in any cases you encounter the following error (for "
"example, using qwen 2.5 VL):"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:62
msgid ""
"RuntimeError: !stack.empty() INTERNAL ASSERT FAILED at \"/pytorch/torch/csrc/"
"autograd/profiler_python.cpp\":983, please report a bug to PyTorch. Python "
"replay stack is empty.\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:65
msgid ""
"This is likely a PyTorch Bug reported in [Bug: vLLM Profiler](https://github."
"com/vllm-project/vllm/issues/18240) and [Bug: torch.profiler.profile]"
"(https://github.com/pytorch/pytorch/issues/101632). As a workaround, you may "
"disable `with_stack` with an environment variable such as follows:"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:66
msgid ""
"export SGLANG_PROFILE_WITH_STACK=False\n"
"python -m sglang.bench_offline_throughput --model-path meta-llama/"
"Llama-3.1-8B-Instruct --dataset-name random --num-prompts 10 --profile --mem-"
"frac=0.8\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:71
msgid "View Traces"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:73
msgid "Trace files can be loaded and visualized from:"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:75
msgid "https://ui.perfetto.dev/ (any browser)"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:76
msgid "chrome://tracing (Chrome browser only)"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:78
msgid ""
"If browser cannot open trace file due to its large size, client can generate "
"a small trace file (<100MB) by controlling number of prompts and lengths of "
"prompt outputs. For example, when profiling a server,"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:82
msgid ""
"python -m sglang.bench_serving --backend sglang --model meta-llama/"
"Llama-3.1-8B-Instruct --num-prompts 2 --sharegpt-output-len 100 --profile\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:86
msgid ""
"This command sets the number of prompts to 2 with `--num-prompts` argument "
"and limits the length of output sequences to 100 with `--sharegpt-output-"
"len` argument, which can generate a small trace file for browser to open "
"smoothly."
msgstr ""

#: ../../../references/benchmark_and_profiling.md:88
msgid ""
"Additionally, if you want to locate the SGLang Python source code through "
"the cuda kernel in Trace, you need to disable CUDA Graph when starting the "
"service. This can be done by using the `--disable-cuda-graph` parameter in "
"the command to start the service."
msgstr ""

#: ../../../references/benchmark_and_profiling.md:90
msgid "Profile with Nsight"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:92
msgid ""
"[Nsight systems](https://docs.nvidia.com/nsight-systems/) is an advanced "
"tool that exposes more profiling details, such as register and shared memory "
"usage, annotated code regions and low-level CUDA APIs and events."
msgstr ""

#: ../../../references/benchmark_and_profiling.md:94
msgid "Prerequisite:"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:96
msgid ""
"Install using apt, or run inside a [NVIDIA Docker container](https://catalog."
"ngc.nvidia.com/orgs/nvidia/containers/pytorch/tags) or [SGLang Docker "
"container](https://github.com/sgl-project/sglang/tree/main/docker)."
msgstr ""

#: ../../../references/benchmark_and_profiling.md:98
msgid ""
"# install nsys\n"
"# https://docs.nvidia.com/nsight-systems/InstallationGuide/index.html\n"
"apt update\n"
"apt install -y --no-install-recommends gnupg\n"
"echo \"deb http://developer.download.nvidia.com/devtools/repos/"
"ubuntu$(source /etc/lsb-release; echo \"$DISTRIB_RELEASE\" | tr -d .)/$(dpkg "
"--print-architecture) /\" | tee /etc/apt/sources.list.d/nvidia-devtools."
"list\n"
"apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/"
"repos/ubuntu1804/x86_64/7fa2af80.pub\n"
"apt update\n"
"apt install nsight-systems-cli\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:109
msgid "To profile a single batch, use"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:111
msgid ""
"nsys profile --trace-fork-before-exec=true --cuda-graph-trace=node python3 -"
"m sglang.bench_one_batch --model meta-llama/Meta-Llama-3-8B --batch-size 64 "
"--input-len 512\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:115
msgid "To profile a server, e.g."
msgstr ""

#: ../../../references/benchmark_and_profiling.md:117
msgid ""
"# launch the server, set the delay and duration times according to needs\n"
"# after the duration time has been used up, server will be killed by nsys\n"
"\n"
"nsys profile --trace-fork-before-exec=true --cuda-graph-trace=node -o sglang."
"out --delay 60 --duration 70 python3 -m sglang.launch_server --model-path "
"meta-llama/Llama-3.1-8B-Instruct --disable-radix-cache\n"
"\n"
"# client\n"
"python3 -m sglang.bench_serving --backend sglang --num-prompts 1000 --"
"dataset-name random --random-input 1024 --random-output 512\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:127
msgid ""
"In practice, we recommend users to set `--duration` argument to a large "
"value. Whenever user wants the server to stop profiling. Firstly run:"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:129
msgid "nsys sessions list\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:133
msgid "to get the session id in the form of `profile-XXXXX`, then run:"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:135
msgid "nsys stop --session=profile-XXXXX\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:139
msgid "to manually kill the profiler and generate `nsys-rep` files instantly."
msgstr ""

#: ../../../references/benchmark_and_profiling.md:141
msgid "Use NVTX to annotate code regions, e.g. to see their execution time."
msgstr ""

#: ../../../references/benchmark_and_profiling.md:143
msgid ""
"# install nvtx\n"
"pip install nvtx\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:148
msgid ""
"# code snippets\n"
"import nvtx\n"
"with nvtx.annotate(\"description\", color=\"color\"):\n"
"    # some critical code\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:155
msgid "Other tips"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:157
msgid ""
"You can benchmark a model using dummy weights by only providing the config."
"json file. This allows for quick testing of model variants without training. "
"To do so, add `--load-format dummy` to the above commands and then you only "
"need a correct `config.json` under the checkpoint folder."
msgstr ""

#: ../../../references/benchmark_and_profiling.md:158
msgid ""
"You can benchmark a model with modified configs (e.g., less layers) by using "
"`--json-model-override-args`. For example, you can benchmark a model with "
"only 2 layers and 2 kv heads using:"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:160
msgid ""
"python -m sglang.bench_one_batch --model-path meta-llama/Meta-Llama-3.1-8B-"
"Instruct --batch 32 --input-len 256 --output-len 32 --load-format dummy --"
"json-model-override-args '{\"num_hidden_layers\": 1, "
"\"num_key_value_heads\": 1}'\n"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:164
msgid ""
"You can use `--python-backtrace=cuda` to see python call stack for all CUDA "
"kernels, as in PyTorch Profiler. (Caveat: this can cause inaccurately long "
"kernel runtimes for CUDA event based timing)"
msgstr ""

#: ../../../references/benchmark_and_profiling.md:165
msgid ""
"For more arguments see [Nsight Systems User Guide](https://docs.nvidia.com/"
"nsight-systems/UserGuide/index.html)."
msgstr ""
