# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.4\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../backend/sampling_params.md:1
msgid "Sampling Parameters"
msgstr ""

#: ../../../backend/sampling_params.md:3
msgid ""
"This doc describes the sampling parameters of the SGLang Runtime. It is the "
"low-level endpoint of the runtime."
msgstr ""

#: ../../../backend/sampling_params.md:5
msgid ""
"If you want a high-level endpoint that can automatically handle chat "
"templates, consider using the [OpenAI Compatible API](./"
"openai_api_completions.ipynb)."
msgstr ""

#: ../../../backend/sampling_params.md:7
msgid "`/generate` Endpoint"
msgstr ""

#: ../../../backend/sampling_params.md:9
msgid ""
"The `/generate` endpoint accepts the following parameters in JSON format. "
"For detailed usage, see the [native API doc](./native_api.ipynb). The object "
"is defined at `io_struct.py::GenerateReqInput`. You can also read the source "
"code to find more arguments and docs."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "Argument"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "Type/Default"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "Description"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "text"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`Optional[Union[List[str], str]] = None`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "The input prompt. Can be a single prompt or a batch of prompts."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "input_ids"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`Optional[Union[List[List[int]], List[int]]] = None`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "The token IDs for text; one can specify either text or input_ids."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "input_embeds"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`Optional[Union[List[List[List[float]]], List[List[float]]]] = None`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"The embeddings for input_ids; one can specify either text, input_ids, or "
"input_embeds."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "image_data"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"`Optional[Union[List[List[ImageDataItem]], List[ImageDataItem], "
"ImageDataItem]] = None`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"The image input. Can be an image instance, file name, URL, or base64 encoded "
"string. Can be a single image, list of images, or list of lists of images."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "audio_data"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`Optional[Union[List[AudioDataItem], AudioDataItem]] = None`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "The audio input. Can be a file name, URL, or base64 encoded string."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "sampling_params"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`Optional[Union[List[Dict], Dict]] = None`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "The sampling parameters as described in the sections below."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "rid"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "The request ID."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "return_logprob"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`Optional[Union[List[bool], bool]] = None`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "Whether to return log probabilities for tokens."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "logprob_start_len"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`Optional[Union[List[int], int]] = None`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"If return_logprob, the start location in the prompt for returning logprobs. "
"Default is \"-1\", which returns logprobs for output tokens only."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "top_logprobs_num"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"If return_logprob, the number of top logprobs to return at each position."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "token_ids_logprob"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "If return_logprob, the token IDs to return logprob for."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "return_text_in_logprobs"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`bool = False`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "Whether to detokenize tokens in text in the returned logprobs."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "stream"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "Whether to stream output."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "lora_path"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`Optional[Union[List[Optional[str]], Optional[str]]] = None`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "The path to the LoRA."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "custom_logit_processor"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`Optional[Union[List[Optional[str]], str]] = None`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"Custom logit processor for advanced sampling control. Must be a serialized "
"instance of `CustomLogitProcessor` using its `to_str()` method. For usage "
"see below."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "return_hidden_states"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`Union[List[bool], bool] = False`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "Whether to return hidden states."
msgstr ""

#: ../../../backend/sampling_params.md:30
msgid "Sampling parameters"
msgstr ""

#: ../../../backend/sampling_params.md:32
msgid ""
"The object is defined at `sampling_params.py::SamplingParams`. You can also "
"read the source code to find more arguments and docs."
msgstr ""

#: ../../../backend/sampling_params.md:34
msgid "Core parameters"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "max_new_tokens"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`int = 128`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "The maximum output length measured in tokens."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "stop"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`Optional[Union[str, List[str]]] = None`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"One or multiple [stop words](https://platform.openai.com/docs/api-reference/"
"chat/create#chat-create-stop). Generation will stop if one of these words is "
"sampled."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "stop_token_ids"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`Optional[List[int]] = None`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"Provide stop words in the form of token IDs. Generation will stop if one of "
"these token IDs is sampled."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "temperature"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`float = 1.0`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"[Temperature](https://platform.openai.com/docs/api-reference/chat/"
"create#chat-create-temperature) when sampling the next token. `temperature = "
"0` corresponds to greedy sampling, a higher temperature leads to more "
"diversity."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "top_p"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"[Top-p](https://platform.openai.com/docs/api-reference/chat/create#chat-"
"create-top_p) selects tokens from the smallest sorted set whose cumulative "
"probability exceeds `top_p`. When `top_p = 1`, this reduces to unrestricted "
"sampling from all tokens."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "top_k"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`int = -1`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"[Top-k](https://developer.nvidia.com/blog/how-to-get-better-outputs-from-"
"your-large-language-model/#predictability_vs_creativity) randomly selects "
"from the `k` highest-probability tokens."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "min_p"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`float = 0.0`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"[Min-p](https://github.com/huggingface/transformers/issues/27670) samples "
"from tokens with probability larger than `min_p * highest_token_probability`."
msgstr ""

#: ../../../backend/sampling_params.md:46
msgid "Penalizers"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "frequency_penalty"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"Penalizes tokens based on their frequency in generation so far. Must be "
"between `-2` and `2` where negative numbers encourage repeatment of tokens "
"and positive number encourages sampling of new tokens. The scaling of "
"penalization grows linearly with each appearance of a token."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "presence_penalty"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"Penalizes tokens if they appeared in the generation so far. Must be between "
"`-2` and `2` where negative numbers encourage repeatment of tokens and "
"positive number encourages sampling of new tokens. The scaling of the "
"penalization is constant if a token occurred."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "min_new_tokens"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`int = 0`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"Forces the model to generate at least `min_new_tokens` until a stop word or "
"EOS token is sampled. Note that this might lead to unintended behavior, for "
"example, if the distribution is highly skewed towards these tokens."
msgstr ""

#: ../../../backend/sampling_params.md:54
msgid "Constrained decoding"
msgstr ""

#: ../../../backend/sampling_params.md:56
msgid ""
"Please refer to our dedicated guide on [constrained decoding](./"
"structured_outputs.ipynb) for the following parameters."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "json_schema"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`Optional[str] = None`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "JSON schema for structured outputs."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "regex"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "Regex for structured outputs."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "ebnf"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "EBNF for structured outputs."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "structural_tag"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "The structal tag for structured outputs."
msgstr ""

#: ../../../backend/sampling_params.md:65
msgid "Other options"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "n"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`int = 1`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"Specifies the number of output sequences to generate per request. "
"(Generating multiple outputs in one request (n > 1) is discouraged; "
"repeating the same prompts several times offers better control and "
"efficiency.)"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "ignore_eos"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "Don't stop generation when EOS token is sampled."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "skip_special_tokens"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`bool = True`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "Remove special tokens during decoding."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "spaces_between_special_tokens"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid ""
"Whether or not to add spaces between special tokens during detokenization."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "no_stop_trim"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "Don't trim stop words or EOS token from the generated text."
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "custom_params"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "`Optional[List[Optional[Dict[str, Any]]]] = None`"
msgstr ""

#: ../../../backend/sampling_params.md:0
msgid "Used when employing `CustomLogitProcessor`. For usage, see below."
msgstr ""

#: ../../../backend/sampling_params.md:76
msgid "Examples"
msgstr ""

#: ../../../backend/sampling_params.md:78
msgid "Normal"
msgstr ""

#: ../../../backend/sampling_params.md:80
#: ../../../backend/sampling_params.md:143
msgid "Launch a server:"
msgstr ""

#: ../../../backend/sampling_params.md:82
msgid ""
"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-"
"Instruct --port 30000\n"
msgstr ""

#: ../../../backend/sampling_params.md:86
#: ../../../backend/sampling_params.md:155
#: ../../../backend/sampling_params.md:288
msgid "Send a request:"
msgstr ""

#: ../../../backend/sampling_params.md:88
msgid ""
"import requests\n"
"\n"
"response = requests.post(\n"
"    \"http://localhost:30000/generate\",\n"
"    json={\n"
"        \"text\": \"The capital of France is\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 32,\n"
"        },\n"
"    },\n"
")\n"
"print(response.json())\n"
msgstr ""

#: ../../../backend/sampling_params.md:104
msgid "Detailed example in [send request](./send_request.ipynb)."
msgstr ""

#: ../../../backend/sampling_params.md:106
msgid "Streaming"
msgstr ""

#: ../../../backend/sampling_params.md:108
msgid "Send a request and stream the output:"
msgstr ""

#: ../../../backend/sampling_params.md:110
msgid ""
"import requests, json\n"
"\n"
"response = requests.post(\n"
"    \"http://localhost:30000/generate\",\n"
"    json={\n"
"        \"text\": \"The capital of France is\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 32,\n"
"        },\n"
"        \"stream\": True,\n"
"    },\n"
"    stream=True,\n"
")\n"
"\n"
"prev = 0\n"
"for chunk in response.iter_lines(decode_unicode=False):\n"
"    chunk = chunk.decode(\"utf-8\")\n"
"    if chunk and chunk.startswith(\"data:\"):\n"
"        if chunk == \"data: [DONE]\":\n"
"            break\n"
"        data = json.loads(chunk[5:].strip(\"\\n\"))\n"
"        output = data[\"text\"].strip()\n"
"        print(output[prev:], end=\"\", flush=True)\n"
"        prev = len(output)\n"
"print(\"\")\n"
msgstr ""

#: ../../../backend/sampling_params.md:139
msgid ""
"Detailed example in [openai compatible api](https://docs.sglang.ai/backend/"
"openai_api_completions.html#id2)."
msgstr ""

#: ../../../backend/sampling_params.md:141
msgid "Multimodal"
msgstr ""

#: ../../../backend/sampling_params.md:145
msgid ""
"python3 -m sglang.launch_server --model-path lmms-lab/llava-onevision-"
"qwen2-7b-ov\n"
msgstr ""

#: ../../../backend/sampling_params.md:149
msgid "Download an image:"
msgstr ""

#: ../../../backend/sampling_params.md:151
msgid ""
"curl -o example_image.png -L https://github.com/sgl-project/sglang/blob/main/"
"test/lang/example_image.png?raw=true\n"
msgstr ""

#: ../../../backend/sampling_params.md:157
msgid ""
"import requests\n"
"\n"
"response = requests.post(\n"
"    \"http://localhost:30000/generate\",\n"
"    json={\n"
"        \"text\": \"<|im_start|>system\\nYou are a helpful assistant.<|"
"im_end|>\\n\"\n"
"                \"<|im_start|>user\\n<image>\\nDescribe this image in a very "
"short sentence.<|im_end|>\\n\"\n"
"                \"<|im_start|>assistant\\n\",\n"
"        \"image_data\": \"example_image.png\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 32,\n"
"        },\n"
"    },\n"
")\n"
"print(response.json())\n"
msgstr ""

#: ../../../backend/sampling_params.md:176
msgid ""
"The `image_data` can be a file name, a URL, or a base64 encoded string. See "
"also `python/sglang/srt/utils.py:load_image`."
msgstr ""

#: ../../../backend/sampling_params.md:178
msgid "Streaming is supported in a similar manner as [above](#streaming)."
msgstr ""

#: ../../../backend/sampling_params.md:180
msgid "Detailed example in [openai api vision](./openai_api_vision.ipynb)."
msgstr ""

#: ../../../backend/sampling_params.md:182
msgid "Structured Outputs (JSON, Regex, EBNF)"
msgstr ""

#: ../../../backend/sampling_params.md:184
msgid ""
"You can specify a JSON schema, regular expression or [EBNF](https://en."
"wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form) to constrain the model "
"output. The model output will be guaranteed to follow the given constraints. "
"Only one constraint parameter (`json_schema`, `regex`, or `ebnf`) can be "
"specified for a request."
msgstr ""

#: ../../../backend/sampling_params.md:186
msgid "SGLang supports two grammar backends:"
msgstr ""

#: ../../../backend/sampling_params.md:188
msgid ""
"[Outlines](https://github.com/dottxt-ai/outlines): Supports JSON schema and "
"regular expression constraints."
msgstr ""

#: ../../../backend/sampling_params.md:189
msgid ""
"[XGrammar](https://github.com/mlc-ai/xgrammar) (default): Supports JSON "
"schema, regular expression, and EBNF constraints."
msgstr ""

#: ../../../backend/sampling_params.md:190
msgid ""
"XGrammar currently uses the [GGML BNF format](https://github.com/ggerganov/"
"llama.cpp/blob/master/grammars/README.md)."
msgstr ""

#: ../../../backend/sampling_params.md:192
msgid ""
"If instead you want to initialize the Outlines backend, you can use `--"
"grammar-backend outlines` flag:"
msgstr ""

#: ../../../backend/sampling_params.md:194
msgid ""
"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-"
"Instruct \\\n"
"--port 30000 --host 0.0.0.0 --grammar-backend [xgrammar|outlines] # xgrammar "
"or outlines (default: xgrammar)\n"
msgstr ""

#: ../../../backend/sampling_params.md:199
msgid ""
"import json\n"
"import requests\n"
"\n"
"json_schema = json.dumps({\n"
"    \"type\": \"object\",\n"
"    \"properties\": {\n"
"        \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n"
"        \"population\": {\"type\": \"integer\"},\n"
"    },\n"
"    \"required\": [\"name\", \"population\"],\n"
"})\n"
"\n"
"# JSON (works with both Outlines and XGrammar)\n"
"response = requests.post(\n"
"    \"http://localhost:30000/generate\",\n"
"    json={\n"
"        \"text\": \"Here is the information of the capital of France in the "
"JSON format.\\n\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 64,\n"
"            \"json_schema\": json_schema,\n"
"        },\n"
"    },\n"
")\n"
"print(response.json())\n"
"\n"
"# Regular expression (Outlines backend only)\n"
"response = requests.post(\n"
"    \"http://localhost:30000/generate\",\n"
"    json={\n"
"        \"text\": \"Paris is the capital of\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 64,\n"
"            \"regex\": \"(France|England)\",\n"
"        },\n"
"    },\n"
")\n"
"print(response.json())\n"
"\n"
"# EBNF (XGrammar backend only)\n"
"response = requests.post(\n"
"    \"http://localhost:30000/generate\",\n"
"    json={\n"
"        \"text\": \"Write a greeting.\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 64,\n"
"            \"ebnf\": 'root ::= \"Hello\" | \"Hi\" | \"Hey\"',\n"
"        },\n"
"    },\n"
")\n"
"print(response.json())\n"
msgstr ""

#: ../../../backend/sampling_params.md:255
msgid "Detailed example in [structured outputs](./structured_outputs.ipynb)."
msgstr ""

#: ../../../backend/sampling_params.md:257
msgid "Custom logit processor"
msgstr ""

#: ../../../backend/sampling_params.md:259
msgid "Launch a server with `--enable-custom-logit-processor` flag on."
msgstr ""

#: ../../../backend/sampling_params.md:261
msgid ""
"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-"
"Instruct --port 30000 --enable-custom-logit-processor\n"
msgstr ""

#: ../../../backend/sampling_params.md:265
msgid ""
"Define a custom logit processor that will always sample a specific token id."
msgstr ""

#: ../../../backend/sampling_params.md:267
msgid ""
"from sglang.srt.sampling.custom_logit_processor import CustomLogitProcessor\n"
"\n"
"class DeterministicLogitProcessor(CustomLogitProcessor):\n"
"    \"\"\"A dummy logit processor that changes the logits to always\n"
"    sample the given token id.\n"
"    \"\"\"\n"
"\n"
"    def __call__(self, logits, custom_param_list):\n"
"        # Check that the number of logits matches the number of custom "
"parameters\n"
"        assert logits.shape[0] == len(custom_param_list)\n"
"        key = \"token_id\"\n"
"\n"
"        for i, param_dict in enumerate(custom_param_list):\n"
"            # Mask all other tokens\n"
"            logits[i, :] = -float(\"inf\")\n"
"            # Assign highest probability to the specified token\n"
"            logits[i, param_dict[key]] = 0.0\n"
"        return logits\n"
msgstr ""

#: ../../../backend/sampling_params.md:290
msgid ""
"import requests\n"
"\n"
"response = requests.post(\n"
"    \"http://localhost:30000/generate\",\n"
"    json={\n"
"        \"text\": \"The capital of France is\",\n"
"        \"custom_logit_processor\": DeterministicLogitProcessor().to_str(),\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0.0,\n"
"            \"max_new_tokens\": 32,\n"
"            \"custom_params\": {\"token_id\": 5},\n"
"        },\n"
"    },\n"
")\n"
"print(response.json())\n"
msgstr ""
