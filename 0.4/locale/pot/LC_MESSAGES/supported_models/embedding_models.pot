# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.4\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../supported_models/embedding_models.md:1
msgid "Embedding Models"
msgstr ""

#: ../../../supported_models/embedding_models.md:3
msgid ""
"SGLang provides robust support for embedding models by integrating efficient "
"serving mechanisms with its flexible programming interface. This integration "
"allows for streamlined handling of embedding tasks, facilitating faster and "
"more accurate retrieval and semantic search operations. SGLang's "
"architecture enables better resource utilization and reduced latency in "
"embedding model deployment."
msgstr ""

#: ../../../supported_models/embedding_models.md:6
msgid ""
"They are executed with `--is-embedding` and some may require `--trust-remote-"
"code`"
msgstr ""

#: ../../../supported_models/embedding_models.md:9
msgid "Example Launch Command"
msgstr ""

#: ../../../supported_models/embedding_models.md:11
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model-path Alibaba-NLP/gme-Qwen2-VL-2B-Instruct \\\n"
"  --is-embedding \\\n"
"  --host 0.0.0.0 \\\n"
"  --chat-template gme-qwen2-vl \\\n"
"  --port 30000\n"
msgstr ""

#: ../../../supported_models/embedding_models.md:19
msgid "Example Client Request"
msgstr ""

#: ../../../supported_models/embedding_models.md:20
msgid ""
"import requests\n"
"\n"
"url = \"http://127.0.0.1:30000\"\n"
"\n"
"text_input = \"Represent this image in embedding space.\"\n"
"image_path = \"https://huggingface.co/datasets/liuhaotian/llava-bench-in-the-"
"wild/resolve/main/images/023.jpg\"\n"
"\n"
"payload = {\n"
"    \"model\": \"gme-qwen2-vl\",\n"
"    \"input\": [\n"
"        {\n"
"            \"text\": text_input\n"
"        },\n"
"        {\n"
"            \"image\": image_path\n"
"        }\n"
"    ],\n"
"}\n"
"\n"
"response = requests.post(url + \"/v1/embeddings\", json=payload).json()\n"
"\n"
"print(\"Embeddings:\", [x.get(\"embedding\") for x in response.get(\"data\", "
"[])])\n"
msgstr ""

#: ../../../supported_models/embedding_models.md:46
msgid "Supported models"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "Model Family (Embedding)"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "Example HuggingFace Identifier"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "Chat Template"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "Description"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "**Llama/Mistral based (E5EmbeddingModel)**"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "`intfloat/e5-mistral-7b-instruct`"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "N/A"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid ""
"Mistral/Llama-based embedding model fine‑tuned for high‑quality text "
"embeddings (top‑ranked on the MTEB benchmark)."
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "**GTE (QwenEmbeddingModel)**"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "`Alibaba-NLP/gte-Qwen2-7B-instruct`"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid ""
"Alibaba’s general text embedding model (7B), achieving state‑of‑the‑art "
"multilingual performance in English and Chinese."
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "**GME (MultimodalEmbedModel)**"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "`Alibaba-NLP/gme-Qwen2-VL-2B-Instruct`"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "`gme-qwen2-vl`"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid ""
"Multimodal embedding model (2B) based on Qwen2‑VL, encoding image + text "
"into a unified vector space for cross‑modal retrieval."
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "**CLIP (CLIPEmbeddingModel)**"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "`openai/clip-vit-large-patch14-336`"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid ""
"OpenAI’s CLIP model (ViT‑L/14) for embedding images (and text) into a joint "
"latent space; widely used for image similarity search."
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "**BGE (BgeEmbeddingModel)**"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid "`BAAI/bge-large-en-v1.5`"
msgstr ""

#: ../../../supported_models/embedding_models.md:5
msgid ""
"Currently only support `attention-backend`   `triton` and `torch_native`. "
"BAAI's BGE embedding models optimized for retrieval and reranking tasks."
msgstr ""
