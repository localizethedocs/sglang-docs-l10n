# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.4\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../supported_models/multimodal_language_models.md:1
msgid "Multimodal Language Models"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:3
msgid ""
"These models accept multi-modal inputs (e.g., images and text) and generate "
"text output. They augment language models with multimodal encoders."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:5
msgid "Example launch Command"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:7
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model-path meta-llama/Llama-3.2-11B-Vision-Instruct \\  # example HF/"
"local path\n"
"  --host 0.0.0.0 \\\n"
"  --port 30000 \\\n"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:14
msgid "Supported models"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:16
msgid "Below the supported models are summarized in a table."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:18
msgid ""
"If you are unsure if a specific architecture is implemented, you can search "
"for it via GitHub. For example, to search for "
"`Qwen2_5_VLForConditionalGeneration`, use the expression:"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:20
msgid ""
"repo:sgl-project/sglang path:/^python\\/sglang\\/srt\\/models\\// "
"Qwen2_5_VLForConditionalGeneration\n"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:24
msgid "in the GitHub search bar."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "Model Family (Variants)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "Example HuggingFace Identifier"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "Chat Template"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "Description"
msgstr "描述"

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**Qwen-VL** (Qwen2 series)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`Qwen/Qwen2.5-VL-7B-Instruct`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`qwen2-vl`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Alibaba’s vision-language extension of Qwen; for example, Qwen2.5-VL (7B and "
"larger variants) can analyze and converse about image content."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**DeepSeek-VL2**"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`deepseek-ai/deepseek-vl2`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`deepseek-vl2`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Vision-language variant of DeepSeek (with a dedicated image processor), "
"enabling advanced multimodal reasoning on image and text inputs."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**Janus-Pro** (1B, 7B)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`deepseek-ai/Janus-Pro-7B`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`janus-pro`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"DeepSeek’s open-source multimodal model capable of both image understanding "
"and generation. Janus-Pro employs a decoupled architecture for separate "
"visual encoding paths, enhancing performance in both tasks."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**MiniCPM-V / MiniCPM-o**"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`openbmb/MiniCPM-V-2_6`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`minicpmv`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"MiniCPM-V (2.6, ~8B) supports image inputs, and MiniCPM-o adds audio/video; "
"these multimodal LLMs are optimized for end-side deployment on mobile/edge "
"devices."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**Llama 3.2 Vision** (11B)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`meta-llama/Llama-3.2-11B-Vision-Instruct`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`llama_3_vision`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Vision-enabled variant of Llama 3 (11B) that accepts image inputs for visual "
"question answering and other multimodal tasks."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**LLaVA** (v1.5 & v1.6)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "*e.g.* `liuhaotian/llava-v1.5-13b`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`vicuna_v1.1`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Open vision-chat models that add an image encoder to LLaMA/Vicuna (e.g. "
"LLaMA2 13B) for following multimodal instruction prompts."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**LLaVA-NeXT** (8B, 72B)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`lmms-lab/llava-next-72b`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`chatml-llava`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Improved LLaVA models (with an 8B Llama3 version and a 72B version) offering "
"enhanced visual instruction-following and accuracy on multimodal benchmarks."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**LLaVA-OneVision**"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`lmms-lab/llava-onevision-qwen2-7b-ov`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Enhanced LLaVA variant integrating Qwen as the backbone; supports multiple "
"images (and even video frames) as inputs via an OpenAI Vision API-compatible "
"format."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**Gemma 3 (Multimodal)**"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`google/gemma-3-4b-it`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`gemma-it`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Gemma 3's larger models (4B, 12B, 27B) accept images (each image encoded as "
"256 tokens) alongside text in a combined 128K-token context."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**Kimi-VL** (A3B)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`moonshotai/Kimi-VL-A3B-Instruct`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`kimi-vl`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Kimi-VL is a multimodal model that can understand and generate text from "
"images."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**Mistral-Small-3.1-24B**"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`mistralai/Mistral-Small-3.1-24B-Instruct-2503`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`mistral`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Mistral 3.1 is a multimodal model that can generate text from text or images "
"input. It also supports tool calling and structured output."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**Phi-4-multimodal-instruct**"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`microsoft/Phi-4-multimodal-instruct`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`phi-4-mm`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Phi-4-multimodal-instruct is the multimodal variant of the Phi-4-mini model, "
"enhanced with LoRA for improved multimodal capabilities. It supports text, "
"vision and audio modalities in SGLang."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**MiMo-VL** (7B)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`XiaomiMiMo/MiMo-VL-7B-RL`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`mimo-vl`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Xiaomi's compact yet powerful vision-language model featuring a native "
"resolution ViT encoder for fine-grained visual details, an MLP projector for "
"cross-modal alignment, and the MiMo-7B language model optimized for complex "
"reasoning tasks."
msgstr ""
