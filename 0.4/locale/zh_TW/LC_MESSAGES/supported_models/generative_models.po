# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.4\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../supported_models/generative_models.md:1
msgid "Large Language Models"
msgstr ""

#: ../../../supported_models/generative_models.md:3
msgid ""
"These models accept text input and produce text output (e.g., chat "
"completions). They are primarily large language models (LLMs), some with "
"mixture-of-experts (MoE) architectures for scaling."
msgstr ""

#: ../../../supported_models/generative_models.md:5
msgid "Example launch Command"
msgstr ""

#: ../../../supported_models/generative_models.md:7
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model-path meta-llama/Llama-3.2-1B-Instruct \\  # example HF/local path\n"
"  --host 0.0.0.0 \\\n"
"  --port 30000 \\\n"
msgstr ""

#: ../../../supported_models/generative_models.md:14
msgid "Supported models"
msgstr ""

#: ../../../supported_models/generative_models.md:16
msgid "Below the supported models are summarized in a table."
msgstr ""

#: ../../../supported_models/generative_models.md:18
msgid ""
"If you are unsure if a specific architecture is implemented, you can search "
"for it via GitHub. For example, to search for `Qwen3ForCausalLM`, use the "
"expression:"
msgstr ""

#: ../../../supported_models/generative_models.md:20
msgid ""
"repo:sgl-project/sglang path:/^python\\/sglang\\/srt\\/models\\// "
"Qwen3ForCausalLM\n"
msgstr ""

#: ../../../supported_models/generative_models.md:24
msgid "in the GitHub search bar."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "Model Family (Variants)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "Example HuggingFace Identifier"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "Description"
msgstr "描述"

#: ../../../supported_models/generative_models.md:0
msgid "**DeepSeek** (v1, v2, v3/R1)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`deepseek-ai/DeepSeek-R1`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Series of advanced reasoning-optimized models (including a 671B MoE) trained "
"with reinforcement learning; top performance on complex reasoning, math, and "
"code tasks. [SGLang provides Deepseek v3/R1 model-specific optimizations]"
"(https://docs.sglang.ai/references/deepseek) and [Reasoning Parser](https://"
"docs.sglang.ai/backend/separate_reasoning)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Qwen** (3, 3MoE, 2.5, 2 series)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`Qwen/Qwen3-0.6B`, `Qwen/Qwen3-30B-A3B`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Alibaba’s latest Qwen3 series for complex reasoning, language understanding, "
"and generation tasks; Support for MoE variants along with previous "
"generation 2.5, 2, etc. [SGLang provides Qwen3 specific reasoning parser]"
"(https://docs.sglang.ai/backend/separate_reasoning)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Llama** (2, 3.x, 4 series)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`meta-llama/Llama-4-Scout-17B-16E-Instruct`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Meta’s open LLM series, spanning 7B to 400B parameters (Llama 2, 3, and new "
"Llama 4) with well-recognized performance. [SGLang provides Llama-4 model-"
"specific optimizations](https://docs.sglang.ai/references/llama4)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Mistral** (Mixtral, NeMo, Small3)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`mistralai/Mistral-7B-Instruct-v0.2`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Open 7B LLM by Mistral AI with strong performance; extended into MoE "
"(“Mixtral”) and NeMo Megatron variants for larger scale."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Gemma** (v1, v2, v3)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`google/gemma-3-1b-it`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Google’s family of efficient multilingual models (1B–27B); Gemma 3 offers a "
"128K context window, and its larger (4B+) variants support vision input."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Phi** (Phi-1.5, Phi-2, Phi-3, Phi-4, Phi-MoE series)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`microsoft/Phi-4-multimodal-instruct`, `microsoft/Phi-3.5-MoE-instruct`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Microsoft’s Phi family of small models (1.3B–5.6B); Phi-4-multimodal (5.6B) "
"processes text, images, and speech, Phi-4-mini is a high-accuracy text model "
"and Phi-3.5-MoE is a mixture-of-experts model."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**MiniCPM** (v3, 4B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`openbmb/MiniCPM3-4B`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"OpenBMB’s series of compact LLMs for edge devices; MiniCPM 3 (4B) achieves "
"GPT-3.5-level results in text tasks."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**OLMoE** (Open MoE)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`allenai/OLMoE-1B-7B-0924`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Allen AI’s open Mixture-of-Experts model (7B total, 1B active parameters) "
"delivering state-of-the-art results with sparse expert activation."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**StableLM** (3B, 7B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`stabilityai/stablelm-tuned-alpha-7b`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"StabilityAI’s early open-source LLM (3B & 7B) for general text generation; a "
"demonstration model with basic instruction-following ability."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Command-R** (Cohere)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`CohereForAI/c4ai-command-r-v01`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Cohere’s open conversational LLM (Command series) optimized for long "
"context, retrieval-augmented generation, and tool use."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**DBRX** (Databricks)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`databricks/dbrx-instruct`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Databricks’ 132B-parameter MoE model (36B active) trained on 12T tokens; "
"competes with GPT-3.5 quality as a fully open foundation model."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Grok** (xAI)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`xai-org/grok-1`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"xAI’s grok-1 model known for vast size(314B parameters) and high quality; "
"integrated in SGLang for high-performance inference."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**ChatGLM** (GLM-130B family)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`THUDM/chatglm2-6b`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Zhipu AI’s bilingual chat model (6B) excelling at Chinese-English dialogue; "
"fine-tuned for conversational quality and alignment."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**InternLM 2** (7B, 20B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`internlm/internlm2-7b`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Next-gen InternLM (7B and 20B) from SenseTime, offering strong reasoning and "
"ultra-long context support (up to 200K tokens)."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**ExaONE 3** (Korean-English)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"LG AI Research’s Korean-English model (7.8B) trained on 8T tokens; provides "
"high-quality bilingual understanding and generation."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Baichuan 2** (7B, 13B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`baichuan-inc/Baichuan2-13B-Chat`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"BaichuanAI’s second-generation Chinese-English LLM (7B/13B) with improved "
"performance and an open commercial license."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**XVERSE** (MoE)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`xverse/XVERSE-MoE-A36B`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Yuanxiang’s open MoE LLM (XVERSE-MoE-A36B: 255B total, 36B active) "
"supporting ~40 languages; delivers 100B+ dense-level performance via expert "
"routing."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**SmolLM** (135M–1.7B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`HuggingFaceTB/SmolLM-1.7B`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Hugging Face’s ultra-small LLM series (135M–1.7B params) offering "
"surprisingly strong results, enabling advanced AI on mobile/edge devices."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**GLM-4** (Multilingual 9B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`ZhipuAI/glm-4-9b-chat`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Zhipu’s GLM-4 series (up to 9B parameters) – open multilingual models with "
"support for 1M-token context and even a 5.6B multimodal variant (Phi-4V)."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**MiMo** (7B series)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`XiaomiMiMo/MiMo-7B-RL`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Xiaomi's reasoning-optimized model series, leverages Multiple-Token "
"Prediction for faster inference."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Arcee AFM-4.5B**"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`arcee-ai/AFM-4.5B-Base`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Arcee's foundational model series for real world reliability and edge "
"deployments."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Persimmon** (8B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`adept/persimmon-8b-chat`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Adept’s open 8B model with a 16K context window and fast inference; trained "
"for broad usability and licensed under Apache 2.0."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Granite 3.0, 3.1** (IBM)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`ibm-granite/granite-3.1-8b-instruct`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"IBM's open dense foundation models optimized for reasoning, code, and "
"business AI use cases. Integrated with Red Hat and watsonx systems."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Granite 3.0 MoE** (IBM)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`ibm-granite/granite-3.0-3b-a800m-instruct`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"IBM’s Mixture-of-Experts models offering strong performance with cost-"
"efficiency. MoE expert routing designed for enterprise deployment at scale."
msgstr ""
