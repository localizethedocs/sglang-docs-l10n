# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.4\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../backend/offline_engine_api.ipynb:2
msgid ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:9
msgid "Offline Engine API"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:11
msgid ""
"SGLang provides a direct inference engine without the need for an HTTP "
"server, especially for use cases where additional HTTP server adds "
"unnecessary complexity or overhead. Here are two general use cases:"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:13
#: ../../../backend/offline_engine_api.ipynb:71
msgid "Offline Batch Inference"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:14
msgid "Custom Server on Top of the Engine"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:16
msgid ""
"This document focuses on the offline batch inference, demonstrating four "
"different inference modes:"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:18
msgid "Non-streaming synchronous generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:19
msgid "Streaming synchronous generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:20
msgid "Non-streaming asynchronous generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:21
msgid "Streaming asynchronous generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:23
msgid ""
"Additionally, you can easily build a custom server on top of the SGLang "
"offline engine. A detailed example working in a python script can be found "
"in `custom_server <https://github.com/sgl-project/sglang/blob/main/examples/"
"runtime/engine/custom_server.py>`__."
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:35
msgid "Nest Asyncio"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:37
msgid ""
"Note that if you want to use **Offline Engine** in ipython or some other "
"nested loop code, you need to add the following code:"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:39
msgid ""
"import nest_asyncio\n"
"\n"
"nest_asyncio.apply()"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:55
msgid "Advanced Usage"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:57
msgid ""
"The engine supports `vlm inference <https://github.com/sgl-project/sglang/"
"blob/main/examples/runtime/engine/offline_batch_inference_vlm.py>`__ as well "
"as `extracting hidden states <https://github.com/sgl-project/sglang/blob/"
"main/examples/runtime/hidden_states>`__."
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:59
msgid ""
"Please see `the examples <https://github.com/sgl-project/sglang/tree/main/"
"examples/runtime/engine>`__ for further use cases."
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:73
msgid ""
"SGLang offline engine supports batch inference with efficient scheduling."
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:-1
msgid "[ ]:"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:-1
msgid ""
"# launch the offline engine\n"
"import asyncio\n"
"import io\n"
"import os\n"
"\n"
"from PIL import Image\n"
"import requests\n"
"import sglang as sgl\n"
"\n"
"from sglang.srt.conversation import chat_templates\n"
"from sglang.test.test_utils import is_in_ci\n"
"from sglang.utils import async_stream_and_merge, stream_and_merge\n"
"\n"
"if is_in_ci():\n"
"    import patch\n"
"else:\n"
"    import nest_asyncio\n"
"\n"
"    nest_asyncio.apply()\n"
"\n"
"\n"
"llm = sgl.Engine(model_path=\"qwen/qwen2.5-0.5b-instruct\")"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:115
msgid "Non-streaming Synchronous Generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:-1
msgid ""
"prompts = [\n"
"    \"Hello, my name is\",\n"
"    \"The president of the United States is\",\n"
"    \"The capital of France is\",\n"
"    \"The future of AI is\",\n"
"]\n"
"\n"
"sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n"
"\n"
"outputs = llm.generate(prompts, sampling_params)\n"
"for prompt, output in zip(prompts, outputs):\n"
"    print(\"===============================\")\n"
"    print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:148
msgid "Streaming Synchronous Generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:-1
msgid ""
"prompts = [\n"
"    \"Write a short, neutral self-introduction for a fictional character. "
"Hello, my name is\",\n"
"    \"Provide a concise factual statement about France’s capital city. The "
"capital of France is\",\n"
"    \"Explain possible future trends in artificial intelligence. The future "
"of AI is\",\n"
"]\n"
"\n"
"sampling_params = {\n"
"    \"temperature\": 0.2,\n"
"    \"top_p\": 0.9,\n"
"}\n"
"\n"
"print(\"\\n=== Testing synchronous streaming generation with overlap removal "
"===\\n\")\n"
"\n"
"for prompt in prompts:\n"
"    print(f\"Prompt: {prompt}\")\n"
"    merged_output = stream_and_merge(llm, prompt, sampling_params)\n"
"    print(\"Generated text:\", merged_output)\n"
"    print()"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:186
msgid "Non-streaming Asynchronous Generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:-1
msgid ""
"prompts = [\n"
"    \"Write a short, neutral self-introduction for a fictional character. "
"Hello, my name is\",\n"
"    \"Provide a concise factual statement about France’s capital city. The "
"capital of France is\",\n"
"    \"Explain possible future trends in artificial intelligence. The future "
"of AI is\",\n"
"]\n"
"\n"
"sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n"
"\n"
"print(\"\\n=== Testing asynchronous batch generation ===\")\n"
"\n"
"\n"
"async def main():\n"
"    outputs = await llm.async_generate(prompts, sampling_params)\n"
"\n"
"    for prompt, output in zip(prompts, outputs):\n"
"        print(f\"\\nPrompt: {prompt}\")\n"
"        print(f\"Generated text: {output['text']}\")\n"
"\n"
"\n"
"asyncio.run(main())"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:226
msgid "Streaming Asynchronous Generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:-1
msgid ""
"prompts = [\n"
"    \"Write a short, neutral self-introduction for a fictional character. "
"Hello, my name is\",\n"
"    \"Provide a concise factual statement about France’s capital city. The "
"capital of France is\",\n"
"    \"Explain possible future trends in artificial intelligence. The future "
"of AI is\",\n"
"]\n"
"\n"
"sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n"
"\n"
"print(\"\\n=== Testing asynchronous streaming generation (no repeats) "
"===\")\n"
"\n"
"\n"
"async def main():\n"
"    for prompt in prompts:\n"
"        print(f\"\\nPrompt: {prompt}\")\n"
"        print(\"Generated text: \", end=\"\", flush=True)\n"
"\n"
"        # Replace direct calls to async_generate with our custom overlap-"
"aware version\n"
"        async for cleaned_chunk in async_stream_and_merge(llm, prompt, "
"sampling_params):\n"
"            print(cleaned_chunk, end=\"\", flush=True)\n"
"\n"
"        print()  # New line after each prompt\n"
"\n"
"\n"
"asyncio.run(main())"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:-1
msgid "llm.shutdown()"
msgstr ""
