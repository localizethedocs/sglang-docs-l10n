# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.4\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../backend/native_api.ipynb:2
msgid ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"
msgstr ""

#: ../../../backend/native_api.ipynb:9
msgid "SGLang Native APIs"
msgstr ""

#: ../../../backend/native_api.ipynb:11
msgid ""
"Apart from the OpenAI compatible APIs, the SGLang Runtime also provides its "
"native server APIs. We introduce these following APIs:"
msgstr ""

#: ../../../backend/native_api.ipynb:13
msgid "``/generate`` (text generation model)"
msgstr ""

#: ../../../backend/native_api.ipynb:14
msgid "``/get_model_info``"
msgstr ""

#: ../../../backend/native_api.ipynb:15
msgid "``/get_server_info``"
msgstr ""

#: ../../../backend/native_api.ipynb:16
msgid "``/health``"
msgstr ""

#: ../../../backend/native_api.ipynb:17
msgid "``/health_generate``"
msgstr ""

#: ../../../backend/native_api.ipynb:18
msgid "``/flush_cache``"
msgstr ""

#: ../../../backend/native_api.ipynb:19
msgid "``/update_weights``"
msgstr ""

#: ../../../backend/native_api.ipynb:20
msgid "``/encode``\\ (embedding model)"
msgstr ""

#: ../../../backend/native_api.ipynb:21
msgid "``/v1/rerank``\\ (cross encoder rerank model)"
msgstr ""

#: ../../../backend/native_api.ipynb:22
msgid "``/classify``\\ (reward model)"
msgstr ""

#: ../../../backend/native_api.ipynb:23
msgid "``/start_expert_distribution_record``"
msgstr ""

#: ../../../backend/native_api.ipynb:24
msgid "``/stop_expert_distribution_record``"
msgstr ""

#: ../../../backend/native_api.ipynb:25
msgid "``/dump_expert_distribution_record``"
msgstr ""

#: ../../../backend/native_api.ipynb:27
msgid ""
"We mainly use ``requests`` to test these APIs in the following examples. You "
"can also use ``curl``."
msgstr ""

#: ../../../backend/native_api.ipynb:39
msgid "Launch A Server"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid "[ ]:"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"import requests\n"
"from sglang.test.test_utils import is_in_ci\n"
"\n"
"if is_in_ci():\n"
"    from patch import launch_server_cmd\n"
"else:\n"
"    from sglang.utils import launch_server_cmd\n"
"\n"
"from sglang.utils import wait_for_server, print_highlight, "
"terminate_process\n"
"\n"
"\n"
"server_process, port = launch_server_cmd(\n"
"    \"python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-"
"instruct --host 0.0.0.0\"\n"
")\n"
"## To run qwen2.5-0.5b-instruct model on the Ascend-Npu, you can execute the "
"following command:\n"
"# server_process, port = launch_server_cmd(\n"
"#     \"python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-"
"instruct --host 0.0.0.0 --device npu --tp 2 --attention-backend "
"torch_native\"\n"
"# )\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../backend/native_api.ipynb:79
msgid "Generate (text generation model)"
msgstr ""

#: ../../../backend/native_api.ipynb:81
msgid ""
"Generate completions. This is similar to the ``/v1/completions`` in OpenAI "
"API. Detailed parameters can be found in the `sampling parameters <./"
"sampling_params.md>`__."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"url = f\"http://localhost:{port}/generate\"\n"
"data = {\"text\": \"What is the capital of France?\"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"print_highlight(response.json())"
msgstr ""

#: ../../../backend/native_api.ipynb:117
msgid "Get Model Info"
msgstr ""

#: ../../../backend/native_api.ipynb:119
msgid "Get the information of the model."
msgstr ""

#: ../../../backend/native_api.ipynb:121
msgid "``model_path``: The path/name of the model."
msgstr ""

#: ../../../backend/native_api.ipynb:122
msgid ""
"``is_generation``: Whether the model is used as generation model or "
"embedding model."
msgstr ""

#: ../../../backend/native_api.ipynb:123
msgid "``tokenizer_path``: The path/name of the tokenizer."
msgstr ""

#: ../../../backend/native_api.ipynb:124
msgid ""
"``preferred_sampling_params``: The default sampling params specified via ``--"
"preferred-sampling-params``. ``None`` is returned in this example as we did "
"not explicitly configure it in server args."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"url = f\"http://localhost:{port}/get_model_info\"\n"
"\n"
"response = requests.get(url)\n"
"response_json = response.json()\n"
"print_highlight(response_json)\n"
"assert response_json[\"model_path\"] == \"qwen/qwen2.5-0.5b-instruct\"\n"
"assert response_json[\"is_generation\"] is True\n"
"assert response_json[\"tokenizer_path\"] == \"qwen/qwen2.5-0.5b-instruct\"\n"
"assert response_json[\"preferred_sampling_params\"] is None\n"
"assert response_json.keys() == {\n"
"    \"model_path\",\n"
"    \"is_generation\",\n"
"    \"tokenizer_path\",\n"
"    \"preferred_sampling_params\",\n"
"}"
msgstr ""

#: ../../../backend/native_api.ipynb:159
msgid "Get Server Info"
msgstr ""

#: ../../../backend/native_api.ipynb:161
msgid ""
"Gets the server information including CLI arguments, token limits, and "
"memory pool sizes."
msgstr ""

#: ../../../backend/native_api.ipynb:163
msgid "Note: ``get_server_info`` merges the following deprecated endpoints:"
msgstr ""

#: ../../../backend/native_api.ipynb:165
msgid "``get_server_args``"
msgstr ""

#: ../../../backend/native_api.ipynb:166
msgid "``get_memory_pool_size``"
msgstr ""

#: ../../../backend/native_api.ipynb:167
msgid "``get_max_total_num_tokens``"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"# get_server_info\n"
"\n"
"url = f\"http://localhost:{port}/get_server_info\"\n"
"\n"
"response = requests.get(url)\n"
"print_highlight(response.text)"
msgstr ""

#: ../../../backend/native_api.ipynb:193
msgid "Health Check"
msgstr ""

#: ../../../backend/native_api.ipynb:195
msgid "``/health``: Check the health of the server."
msgstr ""

#: ../../../backend/native_api.ipynb:196
msgid ""
"``/health_generate``: Check the health of the server by generating one token."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"url = f\"http://localhost:{port}/health_generate\"\n"
"\n"
"response = requests.get(url)\n"
"print_highlight(response.text)"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"url = f\"http://localhost:{port}/health\"\n"
"\n"
"response = requests.get(url)\n"
"print_highlight(response.text)"
msgstr ""

#: ../../../backend/native_api.ipynb:232
msgid "Flush Cache"
msgstr ""

#: ../../../backend/native_api.ipynb:234
msgid ""
"Flush the radix cache. It will be automatically triggered when the model "
"weights are updated by the ``/update_weights`` API."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"# flush cache\n"
"\n"
"url = f\"http://localhost:{port}/flush_cache\"\n"
"\n"
"response = requests.post(url)\n"
"print_highlight(response.text)"
msgstr ""

#: ../../../backend/native_api.ipynb:260
msgid "Update Weights From Disk"
msgstr ""

#: ../../../backend/native_api.ipynb:262
msgid ""
"Update model weights from disk without restarting the server. Only "
"applicable for models with the same architecture and parameter size."
msgstr ""

#: ../../../backend/native_api.ipynb:264
msgid ""
"SGLang support ``update_weights_from_disk`` API for continuous evaluation "
"during training (save checkpoint to disk and update weights from disk)."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"# successful update with same architecture and size\n"
"\n"
"url = f\"http://localhost:{port}/update_weights_from_disk\"\n"
"data = {\"model_path\": \"qwen/qwen2.5-0.5b-instruct\"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"print_highlight(response.text)\n"
"assert response.json()[\"success\"] is True\n"
"assert response.json()[\"message\"] == \"Succeeded to update model weights.\""
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"# failed update with different parameter size or wrong name\n"
"\n"
"url = f\"http://localhost:{port}/update_weights_from_disk\"\n"
"data = {\"model_path\": \"qwen/qwen2.5-0.5b-instruct-wrong\"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"response_json = response.json()\n"
"print_highlight(response_json)\n"
"assert response_json[\"success\"] is False\n"
"assert response_json[\"message\"] == (\n"
"    \"Failed to get weights iterator: \"\n"
"    \"qwen/qwen2.5-0.5b-instruct-wrong\"\n"
"    \" (repository not found).\"\n"
")"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid "terminate_process(server_process)"
msgstr ""

#: ../../../backend/native_api.ipynb:324
msgid "Encode (embedding model)"
msgstr ""

#: ../../../backend/native_api.ipynb:326
msgid ""
"Encode text into embeddings. Note that this API is only available for "
"`embedding models <openai_api_embeddings.html#openai-apis-embedding>`__ and "
"will raise an error for generation models. Therefore, we launch a new server "
"to server an embedding model."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"embedding_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model-path Alibaba-NLP/gte-Qwen2-1.5B-"
"instruct \\\n"
"    --host 0.0.0.0 --is-embedding\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"# successful encode for embedding model\n"
"\n"
"url = f\"http://localhost:{port}/encode\"\n"
"data = {\"model\": \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", \"text\": \"Once "
"upon a time\"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"response_json = response.json()\n"
"print_highlight(f\"Text embedding (first 10): {response_json['embedding']"
"[:10]}\")"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid "terminate_process(embedding_process)"
msgstr ""

#: ../../../backend/native_api.ipynb:379
msgid "v1/rerank (cross encoder rerank model)"
msgstr ""

#: ../../../backend/native_api.ipynb:381
msgid ""
"Rerank a list of documents given a query using a cross-encoder model. Note "
"that this API is only available for cross encoder model like `BAAI/bge-"
"reranker-v2-m3 <https://huggingface.co/BAAI/bge-reranker-v2-m3>`__ with "
"``attention-backend`` ``triton`` and ``torch_native``."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"reranker_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model-path BAAI/bge-reranker-v2-m3 \\\n"
"    --host 0.0.0.0 --disable-radix-cache --chunked-prefill-size -1 --"
"attention-backend triton --is-embedding\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"# compute rerank scores for query and documents\n"
"\n"
"url = f\"http://localhost:{port}/v1/rerank\"\n"
"data = {\n"
"    \"model\": \"BAAI/bge-reranker-v2-m3\",\n"
"    \"query\": \"what is panda?\",\n"
"    \"documents\": [\n"
"        \"hi\",\n"
"        \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda "
"bear or simply panda, is a bear species endemic to China.\",\n"
"    ],\n"
"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"response_json = response.json()\n"
"for item in response_json:\n"
"    print_highlight(f\"Score: {item['score']:.2f} - Document: "
"'{item['document']}'\")"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid "terminate_process(reranker_process)"
msgstr ""

#: ../../../backend/native_api.ipynb:442
msgid "Classify (reward model)"
msgstr ""

#: ../../../backend/native_api.ipynb:444
msgid ""
"SGLang Runtime also supports reward models. Here we use a reward model to "
"classify the quality of pairwise generations."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"# Note that SGLang now treats embedding models and reward models as the same "
"type of models.\n"
"# This will be updated in the future.\n"
"\n"
"reward_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model-path Skywork/Skywork-Reward-"
"Llama-3.1-8B-v0.2 --host 0.0.0.0 --is-embedding\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"from transformers import AutoTokenizer\n"
"\n"
"PROMPT = (\n"
"    \"What is the range of the numeric output of a sigmoid node in a neural "
"network?\"\n"
")\n"
"\n"
"RESPONSE1 = \"The output of a sigmoid node is bounded between -1 and 1.\"\n"
"RESPONSE2 = \"The output of a sigmoid node is bounded between 0 and 1.\"\n"
"\n"
"CONVS = [\n"
"    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", "
"\"content\": RESPONSE1}],\n"
"    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", "
"\"content\": RESPONSE2}],\n"
"]\n"
"\n"
"tokenizer = AutoTokenizer.from_pretrained(\"Skywork/Skywork-Reward-"
"Llama-3.1-8B-v0.2\")\n"
"prompts = tokenizer.apply_chat_template(CONVS, tokenize=False)\n"
"\n"
"url = f\"http://localhost:{port}/classify\"\n"
"data = {\"model\": \"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\", \"text\": "
"prompts}\n"
"\n"
"responses = requests.post(url, json=data).json()\n"
"for response in responses:\n"
"    print_highlight(f\"reward: {response['embedding'][0]}\")"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid "terminate_process(reward_process)"
msgstr ""

#: ../../../backend/native_api.ipynb:514
msgid "Capture expert selection distribution in MoE models"
msgstr ""

#: ../../../backend/native_api.ipynb:516
msgid ""
"SGLang Runtime supports recording the number of times an expert is selected "
"in a MoE model run for each expert in the model. This is useful when "
"analyzing the throughput of the model and plan for optimization."
msgstr ""

#: ../../../backend/native_api.ipynb:518
msgid ""
"*Note: We only print out the first 10 lines of the csv below for better "
"readability. Please adjust accordingly if you want to analyze the results "
"more deeply.*"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"expert_record_server_process, port = launch_server_cmd(\n"
"    \"python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --"
"host 0.0.0.0 --expert-distribution-recorder-mode stat\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"response = requests.post(f\"http://localhost:{port}/"
"start_expert_distribution_record\")\n"
"print_highlight(response)\n"
"\n"
"url = f\"http://localhost:{port}/generate\"\n"
"data = {\"text\": \"What is the capital of France?\"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"print_highlight(response.json())\n"
"\n"
"response = requests.post(f\"http://localhost:{port}/"
"stop_expert_distribution_record\")\n"
"print_highlight(response)\n"
"\n"
"response = requests.post(f\"http://localhost:{port}/"
"dump_expert_distribution_record\")\n"
"print_highlight(response)"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid "terminate_process(expert_record_server_process)"
msgstr ""
