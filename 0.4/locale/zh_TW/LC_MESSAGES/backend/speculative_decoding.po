# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.4\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../backend/speculative_decoding.ipynb:2
msgid ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"
msgstr ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"

#: ../../../backend/speculative_decoding.ipynb:9
msgid "Speculative Decoding"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:11
msgid ""
"SGLang now provides an EAGLE-based (EAGLE-2/EAGLE-3) speculative decoding "
"option. Our implementation aims to maximize speed and efficiency and is "
"considered to be among the fastest in open-source LLM engines. **Note:** "
"Currently, Speculative Decoding in SGLang is compatible with radix cache and "
"chunked prefill."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:14
msgid "Performance Highlights"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:16
msgid ""
"Please see below for the huge improvements on throughput for LLaMA-Instruct "
"3.1 8B tested on MT bench that can be achieved via EAGLE3 decoding. For "
"further details please see the `EAGLE3 paper <https://arxiv.org/"
"pdf/2503.01840>`__."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:19
msgid "Method"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:19
msgid "Throughput (tokens/s)"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:21
msgid "SGLang (w/o speculative, 1x H100)"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:21
msgid "158.34 tokens/s"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:22
msgid "SGLang + EAGLE-2 (1x H100)"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:22
msgid "244.10 tokens/s"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:23
msgid "SGLang + EAGLE-3 (1x H100)"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:23
msgid "373.25 tokens/s"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:36
msgid "EAGLE Decoding"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:38
msgid ""
"To enable EAGLE speculative decoding the following parameters are relevant:"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:40
msgid ""
"``speculative_draft_model_path``: Specifies draft model. This parameter is "
"required."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:42
msgid ""
"``speculative_num_steps``: Depth of autoregressive drafting. Increases "
"speculation range but risks rejection cascades. Default is 5."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:44
msgid ""
"``speculative_eagle_topk``: Branching factor per step. Improves candidate "
"diversity, will lead to higher acceptance rate, but more lead to higher "
"memory/compute consumption. Default is 4."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:46
msgid ""
"``speculative_num_draft_tokens``: Maximum parallel verification capacity. "
"Allows deeper tree evaluation but will lead to higher GPU memory usage. "
"Default is 8."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:48
msgid "These parameters are the same for EAGLE-2 and EAGLE-3."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:60
msgid "EAGLE-2 decoding"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:62
msgid ""
"You can enable EAGLE-2 decoding by setting ``--speculative_algorithm EAGLE`` "
"and choosing an appropriate model."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:-1
msgid "[ ]:"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:-1
msgid ""
"from sglang.test.test_utils import is_in_ci\n"
"\n"
"if is_in_ci():\n"
"    from patch import launch_server_cmd\n"
"else:\n"
"    from sglang.utils import launch_server_cmd\n"
"\n"
"from sglang.utils import wait_for_server, print_highlight, "
"terminate_process\n"
"\n"
"import openai"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:-1
msgid ""
"server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model meta-llama/Llama-2-7b-chat-hf  --"
"speculative-algorithm EAGLE \\\n"
"    --speculative-draft-model-path lmsys/sglang-EAGLE-llama2-chat-7B --"
"speculative-num-steps 3 \\\n"
"    --speculative-eagle-topk 4 --speculative-num-draft-tokens 16 --cuda-"
"graph-max-bs 8\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:-1
msgid ""
"client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", "
"api_key=\"None\")\n"
"\n"
"response = client.chat.completions.create(\n"
"    model=\"meta-llama/Llama-2-7b-chat-hf\",\n"
"    messages=[\n"
"        {\"role\": \"user\", \"content\": \"List 3 countries and their "
"capitals.\"},\n"
"    ],\n"
"    temperature=0,\n"
"    max_tokens=64,\n"
")\n"
"\n"
"print_highlight(f\"Response: {response}\")"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:-1
msgid "terminate_process(server_process)"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:138
msgid "EAGLE-2 Decoding with ``torch.compile``"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:140
msgid ""
"You can also enable ``torch.compile`` for further optimizations and "
"optionally set ``--torch-compile-max-bs``:"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:-1
msgid ""
"server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model meta-llama/Llama-2-7b-chat-hf  --"
"speculative-algorithm EAGLE \\\n"
"    --speculative-draft-model-path lmsys/sglang-EAGLE-llama2-chat-7B --"
"speculative-num-steps 5 \\\n"
"        --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --mem-"
"fraction 0.6 \\\n"
"            --enable-torch-compile --torch-compile-max-bs 2\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:199
msgid "EAGLE-2 Decoding via Frequency-Ranked Speculative Sampling"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:201
msgid ""
"By employing a truncated high-frequency token vocabulary in the draft model, "
"Eagle speculative decoding reduces ``lm_head`` computational overhead while "
"accelerating the pipeline without quality degradation. For more details, "
"checkout `the paper <https://arxiv.org/pdf/arXiv:2502.14856>`__."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:203
msgid ""
"In our implementation, set ``--speculative-token-map`` to enable the "
"optimization. You can get the high-frequency token in FR-Spec from `this "
"model <https://huggingface.co/thunlp/LLaMA3-Instruct-8B-FR-Spec>`__. Or you "
"can obtain high-frequency token by directly downloading these token from "
"`this repo <https://github.com/thunlp/FR-Spec/tree/main?tab=readme-ov-"
"file#prepare-fr-spec-vocabulary-subset>`__."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:205
msgid ""
"Thanks for the contribution from `Weilin Zhao <https://github.com/"
"Achazwl>`__ and `Zhousx <https://github.com/Zhou-sx>`__."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:-1
msgid ""
"server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model meta-llama/Meta-Llama-3-8B-Instruct "
"--speculative-algorithm EAGLE \\\n"
"    --speculative-draft-model-path lmsys/sglang-EAGLE-LLaMA3-Instruct-8B --"
"speculative-num-steps 5 \\\n"
"    --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --"
"speculative-token-map thunlp/LLaMA3-Instruct-8B-FR-Spec/freq_32768.pt \\\n"
"    --mem-fraction 0.7 --cuda-graph-max-bs 2 --dtype float16\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:-1
msgid ""
"client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", "
"api_key=\"None\")\n"
"\n"
"response = client.chat.completions.create(\n"
"    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n"
"    messages=[\n"
"        {\"role\": \"user\", \"content\": \"List 3 countries and their "
"capitals.\"},\n"
"    ],\n"
"    temperature=0,\n"
"    max_tokens=64,\n"
")\n"
"\n"
"print_highlight(f\"Response: {response}\")"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:264
msgid "EAGLE-3 Decoding"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:266
msgid ""
"You can enable EAGLE-3 decoding by setting ``--speculative_algorithm "
"EAGLE3`` and choosing an appropriate model."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:-1
msgid ""
"server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model meta-llama/Llama-3.1-8B-Instruct  --"
"speculative-algorithm EAGLE3 \\\n"
"    --speculative-draft-model-path jamesliu1/sglang-EAGLE3-Llama-3.1-"
"Instruct-8B --speculative-num-steps 5 \\\n"
"        --speculative-eagle-topk 8 --speculative-num-draft-tokens 32 --mem-"
"fraction 0.6 \\\n"
"        --cuda-graph-max-bs 2 --dtype float16\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:-1
msgid ""
"client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", "
"api_key=\"None\")\n"
"\n"
"response = client.chat.completions.create(\n"
"    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n"
"    messages=[\n"
"        {\"role\": \"user\", \"content\": \"List 3 countries and their "
"capitals.\"},\n"
"    ],\n"
"    temperature=0,\n"
"    max_tokens=64,\n"
")\n"
"\n"
"print_highlight(f\"Response: {response}\")"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:325
msgid "Multi Token Prediction"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:327
msgid ""
"We support `MTP(Multi-Token Prediction) <https://arxiv.org/"
"pdf/2404.19737>`__ in SGLang by using speculative decoding. We use Xiaomi/"
"MiMo-7B-RL model as example here (deepseek mtp usage refer to `deepseek doc "
"<../references/deepseek.md#multi-token-prediction>`__)"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:-1
msgid ""
"server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"    python3 -m sglang.launch_server --model-path XiaomiMiMo/MiMo-7B-RL --"
"host 0.0.0.0 --trust-remote-code \\\n"
"    --speculative-algorithm EAGLE --speculative-num-steps 1 --speculative-"
"eagle-topk 1 --speculative-num-draft-tokens 2 \\\n"
"    --mem-fraction 0.5\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:-1
msgid ""
"import requests\n"
"\n"
"url = f\"http://localhost:{port}/v1/chat/completions\"\n"
"\n"
"data = {\n"
"    \"model\": \"XiaomiMiMo/MiMo-7B-RL\",\n"
"    \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital "
"of France?\"}],\n"
"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"print_highlight(response.json())"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:384
msgid "References"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:386
msgid "EAGLE process is as follows:"
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:388
msgid ""
"Within EAGLE the draft model predicts the next feature vector, i.e. the last "
"hidden state of the original LLM, using the feature sequence :math:"
"`(f_1, ..., f_k)` and the token sequence :math:`(t_2, ..., t_{k+1})`."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:389
msgid ""
"The next token is then sampled from :math:`p_{k+2}=\\text{LMHead}(f_{k+1})`. "
"Afterwards, the two sequences are extended in a tree style—branching out "
"multiple potential continuations, with the branching factor per step "
"controlled by the ``speculative_eagle_topk`` parameter—to ensure a more "
"coherent connection of context, and are given as input again."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:390
msgid ""
"EAGLE-2 additionally uses the draft model to evaluate how probable certain "
"branches in the draft tree are, dynamically stopping the expansion of "
"unlikely branches. After the expansion phase, reranking is employed to "
"select only the top ``speculative_num_draft_tokens`` final nodes as draft "
"tokens."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:391
msgid ""
"EAGLE-3 removes the feature prediction objective, incorporates low and mid-"
"layer features, and is trained in an on-policy manner."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:393
msgid ""
"This enhances drafting accuracy by operating on the features instead of "
"tokens for more regular inputs and passing the tokens from the next timestep "
"additionally to minimize randomness effects from sampling. Furthermore the "
"dynamic adjustment of the draft tree and selection of reranked final nodes "
"increases acceptance rate of draft tokens further. For more details see "
"`EAGLE-2 <https://arxiv.org/abs/2406.16858>`__ and `EAGLE-3 <https://arxiv."
"org/abs/2503.01840>`__ paper."
msgstr ""

#: ../../../backend/speculative_decoding.ipynb:395
msgid ""
"For guidance how to train your own EAGLE model please see the `EAGLE repo "
"<https://github.com/SafeAILab/EAGLE/tree/main?tab=readme-ov-file#train>`__."
msgstr ""
