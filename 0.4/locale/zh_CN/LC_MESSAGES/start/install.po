# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.4\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../start/install.md:1
msgid "Install SGLang"
msgstr ""

#: ../../../start/install.md:3
msgid "You can install SGLang using any of the methods below."
msgstr ""

#: ../../../start/install.md:5
msgid ""
"For running DeepSeek V3/R1, refer to [DeepSeek V3 Support](https://github."
"com/sgl-project/sglang/tree/main/benchmark/deepseek_v3). It is recommended "
"to use the latest version and deploy it with [Docker](https://github.com/sgl-"
"project/sglang/tree/main/benchmark/deepseek_v3#using-docker-recommended) to "
"avoid environment-related issues."
msgstr ""

#: ../../../start/install.md:7
msgid ""
"It is recommended to use uv to install the dependencies for faster "
"installation:"
msgstr ""

#: ../../../start/install.md:9
msgid "Method 1: With pip or uv"
msgstr ""

#: ../../../start/install.md:11
msgid ""
"pip install --upgrade pip\n"
"pip install uv\n"
"uv pip install \"sglang[all]>=0.4.10\"\n"
msgstr ""

#: ../../../start/install.md:17
msgid "**Quick Fixes to Common Problems**"
msgstr ""

#: ../../../start/install.md:19
msgid ""
"SGLang currently uses torch 2.7.1, so you need to install flashinfer for "
"torch 2.7.1. If you want to install flashinfer separately, please refer to "
"[FlashInfer installation doc](https://docs.flashinfer.ai/installation.html). "
"Please note that the FlashInfer pypi package is called `flashinfer-python` "
"instead of `flashinfer`."
msgstr ""

#: ../../../start/install.md:21
msgid ""
"If you encounter `OSError: CUDA_HOME environment variable is not set`. "
"Please set it to your CUDA install root with either of the following "
"solutions:"
msgstr ""

#: ../../../start/install.md:23
msgid ""
"Use `export CUDA_HOME=/usr/local/cuda-<your-cuda-version>` to set the "
"`CUDA_HOME` environment variable."
msgstr ""

#: ../../../start/install.md:24
msgid ""
"Install FlashInfer first following [FlashInfer installation doc](https://"
"docs.flashinfer.ai/installation.html), then install SGLang as described "
"above."
msgstr ""

#: ../../../start/install.md:26
msgid "Method 2: From source"
msgstr ""

#: ../../../start/install.md:28
msgid ""
"# Use the last release branch\n"
"git clone -b v0.4.10 https://github.com/sgl-project/sglang.git\n"
"cd sglang\n"
"\n"
"pip install --upgrade pip\n"
"pip install -e \"python[all]\"\n"
msgstr ""

#: ../../../start/install.md:37
msgid ""
"Note: SGLang currently uses torch 2.7.1, so you need to install flashinfer "
"for torch 2.7.1. If you want to install flashinfer separately, please refer "
"to [FlashInfer installation doc](https://docs.flashinfer.ai/installation."
"html)."
msgstr ""

#: ../../../start/install.md:39
msgid ""
"If you want to develop SGLang, it is recommended to use docker. Please refer "
"to [setup docker container](https://github.com/sgl-project/sglang/blob/main/"
"docs/references/development_guide_using_docker.md#setup-docker-container) "
"for guidance. The docker image is `lmsysorg/sglang:dev`."
msgstr ""

#: ../../../start/install.md:41
msgid "Note: For AMD ROCm system with Instinct/MI GPUs, do following instead:"
msgstr ""

#: ../../../start/install.md:43
msgid ""
"# Use the last release branch\n"
"git clone -b v0.4.10 https://github.com/sgl-project/sglang.git\n"
"cd sglang\n"
"\n"
"pip install --upgrade pip\n"
"cd sgl-kernel\n"
"python setup_rocm.py install\n"
"cd ..\n"
"pip install -e \"python[all_hip]\"\n"
msgstr ""

#: ../../../start/install.md:55
msgid ""
"Note: Please refer to [the CPU environment setup command list](../references/"
"cpu.md#install-from-source) to set up the SGLang environment for running the "
"models with CPU servers."
msgstr ""

#: ../../../start/install.md:58
msgid "Method 3: Using docker"
msgstr ""

#: ../../../start/install.md:60
msgid ""
"The docker images are available on Docker Hub as [lmsysorg/sglang](https://"
"hub.docker.com/r/lmsysorg/sglang/tags), built from [Dockerfile](https://"
"github.com/sgl-project/sglang/tree/main/docker). Replace `<secret>` below "
"with your huggingface hub [token](https://huggingface.co/docs/hub/en/"
"security-tokens)."
msgstr ""

#: ../../../start/install.md:63
msgid ""
"docker run --gpus all \\\n"
"    --shm-size 32g \\\n"
"    -p 30000:30000 \\\n"
"    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n"
"    --env \"HF_TOKEN=<secret>\" \\\n"
"    --ipc=host \\\n"
"    lmsysorg/sglang:latest \\\n"
"    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-"
"Instruct --host 0.0.0.0 --port 30000\n"
msgstr ""

#: ../../../start/install.md:74
msgid ""
"Note: For AMD ROCm system with Instinct/MI GPUs, it is recommended to use "
"`docker/Dockerfile.rocm` to build images, example and usage as below:"
msgstr ""

#: ../../../start/install.md:76
msgid ""
"docker build --build-arg SGL_BRANCH=v0.4.10 -t v0.4.10-rocm630 -f Dockerfile."
"rocm .\n"
"\n"
"alias drun='docker run -it --rm --network=host --device=/dev/kfd --device=/"
"dev/dri --ipc=host \\\n"
"    --shm-size 16G --group-add video --cap-add=SYS_PTRACE --security-opt "
"seccomp=unconfined \\\n"
"    -v $HOME/dockerx:/dockerx -v /data:/data'\n"
"\n"
"drun -p 30000:30000 \\\n"
"    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n"
"    --env \"HF_TOKEN=<secret>\" \\\n"
"    v0.4.10-rocm630 \\\n"
"    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-"
"Instruct --host 0.0.0.0 --port 30000\n"
"\n"
"# Till flashinfer backend available, --attention-backend triton --sampling-"
"backend pytorch are set by default\n"
"drun v0.4.10-rocm630 python3 -m sglang.bench_one_batch --batch-size 32 --"
"input 1024 --output 128 --model amd/Meta-Llama-3.1-8B-Instruct-FP8-KV --tp 8 "
"--quantization fp8\n"
msgstr ""

#: ../../../start/install.md:93
msgid ""
"Note: Please refer to [the CPU installation guide using Docker](../"
"references/cpu.md#install-using-docker) to set up the SGLang environment for "
"running the models with CPU servers."
msgstr ""

#: ../../../start/install.md:96
msgid "Method 4: Using docker compose"
msgstr ""

#: ../../../start/install.md:98 ../../../start/install.md:110
#: ../../../start/install.md:125
msgid ""
"<details>\n"
"<summary>More</summary>\n"
msgstr ""

#: ../../../start/install.md:101
msgid ""
"This method is recommended if you plan to serve it as a service. A better "
"approach is to use the [k8s-sglang-service.yaml](https://github.com/sgl-"
"project/sglang/blob/main/docker/k8s-sglang-service.yaml)."
msgstr ""

#: ../../../start/install.md:104
msgid ""
"Copy the [compose.yml](https://github.com/sgl-project/sglang/blob/main/"
"docker/compose.yaml) to your local machine"
msgstr ""

#: ../../../start/install.md:105
msgid "Execute the command `docker compose up -d` in your terminal."
msgstr ""

#: ../../../start/install.md:106 ../../../start/install.md:121
#: ../../../start/install.md:153 ../../../start/install.md:164
msgid "</details>\n"
msgstr ""

#: ../../../start/install.md:108
msgid "Method 5: Using Kubernetes"
msgstr ""

#: ../../../start/install.md:113
msgid ""
"Option 1: For single node serving (typically when the model size fits into "
"GPUs on one node)"
msgstr ""

#: ../../../start/install.md:115
msgid ""
"Execute command `kubectl apply -f docker/k8s-sglang-service.yaml`, to create "
"k8s deployment and service, with llama-31-8b as example."
msgstr ""

#: ../../../start/install.md:117
msgid ""
"Option 2: For multi-node serving (usually when a large model requires more "
"than one GPU node, such as `DeepSeek-R1`)"
msgstr ""

#: ../../../start/install.md:119
msgid ""
"Modify the LLM model path and arguments as necessary, then execute command "
"`kubectl apply -f docker/k8s-sglang-distributed-sts.yaml`, to create two "
"nodes k8s statefulset and serving service."
msgstr ""

#: ../../../start/install.md:123
msgid "Method 6: Run on Kubernetes or Clouds with SkyPilot"
msgstr ""

#: ../../../start/install.md:128
msgid ""
"To deploy on Kubernetes or 12+ clouds, you can use [SkyPilot](https://github."
"com/skypilot-org/skypilot)."
msgstr ""

#: ../../../start/install.md:130
msgid ""
"Install SkyPilot and set up Kubernetes cluster or cloud access: see "
"[SkyPilot's documentation](https://skypilot.readthedocs.io/en/latest/getting-"
"started/installation.html)."
msgstr ""

#: ../../../start/install.md:131
msgid ""
"Deploy on your own infra with a single command and get the HTTP API endpoint:"
msgstr ""

#: ../../../start/install.md:132
msgid ""
"<details>\n"
"<summary>SkyPilot YAML: <code>sglang.yaml</code></summary>\n"
msgstr ""

#: ../../../start/install.md:135
msgid ""
"# sglang.yaml\n"
"envs:\n"
"  HF_TOKEN: null\n"
"\n"
"resources:\n"
"  image_id: docker:lmsysorg/sglang:latest\n"
"  accelerators: A100\n"
"  ports: 30000\n"
"\n"
"run: |\n"
"  conda deactivate\n"
"  python3 -m sglang.launch_server \\\n"
"    --model-path meta-llama/Llama-3.1-8B-Instruct \\\n"
"    --host 0.0.0.0 \\\n"
"    --port 30000\n"
msgstr ""

#: ../../../start/install.md:155
msgid ""
"# Deploy on any cloud or Kubernetes cluster. Use --cloud <cloud> to select a "
"specific cloud provider.\n"
"HF_TOKEN=<secret> sky launch -c sglang --env HF_TOKEN sglang.yaml\n"
"\n"
"# Get the HTTP API endpoint\n"
"sky status --endpoint 30000 sglang\n"
msgstr ""

#: ../../../start/install.md:163
msgid ""
"To further scale up your deployment with autoscaling and failure recovery, "
"check out the [SkyServe + SGLang guide](https://github.com/skypilot-org/"
"skypilot/tree/master/llm/sglang#serving-llama-2-with-sglang-for-more-traffic-"
"using-skyserve)."
msgstr ""

#: ../../../start/install.md:166
msgid "Common Notes"
msgstr ""

#: ../../../start/install.md:168
msgid ""
"[FlashInfer](https://github.com/flashinfer-ai/flashinfer) is the default "
"attention kernel backend. It only supports sm75 and above. If you encounter "
"any FlashInfer-related issues on sm75+ devices (e.g., T4, A10, A100, L4, "
"L40S, H100), please switch to other kernels by adding `--attention-backend "
"triton --sampling-backend pytorch` and open an issue on GitHub."
msgstr ""

#: ../../../start/install.md:169
msgid ""
"If you only need to use OpenAI models with the frontend language, you can "
"avoid installing other dependencies by using `pip install "
"\"sglang[openai]\"`."
msgstr ""

#: ../../../start/install.md:170
msgid ""
"The language frontend operates independently of the backend runtime. You can "
"install the frontend locally without needing a GPU, while the backend can be "
"set up on a GPU-enabled machine. To install the frontend, run `pip install "
"sglang`, and for the backend, use `pip install sglang[srt]`. `srt` is the "
"abbreviation of SGLang runtime."
msgstr ""

#: ../../../start/install.md:171
msgid ""
"To reinstall flashinfer locally, use the following command: `pip3 install --"
"upgrade flashinfer-python --force-reinstall --no-deps` and then delete the "
"cache with `rm -rf ~/.cache/flashinfer`."
msgstr ""
