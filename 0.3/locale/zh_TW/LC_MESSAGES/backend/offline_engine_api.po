# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2024, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.3\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../backend/offline_engine_api.ipynb:2
msgid ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"
msgstr ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"

#: ../../../backend/offline_engine_api.ipynb:9
msgid "Offline Engine API"
msgstr "離線引擎 API"

#: ../../../backend/offline_engine_api.ipynb:11
msgid ""
"SGLang provides a direct inference engine without the need for an HTTP "
"server, especially for use cases where additional HTTP server adds "
"unnecessary complexity or overhead. Here are two general use cases:"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:13
#: ../../../backend/offline_engine_api.ipynb:35
msgid "Offline Batch Inference"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:14
msgid "Custom Server on Top of the Engine"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:16
msgid ""
"This document focuses on the offline batch inference, demonstrating four "
"different inference modes:"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:18
msgid "Non-streaming synchronous generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:19
msgid "Streaming synchronous generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:20
msgid "Non-streaming asynchronous generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:21
msgid "Streaming asynchronous generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:23
msgid ""
"Additionally, you can easily build a custom server on top of the SGLang "
"offline engine. A detailed example working in a python script can be found "
"in `custom_server <https://github.com/sgl-project/sglang/blob/main/examples/"
"runtime/engine/custom_server.py>`__."
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:37
msgid ""
"SGLang offline engine supports batch inference with efficient scheduling."
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:-1
msgid "[ ]:"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:-1
msgid ""
"# launch the offline engine\n"
"\n"
"import sglang as sgl\n"
"from sglang.utils import print_highlight\n"
"import asyncio\n"
"\n"
"llm = sgl.Engine(model_path=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:64
msgid "Non-streaming Synchronous Generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:-1
msgid ""
"prompts = [\n"
"    \"Hello, my name is\",\n"
"    \"The president of the United States is\",\n"
"    \"The capital of France is\",\n"
"    \"The future of AI is\",\n"
"]\n"
"\n"
"sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n"
"\n"
"outputs = llm.generate(prompts, sampling_params)\n"
"for prompt, output in zip(prompts, outputs):\n"
"    print_highlight(\"===============================\")\n"
"    print_highlight(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:97
msgid "Streaming Synchronous Generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:-1
msgid ""
"prompts = [\n"
"    \"Hello, my name is\",\n"
"    \"The capital of France is\",\n"
"    \"The future of AI is\",\n"
"]\n"
"sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n"
"\n"
"print_highlight(\"\\n=== Testing synchronous streaming generation ===\")\n"
"\n"
"for prompt in prompts:\n"
"    print_highlight(f\"\\nPrompt: {prompt}\")\n"
"    print(\"Generated text: \", end=\"\", flush=True)\n"
"\n"
"    for chunk in llm.generate(prompt, sampling_params, stream=True):\n"
"        print(chunk[\"text\"], end=\"\", flush=True)\n"
"    print()"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:133
msgid "Non-streaming Asynchronous Generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:-1
msgid ""
"prompts = [\n"
"    \"Hello, my name is\",\n"
"    \"The capital of France is\",\n"
"    \"The future of AI is\",\n"
"]\n"
"\n"
"sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n"
"\n"
"print_highlight(\"\\n=== Testing asynchronous batch generation ===\")\n"
"\n"
"\n"
"async def main():\n"
"    outputs = await llm.async_generate(prompts, sampling_params)\n"
"\n"
"    for prompt, output in zip(prompts, outputs):\n"
"        print_highlight(f\"\\nPrompt: {prompt}\")\n"
"        print_highlight(f\"Generated text: {output['text']}\")\n"
"\n"
"\n"
"asyncio.run(main())"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:173
msgid "Streaming Asynchronous Generation"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:-1
msgid ""
"prompts = [\n"
"    \"Hello, my name is\",\n"
"    \"The capital of France is\",\n"
"    \"The future of AI is\",\n"
"]\n"
"sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n"
"\n"
"print_highlight(\"\\n=== Testing asynchronous streaming generation ===\")\n"
"\n"
"\n"
"async def main():\n"
"    for prompt in prompts:\n"
"        print_highlight(f\"\\nPrompt: {prompt}\")\n"
"        print(\"Generated text: \", end=\"\", flush=True)\n"
"\n"
"        generator = await llm.async_generate(prompt, sampling_params, "
"stream=True)\n"
"        async for chunk in generator:\n"
"            print(chunk[\"text\"], end=\"\", flush=True)\n"
"        print()\n"
"\n"
"\n"
"asyncio.run(main())"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:-1
msgid "[6]:"
msgstr ""

#: ../../../backend/offline_engine_api.ipynb:-1
msgid "llm.shutdown()"
msgstr ""
