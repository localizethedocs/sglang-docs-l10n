# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2024, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.3\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../references/supported_models.md:1
msgid "Supported Models"
msgstr ""

#: ../../../references/supported_models.md:3
msgid "Generative Models"
msgstr ""

#: ../../../references/supported_models.md:4
msgid "Llama / Llama 2 / Llama 3 / Llama 3.1 / Llama 3.2"
msgstr ""

#: ../../../references/supported_models.md:5
msgid "Mistral / Mixtral / Mistral NeMo"
msgstr ""

#: ../../../references/supported_models.md:6
msgid "Gemma / Gemma 2"
msgstr ""

#: ../../../references/supported_models.md:7
msgid "Qwen / Qwen 2 / Qwen 2 MoE / Qwen 2 VL"
msgstr ""

#: ../../../references/supported_models.md:8
msgid "DeepSeek / DeepSeek 2"
msgstr ""

#: ../../../references/supported_models.md:9
msgid "OLMoE"
msgstr ""

#: ../../../references/supported_models.md:10
msgid ""
"[LLaVA-OneVision](https://llava-vl.github.io/blog/2024-08-05-llava-"
"onevision/)"
msgstr ""

#: ../../../references/supported_models.md:11
msgid ""
"`python3 -m sglang.launch_server --model-path lmms-lab/llava-onevision-"
"qwen2-7b-ov --port=30000 --chat-template=chatml-llava`"
msgstr ""

#: ../../../references/supported_models.md:12
msgid ""
"`python3 -m sglang.launch_server --model-path lmms-lab/llava-onevision-"
"qwen2-72b-ov --port=30000 --tp-size=8 --chat-template=chatml-llava`"
msgstr ""

#: ../../../references/supported_models.md:13
#: ../../../references/supported_models.md:17
msgid ""
"Query the server with the [OpenAI Vision API](https://platform.openai.com/"
"docs/guides/vision). See examples at [test/srt/test_vision_openai_server.py]"
"(https://github.com/sgl-project/sglang/blob/main/test/srt/"
"test_vision_openai_server.py)"
msgstr ""

#: ../../../references/supported_models.md:14
msgid "LLaVA 1.5 / 1.6 / NeXT"
msgstr ""

#: ../../../references/supported_models.md:15
msgid ""
"`python -m sglang.launch_server --model-path lmms-lab/llama3-llava-next-8b --"
"port=30000 --tp-size=1 --chat-template=llava_llama_3`"
msgstr ""

#: ../../../references/supported_models.md:16
msgid ""
"`python -m sglang.launch_server --model-path lmms-lab/llava-next-72b --"
"port=30000 --tp-size=8 --chat-template=chatml-llava`"
msgstr ""

#: ../../../references/supported_models.md:18
msgid "Yi-VL"
msgstr ""

#: ../../../references/supported_models.md:19
msgid "StableLM"
msgstr ""

#: ../../../references/supported_models.md:20
msgid "Command-R"
msgstr ""

#: ../../../references/supported_models.md:21
msgid "DBRX"
msgstr ""

#: ../../../references/supported_models.md:22
msgid "Grok"
msgstr ""

#: ../../../references/supported_models.md:23
msgid "ChatGLM"
msgstr ""

#: ../../../references/supported_models.md:24
msgid "InternLM 2"
msgstr ""

#: ../../../references/supported_models.md:25
msgid "Exaone 3"
msgstr ""

#: ../../../references/supported_models.md:26
msgid "BaiChuan2"
msgstr ""

#: ../../../references/supported_models.md:27
msgid "MiniCPM / MiniCPM 3"
msgstr ""

#: ../../../references/supported_models.md:28
msgid "XVERSE / XVERSE MoE"
msgstr ""

#: ../../../references/supported_models.md:29
msgid "SmolLM"
msgstr ""

#: ../../../references/supported_models.md:30
msgid "GLM-4"
msgstr ""

#: ../../../references/supported_models.md:31
msgid "Phi-3-Small"
msgstr ""

#: ../../../references/supported_models.md:33
msgid "Embedding Models"
msgstr ""

#: ../../../references/supported_models.md:35
msgid "LlamaEmbeddingModel"
msgstr ""

#: ../../../references/supported_models.md:36
msgid "Mistral embedding models"
msgstr ""

#: ../../../references/supported_models.md:37
msgid "QWen embedding models"
msgstr ""

#: ../../../references/supported_models.md:38
msgid ""
"`python -m sglang.launch_server --model-path Alibaba-NLP/gte-Qwen2-7B-"
"instruct --is-embedding`"
msgstr ""

#: ../../../references/supported_models.md:40
msgid "Reward Models"
msgstr ""

#: ../../../references/supported_models.md:42
msgid "LlamaForSequenceClassification"
msgstr ""

#: ../../../references/supported_models.md:43
msgid ""
"`python -m sglang.launch_server --model-path Skywork/Skywork-Reward-"
"Llama-3.1-8B-v0.2 --is-embedding`"
msgstr ""

#: ../../../references/supported_models.md:44
msgid "Gemma2ForSequenceClassification"
msgstr ""

#: ../../../references/supported_models.md:45
msgid ""
"`python -m sglang.launch_server --model-path Skywork/Skywork-Reward-"
"Gemma-2-27B-v0.2 --is-embedding`"
msgstr ""

#: ../../../references/supported_models.md:46
msgid "InternLM2ForRewardModel"
msgstr ""

#: ../../../references/supported_models.md:47
msgid ""
"`python -m sglang.launch_server --model-path internlm/internlm2-7b-reward --"
"is-embedding --trust-remote-code`"
msgstr ""

#: ../../../references/supported_models.md:49
msgid "How to Support a New Model"
msgstr ""

#: ../../../references/supported_models.md:51
msgid ""
"To support a new model in SGLang, you only need to add a single file under "
"[SGLang Models Directory](https://github.com/sgl-project/sglang/tree/main/"
"python/sglang/srt/models). You can learn from existing model implementations "
"and create new files for the new models. For most models, you should be able "
"to find a similar model to start with (e.g., starting from Llama)."
msgstr ""

#: ../../../references/supported_models.md:55
msgid "Test the correctness"
msgstr ""

#: ../../../references/supported_models.md:57
msgid "Interactive debugging"
msgstr ""

#: ../../../references/supported_models.md:58
msgid ""
"For interactive debugging, you can compare the outputs of huggingface/"
"transformers and SGLang. The following two commands should give the same "
"text output and very similar prefill logits."
msgstr ""

#: ../../../references/supported_models.md:61
msgid ""
"Get the reference output by `python3 scripts/playground/reference_hf.py --"
"model [new model]`"
msgstr ""

#: ../../../references/supported_models.md:62
msgid ""
"Get the SGLang output by `python3 -m sglang.bench_one_batch --correct --"
"model [new model]`"
msgstr ""

#: ../../../references/supported_models.md:64
msgid "Add the model to the test suite"
msgstr ""

#: ../../../references/supported_models.md:65
msgid ""
"To make sure the new model is well maintained in the future, it is better to "
"add it to the test suite. You can add it to the `ALL_OTHER_MODELS` list in "
"the [test_generation_models.py](https://github.com/sgl-project/sglang/blob/"
"main/test/srt/models/test_generation_models.py) and run the following "
"command to test it."
msgstr ""

#: ../../../references/supported_models.md:68
msgid "For example, if the model is Qwen/Qwen2-1.5B"
msgstr ""

#: ../../../references/supported_models.md:69
msgid ""
"ONLY_RUN=Qwen/Qwen2-1.5B python3 -m unittest test_generation_models."
"TestGenerationModels.test_others\n"
msgstr ""

#: ../../../references/supported_models.md:73
msgid "Port a model from vLLM to SGLang"
msgstr ""

#: ../../../references/supported_models.md:74
msgid ""
"Another valuable resource is the [vLLM Models Directory](https://github.com/"
"vllm-project/vllm/tree/main/vllm/model_executor/models). vLLM has extensive "
"coverage of models, and SGLang reuses vLLM's interface and some layers to "
"implement the models. This similarity makes it easy to port many models from "
"vLLM to SGLang."
msgstr ""

#: ../../../references/supported_models.md:76
msgid ""
"To port a model from vLLM to SGLang, you can compare these two files [SGLang "
"Llama Implementation](https://github.com/sgl-project/sglang/blob/main/python/"
"sglang/srt/models/llama.py) and [vLLM Llama Implementation](https://github."
"com/vllm-project/vllm/blob/main/vllm/model_executor/models/llama.py). This "
"comparison will help you understand how to convert a model implementation "
"from vLLM to SGLang. The major difference is the replacement of Attention "
"with RadixAttention. The other parts are almost identical. Specifically,"
msgstr ""

#: ../../../references/supported_models.md:77
msgid ""
"Replace vllm's `Attention` with `RadixAttention`. Note that you need to pass "
"`layer_id` all the way to `RadixAttention`."
msgstr ""

#: ../../../references/supported_models.md:78
msgid "Replace vllm's `LogitsProcessor` with SGLang's `LogitsProcessor`."
msgstr ""

#: ../../../references/supported_models.md:79
msgid ""
"Replace other vLLM layers with SGLang layers (e.g., `RMSNorm`, `SiluAndMul`)."
msgstr ""

#: ../../../references/supported_models.md:80
msgid "Remove `Sample`."
msgstr ""

#: ../../../references/supported_models.md:81
msgid "Change `forward()` functions, and add `forward_batch`."
msgstr ""

#: ../../../references/supported_models.md:82
msgid "Add `EntryClass` at the end."
msgstr ""
