# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2024, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.3\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../start/install.md:1
msgid "Install SGLang"
msgstr "安裝 SGLang"

#: ../../../start/install.md:3
msgid "You can install SGLang using any of the methods below."
msgstr ""

#: ../../../start/install.md:5
msgid "Method 1: With pip"
msgstr ""

#: ../../../start/install.md:6
msgid ""
"pip install --upgrade pip\n"
"pip install \"sglang[all]\"\n"
"\n"
"# Install FlashInfer accelerated kernels (CUDA only for now)\n"
"pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/\n"
msgstr ""

#: ../../../start/install.md:14 ../../../start/install.md:29
msgid ""
"Note: Please check the [FlashInfer installation doc](https://docs.flashinfer."
"ai/installation.html) to install the proper version according to your "
"PyTorch and CUDA versions."
msgstr ""

#: ../../../start/install.md:16
msgid "Method 2: From source"
msgstr ""

#: ../../../start/install.md:17
msgid ""
"# Use the last release branch\n"
"git clone -b v0.3.6 https://github.com/sgl-project/sglang.git\n"
"cd sglang\n"
"\n"
"pip install --upgrade pip\n"
"pip install -e \"python[all]\"\n"
"\n"
"# Install FlashInfer accelerated kernels (CUDA only for now)\n"
"pip install flashinfer -i https://flashinfer.ai/whl/cu121/torch2.4/\n"
msgstr ""

#: ../../../start/install.md:31
msgid "Method 3: Using docker"
msgstr "方法三：使用 docker"

#: ../../../start/install.md:32
msgid ""
"The docker images are available on Docker Hub as [lmsysorg/sglang](https://"
"hub.docker.com/r/lmsysorg/sglang/tags), built from [Dockerfile](https://"
"github.com/sgl-project/sglang/tree/main/docker). Replace `<secret>` below "
"with your huggingface hub [token](https://huggingface.co/docs/hub/en/"
"security-tokens)."
msgstr ""

#: ../../../start/install.md:35
msgid ""
"docker run --gpus all \\\n"
"    --shm-size 32g \\\n"
"    -p 30000:30000 \\\n"
"    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n"
"    --env \"HF_TOKEN=<secret>\" \\\n"
"    --ipc=host \\\n"
"    lmsysorg/sglang:latest \\\n"
"    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-"
"Instruct --host 0.0.0.0 --port 30000\n"
msgstr ""

#: ../../../start/install.md:46
msgid ""
"Note: To AMD ROCm system with Instinct/MI GPUs, it is recommended to use "
"`docker/Dockerfile.rocm` to build images, example and usage as below:"
msgstr ""

#: ../../../start/install.md:48
msgid ""
"docker build --build-arg SGL_BRANCH=v0.3.6 -t v0.3.6-rocm620 -f Dockerfile."
"rocm .\n"
"\n"
"alias drun='docker run -it --rm --network=host --device=/dev/kfd --device=/"
"dev/dri --ipc=host \\\n"
"    --shm-size 16G --group-add video --cap-add=SYS_PTRACE --security-opt "
"seccomp=unconfined \\\n"
"    -v $HOME/dockerx:/dockerx -v /data:/data'\n"
"\n"
"drun -p 30000:30000 \\\n"
"    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n"
"    --env \"HF_TOKEN=<secret>\" \\\n"
"    v0.3.6-rocm620 \\\n"
"    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-"
"Instruct --host 0.0.0.0 --port 30000\n"
"\n"
"# Till flashinfer backend available, --attention-backend triton --sampling-"
"backend pytorch are set by default\n"
"drun v0.3.6-rocm620 python3 -m sglang.bench_one_batch --batch-size 32 --"
"input 1024 --output 128 --model amd/Meta-Llama-3.1-8B-Instruct-FP8-KV --tp 8 "
"--quantization fp8\n"
msgstr ""

#: ../../../start/install.md:65
msgid "Method 4: Using docker compose"
msgstr ""

#: ../../../start/install.md:67 ../../../start/install.md:79
msgid ""
"<details>\n"
"<summary>More</summary>\n"
msgstr ""

#: ../../../start/install.md:70
msgid ""
"This method is recommended if you plan to serve it as a service. A better "
"approach is to use the [k8s-sglang-service.yaml](https://github.com/sgl-"
"project/sglang/blob/main/docker/k8s-sglang-service.yaml)."
msgstr ""

#: ../../../start/install.md:73
msgid ""
"Copy the [compose.yml](https://github.com/sgl-project/sglang/blob/main/"
"docker/compose.yaml) to your local machine"
msgstr ""

#: ../../../start/install.md:74
msgid "Execute the command `docker compose up -d` in your terminal."
msgstr ""

#: ../../../start/install.md:75 ../../../start/install.md:106
#: ../../../start/install.md:116
msgid "</details>\n"
msgstr ""

#: ../../../start/install.md:77
msgid "Method 5: Run on Kubernetes or Clouds with SkyPilot"
msgstr ""

#: ../../../start/install.md:82
msgid ""
"To deploy on Kubernetes or 12+ clouds, you can use [SkyPilot](https://github."
"com/skypilot-org/skypilot)."
msgstr ""

#: ../../../start/install.md:84
msgid ""
"Install SkyPilot and set up Kubernetes cluster or cloud access: see "
"[SkyPilot's documentation](https://skypilot.readthedocs.io/en/latest/getting-"
"started/installation.html)."
msgstr ""

#: ../../../start/install.md:85
msgid ""
"Deploy on your own infra with a single command and get the HTTP API endpoint:"
msgstr ""

#: ../../../start/install.md:86
msgid ""
"<details>\n"
"<summary>SkyPilot YAML: <code>sglang.yaml</code></summary>\n"
msgstr ""

#: ../../../start/install.md:89
msgid ""
"# sglang.yaml\n"
"envs:\n"
"  HF_TOKEN: null\n"
"\n"
"resources:\n"
"  image_id: docker:lmsysorg/sglang:latest\n"
"  accelerators: A100\n"
"  ports: 30000\n"
"\n"
"run: |\n"
"  conda deactivate\n"
"  python3 -m sglang.launch_server \\\n"
"    --model-path meta-llama/Llama-3.1-8B-Instruct \\\n"
"    --host 0.0.0.0 \\\n"
"    --port 30000\n"
msgstr ""

#: ../../../start/install.md:108
msgid ""
"# Deploy on any cloud or Kubernetes cluster. Use --cloud <cloud> to select a "
"specific cloud provider.\n"
"HF_TOKEN=<secret> sky launch -c sglang --env HF_TOKEN sglang.yaml\n"
"\n"
"# Get the HTTP API endpoint\n"
"sky status --endpoint 30000 sglang\n"
msgstr ""

#: ../../../start/install.md:115
msgid ""
"To further scale up your deployment with autoscaling and failure recovery, "
"check out the [SkyServe + SGLang guide](https://github.com/skypilot-org/"
"skypilot/tree/master/llm/sglang#serving-llama-2-with-sglang-for-more-traffic-"
"using-skyserve)."
msgstr ""

#: ../../../start/install.md:118
msgid "Common Notes"
msgstr ""

#: ../../../start/install.md:119
msgid ""
"[FlashInfer](https://github.com/flashinfer-ai/flashinfer) is the default "
"attention kernel backend. It only supports sm75 and above. If you encounter "
"any FlashInfer-related issues on sm75+ devices (e.g., T4, A10, A100, L4, "
"L40S, H100), please switch to other kernels by adding `--attention-backend "
"triton --sampling-backend pytorch` and open an issue on GitHub."
msgstr ""

#: ../../../start/install.md:120
msgid ""
"If you only need to use OpenAI models with the frontend language, you can "
"avoid installing other dependencies by using `pip install "
"\"sglang[openai]\"`."
msgstr ""

#: ../../../start/install.md:121
msgid ""
"The language frontend operates independently of the backend runtime. You can "
"install the frontend locally without needing a GPU, while the backend can be "
"set up on a GPU-enabled machine. To install the frontend, run `pip install "
"sglang`, and for the backend, use `pip install sglang[srt]`. This allows you "
"to build SGLang programs locally and execute them by connecting to the "
"remote backend."
msgstr ""
