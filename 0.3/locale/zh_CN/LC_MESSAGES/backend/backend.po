# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2024, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.3\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../backend/backend.md:1
msgid "Backend: SGLang Runtime (SRT)"
msgstr ""

#: ../../../backend/backend.md:2
msgid "The SGLang Runtime (SRT) is an efficient serving engine."
msgstr ""

#: ../../../backend/backend.md:4
msgid "Quick Start"
msgstr ""

#: ../../../backend/backend.md:5
msgid "Launch a server"
msgstr ""

#: ../../../backend/backend.md:6
msgid ""
"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-"
"Instruct --port 30000\n"
msgstr ""

#: ../../../backend/backend.md:10
msgid "Send a request"
msgstr ""

#: ../../../backend/backend.md:11
msgid ""
"curl http://localhost:30000/generate \\\n"
"  -H \"Content-Type: application/json\" \\\n"
"  -d '{\n"
"    \"text\": \"Once upon a time,\",\n"
"    \"sampling_params\": {\n"
"      \"max_new_tokens\": 16,\n"
"      \"temperature\": 0\n"
"    }\n"
"  }'\n"
msgstr ""

#: ../../../backend/backend.md:23
msgid ""
"Learn more about the argument specification, streaming, and multi-modal "
"support [here](../references/sampling_params.md)."
msgstr ""

#: ../../../backend/backend.md:25
msgid "OpenAI Compatible API"
msgstr ""

#: ../../../backend/backend.md:26
msgid "In addition, the server supports OpenAI-compatible APIs."
msgstr ""

#: ../../../backend/backend.md:28
msgid ""
"import openai\n"
"client = openai.Client(\n"
"    base_url=\"http://127.0.0.1:30000/v1\", api_key=\"EMPTY\")\n"
"\n"
"# Text completion\n"
"response = client.completions.create(\n"
"\tmodel=\"default\",\n"
"\tprompt=\"The capital of France is\",\n"
"\ttemperature=0,\n"
"\tmax_tokens=32,\n"
")\n"
"print(response)\n"
"\n"
"# Chat completion\n"
"response = client.chat.completions.create(\n"
"    model=\"default\",\n"
"    messages=[\n"
"        {\"role\": \"system\", \"content\": \"You are a helpful AI "
"assistant\"},\n"
"        {\"role\": \"user\", \"content\": \"List 3 countries and their "
"capitals.\"},\n"
"    ],\n"
"    temperature=0,\n"
"    max_tokens=64,\n"
")\n"
"print(response)\n"
"\n"
"# Text embedding\n"
"response = client.embeddings.create(\n"
"    model=\"default\",\n"
"    input=\"How are you today\",\n"
")\n"
"print(response)\n"
msgstr ""

#: ../../../backend/backend.md:62
msgid ""
"It supports streaming, vision, and almost all features of the Chat/"
"Completions/Models/Batch endpoints specified by the [OpenAI API Reference]"
"(https://platform.openai.com/docs/api-reference/)."
msgstr ""

#: ../../../backend/backend.md:64
msgid "Additional Server Arguments"
msgstr ""

#: ../../../backend/backend.md:65
msgid ""
"To enable multi-GPU tensor parallelism, add `--tp 2`. If it reports the "
"error \"peer access is not supported between these two devices\", add `--"
"enable-p2p-check` to the server launch command."
msgstr ""

#: ../../../backend/backend.md:66
msgid ""
"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-"
"Instruct --tp 2\n"
msgstr ""

#: ../../../backend/backend.md:69
msgid ""
"To enable multi-GPU data parallelism, add `--dp 2`. Data parallelism is "
"better for throughput if there is enough memory. It can also be used "
"together with tensor parallelism. The following command uses 4 GPUs in total."
msgstr ""

#: ../../../backend/backend.md:70
msgid ""
"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-"
"Instruct --dp 2 --tp 2\n"
msgstr ""

#: ../../../backend/backend.md:73
msgid ""
"If you see out-of-memory errors during serving, try to reduce the memory "
"usage of the KV cache pool by setting a smaller value of `--mem-fraction-"
"static`. The default value is `0.9`."
msgstr ""

#: ../../../backend/backend.md:74
msgid ""
"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-"
"Instruct --mem-fraction-static 0.7\n"
msgstr ""

#: ../../../backend/backend.md:77
msgid ""
"See [hyperparameter tuning](../references/hyperparameter_tuning.md) on "
"tuning hyperparameters for better performance."
msgstr ""

#: ../../../backend/backend.md:78
msgid ""
"If you see out-of-memory errors during prefill for long prompts, try to set "
"a smaller chunked prefill size."
msgstr ""

#: ../../../backend/backend.md:79
msgid ""
"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-"
"Instruct --chunked-prefill-size 4096\n"
msgstr ""

#: ../../../backend/backend.md:82
msgid ""
"To enable the experimental overlapped scheduler, add `--enable-overlap-"
"schedule`. It overlaps CPU scheduler with GPU computation and can accelerate "
"almost all workloads. This does not work for constrained decoding currently."
msgstr ""

#: ../../../backend/backend.md:83
msgid ""
"To enable torch.compile acceleration, add `--enable-torch-compile`. It "
"accelerates small models on small batch sizes. This does not work for FP8 "
"currently."
msgstr ""

#: ../../../backend/backend.md:84
msgid ""
"To enable torchao quantization, add `--torchao-config int4wo-128`. It "
"supports various quantization strategies."
msgstr ""

#: ../../../backend/backend.md:85
msgid ""
"To enable fp8 weight quantization, add `--quantization fp8` on a fp16 "
"checkpoint or directly load a fp8 checkpoint without specifying any "
"arguments."
msgstr ""

#: ../../../backend/backend.md:86
msgid "To enable fp8 kv cache quantization, add `--kv-cache-dtype fp8_e5m2`."
msgstr ""

#: ../../../backend/backend.md:87
msgid ""
"If the model does not have a chat template in the Hugging Face tokenizer, "
"you can specify a [custom chat template](../references/custom_chat_template."
"md)."
msgstr ""

#: ../../../backend/backend.md:89
msgid ""
"To run tensor parallelism on multiple nodes, add `--nnodes 2`. If you have "
"two nodes with two GPUs on each node and want to run TP=4, let `sgl-dev-0` "
"be the hostname of the first node and `50000` be an available port, you can "
"use the following commands. If you meet deadlock, please try to add `--"
"disable-cuda-graph`"
msgstr ""

#: ../../../backend/backend.md:90
msgid ""
"# Node 0\n"
"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-"
"Instruct --tp 4 --nccl-init sgl-dev-0:50000 --nnodes 2 --node-rank 0\n"
"\n"
"# Node 1\n"
"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-"
"Instruct --tp 4 --nccl-init sgl-dev-0:50000 --nnodes 2 --node-rank 1\n"
msgstr ""

#: ../../../backend/backend.md:98
msgid "Engine Without HTTP Server"
msgstr ""

#: ../../../backend/backend.md:100
msgid ""
"We also provide an inference engine **without a HTTP server**. For example,"
msgstr ""

#: ../../../backend/backend.md:102
msgid ""
"import sglang as sgl\n"
"\n"
"def main():\n"
"    prompts = [\n"
"        \"Hello, my name is\",\n"
"        \"The president of the United States is\",\n"
"        \"The capital of France is\",\n"
"        \"The future of AI is\",\n"
"    ]\n"
"    sampling_params = {\"temperature\": 0.8, \"top_p\": 0.95}\n"
"    llm = sgl.Engine(model_path=\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n"
"\n"
"    outputs = llm.generate(prompts, sampling_params)\n"
"    for prompt, output in zip(prompts, outputs):\n"
"        print(\"===============================\")\n"
"        print(f\"Prompt: {prompt}\\nGenerated text: {output['text']}\")\n"
"\n"
"if __name__ == \"__main__\":\n"
"    main()\n"
msgstr ""

#: ../../../backend/backend.md:124
msgid ""
"This can be used for offline batch inference and building custom servers. "
"You can view the full example [here](https://github.com/sgl-project/sglang/"
"tree/main/examples/runtime/engine)."
msgstr ""

#: ../../../backend/backend.md:127
msgid "Use Models From ModelScope"
msgstr ""

#: ../../../backend/backend.md:128 ../../../backend/backend.md:154
msgid ""
"<details>\n"
"<summary>More</summary>\n"
msgstr ""

#: ../../../backend/backend.md:131
msgid ""
"To use a model from [ModelScope](https://www.modelscope.cn), set the "
"environment variable SGLANG_USE_MODELSCOPE."
msgstr ""

#: ../../../backend/backend.md:132
msgid "export SGLANG_USE_MODELSCOPE=true\n"
msgstr ""

#: ../../../backend/backend.md:135
msgid ""
"Launch [Qwen2-7B-Instruct](https://www.modelscope.cn/models/qwen/qwen2-7b-"
"instruct) Server"
msgstr ""

#: ../../../backend/backend.md:136
msgid ""
"SGLANG_USE_MODELSCOPE=true python -m sglang.launch_server --model-path qwen/"
"Qwen2-7B-Instruct --port 30000\n"
msgstr ""

#: ../../../backend/backend.md:140
msgid "Or start it by docker."
msgstr ""

#: ../../../backend/backend.md:141
msgid ""
"docker run --gpus all \\\n"
"    -p 30000:30000 \\\n"
"    -v ~/.cache/modelscope:/root/.cache/modelscope \\\n"
"    --env \"SGLANG_USE_MODELSCOPE=true\" \\\n"
"    --ipc=host \\\n"
"    lmsysorg/sglang:latest \\\n"
"    python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-7B-Instruct --"
"host 0.0.0.0 --port 30000\n"
msgstr ""

#: ../../../backend/backend.md:151 ../../../backend/backend.md:169
msgid "</details>\n"
msgstr ""

#: ../../../backend/backend.md:153
msgid "Example: Run Llama 3.1 405B"
msgstr ""

#: ../../../backend/backend.md:157
msgid ""
"# Run 405B (fp8) on a single node\n"
"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-405B-"
"Instruct-FP8 --tp 8\n"
"\n"
"# Run 405B (fp16) on two nodes\n"
"## on the first node, replace the `172.16.4.52:20000` with your own first "
"node ip address and port\n"
"GLOO_SOCKET_IFNAME=eth0 python3 -m sglang.launch_server --model-path meta-"
"llama/Meta-Llama-3.1-405B-Instruct --tp 16 --nccl-init-addr "
"172.16.4.52:20000 --nnodes 2 --node-rank 0 --disable-cuda-graph\n"
"\n"
"## on the first node, replace the `172.16.4.52:20000` with your own first "
"node ip address and port\n"
"GLOO_SOCKET_IFNAME=eth0 python3 -m sglang.launch_server --model-path meta-"
"llama/Meta-Llama-3.1-405B-Instruct --tp 16 --nccl-init-addr "
"172.16.4.52:20000 --nnodes 2 --node-rank 1 --disable-cuda-graph\n"
msgstr ""
