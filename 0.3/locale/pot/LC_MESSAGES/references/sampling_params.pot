# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2024, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.3\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../references/sampling_params.md:1
msgid "Sampling Parameters in SGLang Runtime"
msgstr ""

#: ../../../references/sampling_params.md:2
msgid ""
"This doc describes the sampling parameters of the SGLang Runtime. It is the "
"low-level endpoint of the runtime. If you want a high-level endpoint that "
"can automatically handle chat templates, consider using the [OpenAI "
"Compatible API ](https://github.com/sgl-project/sglang?tab=readme-ov-"
"file#openai-compatible-api)."
msgstr ""

#: ../../../references/sampling_params.md:7
msgid ""
"The `/generate` endpoint accepts the following arguments in the JSON format."
msgstr ""

#: ../../../references/sampling_params.md:9
msgid ""
"@dataclass\n"
"class GenerateReqInput:\n"
"    # The input prompt. It can be a single prompt or a batch of prompts.\n"
"    text: Optional[Union[List[str], str]] = None\n"
"    # The token ids for text; one can either specify text or input_ids.\n"
"    input_ids: Optional[Union[List[List[int]], List[int]]] = None\n"
"    # The image input. It can be a file name, a url, or base64 encoded "
"string.\n"
"    # See also python/sglang/srt/utils.py:load_image.\n"
"    image_data: Optional[Union[List[str], str]] = None\n"
"    # The sampling_params. See descriptions below.\n"
"    sampling_params: Union[List[Dict], Dict] = None\n"
"    # The request id.\n"
"    rid: Optional[Union[List[str], str]] = None\n"
"    # Whether to return logprobs.\n"
"    return_logprob: Optional[Union[List[bool], bool]] = None\n"
"    # The start location of the prompt for return_logprob.\n"
"    # By default, this value is \"-1\", which means it will only return "
"logprobs for output tokens.\n"
"    logprob_start_len: Optional[Union[List[int], int]] = None\n"
"    # The number of top logprobs to return.\n"
"    top_logprobs_num: Optional[Union[List[int], int]] = None\n"
"    # Whether to detokenize tokens in text in the returned logprobs.\n"
"    return_text_in_logprobs: bool = False\n"
"    # Whether to stream output.\n"
"    stream: bool = False\n"
msgstr ""

#: ../../../references/sampling_params.md:36
msgid "The `sampling_params` follows this format"
msgstr ""

#: ../../../references/sampling_params.md:38
msgid ""
"# The maximum number of output tokens\n"
"max_new_tokens: int = 128,\n"
"# Stop when hitting any of the strings in this list.\n"
"stop: Optional[Union[str, List[str]]] = None,\n"
"# Stop when hitting any of the token_ids in this list. Could be useful when "
"mixed with\n"
"# `min_new_tokens`.\n"
"stop_token_ids: Optional[List[int]] = [],\n"
"# Sampling temperature\n"
"temperature: float = 1.0,\n"
"# Top-p sampling\n"
"top_p: float = 1.0,\n"
"# Top-k sampling\n"
"top_k: int = -1,\n"
"# Min-p sampling\n"
"min_p: float = 0.0,\n"
"# Whether to ignore EOS token.\n"
"ignore_eos: bool = False,\n"
"# Whether to skip the special tokens during detokenization.\n"
"skip_special_tokens: bool = True,\n"
"# Whether to add spaces between special tokens during detokenization.\n"
"spaces_between_special_tokens: bool = True,\n"
"# Constrains the output to follow a given regular expression.\n"
"regex: Optional[str] = None,\n"
"# Do parallel sampling and return `n` outputs.\n"
"n: int = 1,\n"
"# Constrains the output to follow a given JSON schema.\n"
"# `regex` and `json_schema` cannot be set at the same time.\n"
"json_schema: Optional[str] = None,\n"
"\n"
"## Penalties. See [Performance Implications on Penalties] section below for "
"more informations.\n"
"\n"
"# Float that penalizes new tokens based on their frequency in the generated "
"text so far.\n"
"# Values > 0 encourage the model to use new tokens, while values < 0 "
"encourage the model to\n"
"# repeat tokens. Must be -2 <= value <= 2. Setting to 0 (default) will "
"disable this penalty.\n"
"frequency_penalty: float = 0.0,\n"
"# Float that penalizes new tokens based on whether they appear in the "
"generated text so far.\n"
"# Values > 0 encourage the model to use new tokens, while values < 0 "
"encourage the model to repeat\n"
"# tokens. Must be -2 <= value <= 2. Setting to 0 (default) will disable this "
"penalty.\n"
"presence_penalty: float = 0.0,\n"
"# Float that penalizes new tokens based on whether they appear in the prompt "
"and the generated text\n"
"# so far. Values > 1 encourage the model to use new tokens, while values < 1 "
"encourage the model to\n"
"# repeat tokens. Must be 0 <= value <= 2. Setting to 1 (default) will "
"disable this penalty.\n"
"repetition_penalty: float = 1.0,\n"
"# Guides inference to generate at least this number of tokens by penalizing "
"logits of tokenizer's\n"
"# EOS token and `stop_token_ids` to -inf, until the output token reaches "
"given length.\n"
"# Note that any of the `stop` string can be generated before reaching "
"`min_new_tokens`, as it is\n"
"# difficult to infer the correct token ID by given `stop` strings.\n"
"# Must be 0 <= value < max_new_tokens. Setting to 0 (default) will disable "
"this penalty.\n"
"min_new_tokens: int = 0,\n"
msgstr ""

#: ../../../references/sampling_params.md:90
msgid "Examples"
msgstr ""

#: ../../../references/sampling_params.md:92
msgid "Normal"
msgstr ""

#: ../../../references/sampling_params.md:93
#: ../../../references/sampling_params.md:148
msgid "Launch a server"
msgstr ""

#: ../../../references/sampling_params.md:94
msgid ""
"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-"
"Instruct --port 30000\n"
msgstr ""

#: ../../../references/sampling_params.md:98
#: ../../../references/sampling_params.md:158
msgid "Send a request"
msgstr ""

#: ../../../references/sampling_params.md:99
msgid ""
"import requests\n"
"\n"
"response = requests.post(\n"
"    \"http://localhost:30000/generate\",\n"
"    json={\n"
"        \"text\": \"The capital of France is\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 32,\n"
"        },\n"
"    },\n"
")\n"
"print(response.json())\n"
msgstr ""

#: ../../../references/sampling_params.md:115
msgid "Streaming"
msgstr ""

#: ../../../references/sampling_params.md:116
msgid "Send a request and stream the output"
msgstr ""

#: ../../../references/sampling_params.md:117
msgid ""
"import requests, json\n"
"\n"
"response = requests.post(\n"
"    \"http://localhost:30000/generate\",\n"
"    json={\n"
"        \"text\": \"The capital of France is\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 32,\n"
"        },\n"
"        \"stream\": True,\n"
"    },\n"
"    stream=True,\n"
")\n"
"\n"
"prev = 0\n"
"for chunk in response.iter_lines(decode_unicode=False):\n"
"    chunk = chunk.decode(\"utf-8\")\n"
"    if chunk and chunk.startswith(\"data:\"):\n"
"        if chunk == \"data: [DONE]\":\n"
"            break\n"
"        data = json.loads(chunk[5:].strip(\"\\n\"))\n"
"        output = data[\"text\"].strip()\n"
"        print(output[prev:], end=\"\", flush=True)\n"
"        prev = len(output)\n"
"print(\"\")\n"
msgstr ""

#: ../../../references/sampling_params.md:146
msgid "Multi modal"
msgstr ""

#: ../../../references/sampling_params.md:149
msgid ""
"python3 -m sglang.launch_server --model-path lmms-lab/llava-onevision-"
"qwen2-7b-ov --chat-template chatml-llava\n"
msgstr ""

#: ../../../references/sampling_params.md:153
msgid "Download an image"
msgstr ""

#: ../../../references/sampling_params.md:154
msgid ""
"curl -o example_image.png -L https://github.com/sgl-project/sglang/blob/main/"
"test/lang/example_image.png?raw=true\n"
msgstr ""

#: ../../../references/sampling_params.md:159
msgid ""
"import requests\n"
"\n"
"response = requests.post(\n"
"    \"http://localhost:30000/generate\",\n"
"    json={\n"
"        \"text\": \"<|im_start|>system\\nYou are a helpful assistant.<|"
"im_end|>\\n\"\n"
"                \"<|im_start|>user\\n<image>\\nDescribe this image in a very "
"short sentence.<|im_end|>\\n\"\n"
"                \"<|im_start|>assistant\\n\",\n"
"        \"image_data\": \"example_image.png\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 32,\n"
"        },\n"
"    },\n"
")\n"
"print(response.json())\n"
msgstr ""

#: ../../../references/sampling_params.md:178
msgid ""
"The `image_data` can be a file name, a URL, or a base64 encoded string. See "
"also `python/sglang/srt/utils.py:load_image`. Streaming is supported in a "
"similar manner as [above](#streaming)."
msgstr ""

#: ../../../references/sampling_params.md:181
msgid "Structured decoding (JSON, Regex)"
msgstr ""

#: ../../../references/sampling_params.md:182
msgid ""
"You can specify a JSON schema or a regular expression to constrain the model "
"output. The model output will be guaranteed to follow the given constraints."
msgstr ""

#: ../../../references/sampling_params.md:184
msgid ""
"import json\n"
"import requests\n"
"\n"
"json_schema = json.dumps(\n"
"    {\n"
"        \"type\": \"object\",\n"
"        \"properties\": {\n"
"            \"name\": {\"type\": \"string\", \"pattern\": \"^[\\\\w]+$\"},\n"
"            \"population\": {\"type\": \"integer\"},\n"
"        },\n"
"        \"required\": [\"name\", \"population\"],\n"
"    }\n"
")\n"
"\n"
"# JSON\n"
"response = requests.post(\n"
"    \"http://localhost:30000/generate\",\n"
"    json={\n"
"        \"text\": \"Here is the information of the capital of France in the "
"JSON format.\\n\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 64,\n"
"            \"json_schema\": json_schema,\n"
"        },\n"
"    },\n"
")\n"
"print(response.json())\n"
"\n"
"# Regular expression\n"
"response = requests.post(\n"
"    \"http://localhost:30000/generate\",\n"
"    json={\n"
"        \"text\": \"Paris is the capital of\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 64,\n"
"            \"regex\": \"(France|England)\",\n"
"        },\n"
"    },\n"
")\n"
"print(response.json())\n"
msgstr ""
