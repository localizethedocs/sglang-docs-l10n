# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-22 08:33+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../basic_usage/deepseek_v32.md:1
msgid "DeepSeek V3.2 Usage"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:3
msgid ""
"DeepSeek-V3.2 model family equips DeepSeek-V3.1-Terminus with DeepSeek "
"Sparse Attention (DSA) through continued training. With DSA, a fine-grained "
"sparse attention mechanism powered by a lightning indexer, DeepSeek-V3.2 "
"achieves efficiency improvements in long-context scenarios."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:5
msgid ""
"For reporting issues or tracking upcoming features, please refer to this "
"[Roadmap](https://github.com/sgl-project/sglang/issues/11060)."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:7
msgid ""
"Note: This document is originally written for the usage of [DeepSeek-V3.2-"
"Exp](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp) model. The usage "
"of [DeepSeek-V3.2](https://huggingface.co/deepseek-ai/DeepSeek-V3.2) or "
"[DeepSeek-V3.2-Speciale](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-"
"Speciale) is the same as DeepSeek-V3.2-Exp except for the tool call parser."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:10
msgid "Installation"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:12
msgid "Docker"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:14
msgid ""
"# H200/B200\n"
"docker pull lmsysorg/sglang:latest\n"
"\n"
"# MI350/MI355\n"
"docker pull lmsysorg/sglang:dsv32-rocm\n"
"\n"
"# NPUs\n"
"docker pull lmsysorg/sglang:dsv32-a2\n"
"docker pull lmsysorg/sglang:dsv32-a3\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:26
msgid "Build From Source"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:28
msgid ""
"# Install SGLang\n"
"git clone https://github.com/sgl-project/sglang\n"
"cd sglang\n"
"pip3 install pip --upgrade\n"
"pip3 install -e \"python\"\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:35
msgid "Launch DeepSeek V3.2 with SGLang"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:37
msgid ""
"To serve [DeepSeek-V3.2-Exp](https://huggingface.co/deepseek-ai/DeepSeek-"
"V3.2-Exp) on 8xH200/B200 GPUs:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:39
msgid ""
"# Launch with TP + DP (Recommended)\n"
"python -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 "
"--dp 8 --enable-dp-attention\n"
"\n"
"# Launch with EP + DP\n"
"python -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 "
"--ep 8 --dp 8 --enable-dp-attention\n"
"\n"
"# Launch with Pure TP\n"
"python -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:50
msgid "Configuration Tips"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:51
msgid ""
"**DP Attention (Recommended)**: For DeepSeek V3.2 model, the kernels are "
"customized for the use case of `dp_size=8`, so DP attention (`--dp 8 --"
"enable-dp-attention`) is the recommended configuration for better stability "
"and performance. All test cases use this configuration by default."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:52
msgid ""
"**Pure TP Mode**: Launching with pure TP (without `--dp` and `--enable-dp-"
"attention`) is also supported. Note that this mode has not been fully "
"validated in PD disaggregation scenarios."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:53
msgid ""
"**Short-sequence MHA prefill (adaptive)**: For short prefill sequences "
"(default threshold: **2048 tokens**), the NSA backend uses standard MHA "
"automatically (no extra flags). On H200 (SM90) this path uses the "
"FlashAttention variable-length kernel; on B200 (SM100) it uses TRT-LLM "
"ragged MHA. MHA uses `MHA_ONE_SHOT` for best performance. `MHA_ONE_SHOT` "
"computes multi-head attention over all tokens (both cached prefix and newly "
"extended tokens) in a single kernel invocation, avoiding the overhead of "
"chunked KV cache processing. This achieves optimal throughput for short "
"sequences where total sequence length fits within the chunk capacity limit."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:54
msgid ""
"**Choices of Attention Kernels**: The attention backend is automatically set "
"to `nsa` attention backend for DeepSeek V3.2 model. In this backend, "
"different kernels for sparse prefilling/decoding are implemented, which can "
"be specified by `--nsa-prefill-backend` and `--nsa-decode-backend` server "
"arguments. The choices of nsa prefill/decode attention kernels include:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:55
msgid ""
"`flashmla_sparse`: `flash_mla_sparse_fwd` kernel from `flash_mla` library. "
"Can run on both Hopper and Blackwell GPUs. It requires bf16 q, kv inputs."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:56
msgid ""
"`flashmla_kv`: `flash_mla_with_kvcache` kernel from `flash_mla` library. Can "
"run on both Hopper and Blackwell GPUs. It requires bf16 q, fp8 k_cache "
"inputs."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:57
msgid ""
"`fa3`: `flash_attn_with_kvcache` kernel from `flash_attn` library. Can only "
"run on Hopper GPUs. It requires bf16 q, kv inputs."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:58
msgid "`tilelang`: `tilelang` implementation that can run on GPU, HPU and NPU."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:59
msgid "`aiter`: Aiter kernel on AMD HPUs. Can only be used as decode kernel."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:60
msgid ""
"On the basis of performance benchmarks, the default configuration on H200 "
"and B200 are set as follows :"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:61
msgid ""
"H200: `flashmla_sparse` prefill attention (short-seq prefill uses MHA via "
"FlashAttention varlen), `fa3` decode attention, `bf16` kv cache dtype."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:62
msgid ""
"B200: `flashmla_auto` prefill attention (short-seq prefill uses MHA via TRT-"
"LLM ragged), `flashmla_kv` decode attention, `fp8_e4m3` kv cache dtype. "
"`flashmla_auto` enables automatic selection of either `flashmla_sparse` or "
"`flashmla_kv` kernel for prefill based on KV cache dtype, hardware, and "
"heuristics. When FP8 KV cache is enabled and `total_kv_tokens < "
"total_q_tokens * 512`, it uses the `flashmla_sparse` kernel; otherwise, it "
"falls back to the `flashmla_kv` kernel. The heuristics may need to be tuned "
"if the performance of either the `flashmla_sparse` or `flashmla_kv` kernel "
"changes significantly."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:64
msgid "Multi-token Prediction"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:65
msgid ""
"SGLang implements Multi-Token Prediction (MTP) for DeepSeek V3.2 based on "
"[EAGLE speculative decoding](https://docs.sglang.io/advanced_features/"
"speculative_decoding.html#EAGLE-Decoding). With this optimization, the "
"decoding speed can be improved significantly on small batch sizes. Please "
"look at [this PR](https://github.com/sgl-project/sglang/pull/11652) for more "
"information."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:67
msgid "Example usage with DP Attention:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:68
msgid ""
"python -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 "
"--dp 8 --enable-dp-attention --speculative-algorithm EAGLE --speculative-num-"
"steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:72
msgid "Example usage with Pure TP:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:73
msgid ""
"python -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 "
"--speculative-algorithm EAGLE --speculative-num-steps 3 --speculative-eagle-"
"topk 1 --speculative-num-draft-tokens 4\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:77
msgid ""
"The best configuration for `--speculative-num-steps`, `--speculative-eagle-"
"topk` and `--speculative-num-draft-tokens` can be searched with "
"[bench_speculative.py](https://github.com/sgl-project/sglang/blob/main/"
"scripts/playground/bench_speculative.py) script for given batch size. The "
"minimum configuration is `--speculative-num-steps 1 --speculative-eagle-topk "
"1 --speculative-num-draft-tokens 2`, which can achieve speedup for larger "
"batch sizes."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:78
msgid ""
"The default value of  `--max-running-requests` is set to `48` for MTP. For "
"larger batch sizes, this value should be increased beyond the default value."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:81
msgid ""
"To enable the experimental overlap scheduler for EAGLE speculative decoding, "
"set the environment variable `SGLANG_ENABLE_SPEC_V2=1`. This can improve "
"performance by enabling overlap scheduling between draft and verification "
"stages."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:85
msgid "Function Calling and Reasoning Parser"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:86
msgid ""
"The usage of function calling and reasoning parser is the same as DeepSeek "
"V3.1. Please refer to [Reasoning Parser](https://docs.sglang.io/"
"advanced_features/separate_reasoning.html) and [Tool Parser](https://docs."
"sglang.io/advanced_features/tool_parser.html) documents."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:88
msgid ""
"To launch `DeepSeek-V3.2-Exp` with function calling and reasoning parser:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:89
msgid ""
"Note: It is recommended to specify the chat-template, ensuring that you are "
"within the sglang's root directory."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:90
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model-path deepseek-ai/DeepSeek-V3.2-Exp \\\n"
"  --trust-remote-code \\\n"
"  --tp-size 8 --dp-size 8 --enable-dp-attention \\\n"
"  --tool-call-parser deepseekv31 \\\n"
"  --reasoning-parser deepseek-v3 \\\n"
"  --chat-template ./examples/chat_template/tool_chat_template_deepseekv32."
"jinja\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:100
msgid "To launch `DeepSeek-V3.2` with function calling and reasoning parser:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:101
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model-path deepseek-ai/DeepSeek-V3.2 \\\n"
"  --trust-remote-code \\\n"
"  --tp-size 8 --dp-size 8 --enable-dp-attention \\\n"
"  --tool-call-parser deepseekv32 \\\n"
"  --reasoning-parser deepseek-v3\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:110
msgid ""
"`DeepSeek-V3.2-Speciale` doesn't support tool calling, so can only be "
"launched with reasoning parser:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:111
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model-path deepseek-ai/DeepSeek-V3.2-Speciale \\\n"
"  --trust-remote-code \\\n"
"  --tp-size 8 --dp-size 8 --enable-dp-attention \\\n"
"  --reasoning-parser deepseek-v3\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:120
msgid "PD Disaggregation"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:122
msgid "Prefill Command:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:123
msgid ""
"python -m sglang.launch_server \\\n"
"        --model-path deepseek-ai/DeepSeek-V3.2-Exp \\\n"
"        --disaggregation-mode prefill \\\n"
"        --host $LOCAL_IP \\\n"
"        --port $PORT \\\n"
"        --tp 8 \\\n"
"        --dp 8 \\\n"
"        --enable-dp-attention \\\n"
"        --dist-init-addr ${HOST}:${DIST_PORT} \\\n"
"        --trust-remote-code \\\n"
"        --disaggregation-bootstrap-port 8998 \\\n"
"        --mem-fraction-static 0.9 \\\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:138
msgid "Decode command:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:139
msgid ""
"python -m sglang.launch_server \\\n"
"        --model-path deepseek-ai/DeepSeek-V3.2-Exp \\\n"
"        --disaggregation-mode decode \\\n"
"        --host $LOCAL_IP \\\n"
"        --port $PORT \\\n"
"        --tp 8 \\\n"
"        --dp 8 \\\n"
"        --enable-dp-attention \\\n"
"        --dist-init-addr ${HOST}:${DIST_PORT} \\\n"
"        --trust-remote-code \\\n"
"        --mem-fraction-static 0.9 \\\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:153
msgid "Router command:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:154
msgid ""
"python -m sglang_router.launch_router --pd-disaggregation \\\n"
"  --prefill $PREFILL_ADDR 8998 \\\n"
"  --decode $DECODE_ADDR \\\n"
"  --host 127.0.0.1 \\\n"
"  --port 8000 \\\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:162
msgid ""
"If you need more advanced deployment methods or production-ready deployment "
"methods, such as RBG or LWS-based deployment, please refer to [references/"
"multi_node_deployment/rbg_pd/deepseekv32_pd.md](../references/"
"multi_node_deployment/rbg_pd/deepseekv32_pd.md). Additionally, you can also "
"find startup commands for DeepEP-based EP parallelism in the aforementioned "
"documentation."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:165
msgid "Benchmarking Results"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:167
msgid "Accuracy Test with `gsm8k`"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:168
msgid "A simple accuracy benchmark can be tested with `gsm8k` dataset:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:169
msgid ""
"python3 benchmark/gsm8k/bench_sglang.py --num-shots 8 --num-questions 1319 --"
"parallel 1319\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:173
msgid "The result is 0.956, which matches our expectation:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:174
msgid ""
"Accuracy: 0.956\n"
"Invalid: 0.000\n"
"Latency: 25.109 s\n"
"Output throughput: 5226.235 token/s\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:181
msgid ""
"To test long-context accuracy, run gsm8k with `--num-shots 20`. The results "
"are very close to the 8 shots results:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:182
msgid ""
"Accuracy: 0.956\n"
"Invalid: 0.000\n"
"Latency: 29.545 s\n"
"Output throughput: 4418.617 token/s\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:189
msgid "Accuracy Test with `gpqa-diamond`"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:191
#: ../../../basic_usage/deepseek_v32.md:277
msgid ""
"Accuracy benchmark on long context can be tested on GPQA-diamond dataset "
"with long output tokens and thinking enabled:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:192
msgid ""
"python3 -m sglang.test.run_eval --port 30000 --eval-name gpqa --num-examples "
"198 --max-tokens 120000 --repeat 8 --thinking-mode deepseek-v3\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:196
msgid ""
"The mean accuracy over 8 runs shows 0.797, which matches the number 79.9 in "
"official tech report."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:197
msgid ""
"Repeat: 8, mean: 0.797\n"
"Scores: ['0.808', '0.798', '0.808', '0.798', '0.783', '0.788', '0.803', "
"'0.793']\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:202
msgid "Accuracy Test with `aime 2025`"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:204
msgid ""
"Prepare the environment by installing NeMo-Skills in the docker or your own "
"virtual environment:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:206
msgid ""
"pip install git+https://github.com/NVIDIA/NeMo-Skills.git --ignore-installed "
"blinker\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:210
msgid "Then launch the SGLang server:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:211
msgid ""
"python -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 "
"--dp 8 --enable-dp-attention\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:215
msgid "**For `DeepSeek-V3.2` and `DeepSeek-V3.2-Speciale`**:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:217
msgid ""
"python3 -m sglang.launch_server   --model-path deepseek-ai/DeepSeek-V3.2   --"
"trust-remote-code   --tp-size 8 --dp-size 8 --enable-dp-attention   --tool-"
"call-parser deepseekv32   --reasoning-parser deepseek-v3\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:221
msgid "Run the following script to evaluate AIME 2025:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:222
msgid ""
"#! /bin/bash\n"
"export NEMO_SKILLS_DISABLE_UNCOMMITTED_CHANGES_CHECK=1\n"
"\n"
"ns prepare_data aime25\n"
"\n"
"PORT=30000\n"
"BACKEND=sglang\n"
"MODEL=\"deepseek-ai/DeepSeek-V3.2-Exp\" # Should be changed to the model "
"name\n"
"MODEL_NAME=\"dsv32-fp8\"\n"
"\n"
"echo \"Starting AIME25 evaluation with model $MODEL on port $PORT using "
"backend $BACKEND...\"\n"
"ns eval \\\n"
"  --benchmarks=aime25:4 \\\n"
"  --server_type=$BACKEND \\\n"
"  --model=$MODEL \\\n"
"  --server_address=http://localhost:${PORT}/v1 \\\n"
"  --output_dir=nemo_skills_aime25_${MODEL_NAME}_output_${BACKEND}_$(date +"
"%Y%m%d_%H%M%S) \\\n"
"  ++chat_template_kwargs.thinking=true \\\n"
"  ++inference.temperature=1.0 \\\n"
"  ++inference.top_p=0.95 \\\n"
"  ++inference.tokens_to_generate=64000\n"
"  # ++inference.tokens_to_generate=120000 for Speciale model\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:247
msgid "Test results (8*B200):"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:249
msgid "DeepSeek-V3.2-Exp："
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "evaluation_mode"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "num_entries"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "avg_tokens"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "gen_seconds"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "symbolic_correct"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "no_answer"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "pass@1[avg-of-4]"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "30"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "15040"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "1673"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "87.50% ± 1.67%"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "0.00%"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "majority@4"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "90.00%"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "pass@4"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:258
msgid "DeepSeek-V3.2:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "13550"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "1632"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "92.50% ± 1.67%"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "94.71%"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "96.67%"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:266
msgid "DeepSeek-V3.2-Speciale:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "24155"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "3583"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "95.00% ± 1.92%"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "95.83%"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:80
msgid "100.00%"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:275
msgid "DSA long sequence context parallel optimization(experimental)"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:279
msgid "Example usage:"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:280
msgid ""
"# Launch with EP + DP\n"
"python -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp  --tp 8 "
"--ep 8 --dp 2 --enable-dp-attention --enable-nsa-prefill-context-parallel --"
"max-running-requests 32\n"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:284
msgid "Context-parallel Tips"
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:285
msgid ""
"`CP_size` reuses `atten_tp_size`, which is equal to `TP_size` / `DP_size`. "
"Some features are still not supported at present."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:287
msgid ""
"**Multi-batch prefill**: Currently, only single-request processing is "
"supported during the prefill process."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:288
msgid "**disaggregation**: P/D disaggregation."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:289
msgid ""
"**Cross-machine support**: - Currently only tested on a single machine (TP=8,"
"EP=8)."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:290
msgid ""
"**Other Args**: Currently only supports moe_dense_tp_size=1, kv_cache_dtype "
"= \"bf16\", moe_a2a_backend = \"deepep\","
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:291
msgid ""
"**DP_size**: `CP_size` reuses `atten_tp_size`, which is equal to `TP_size` / "
"`DP_size`. For the cp function to work correctly, `TP_size` must be "
"divisible by `DP_size`, and TP_size / DP_size > 1 (to ensure CP_size > 1)."
msgstr ""

#: ../../../basic_usage/deepseek_v32.md:292
msgid ""
"**Detailed design reference**: https://github.com/sgl-project/sglang/"
"pull/12065"
msgstr ""
