# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:37+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:1
msgid "Deploy On Kubernetes"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:3
msgid ""
"This document is for deploying a RoCE network-based SGLang two-node "
"inference service on a Kubernetes (K8S) cluster."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:5
msgid ""
"[LeaderWorkerSet (LWS)](https://github.com/kubernetes-sigs/lws) is a "
"Kubernetes API that aims to address common deployment patterns of AI/ML "
"inference workloads. A major use case is for multi-host/multi-node "
"distributed inference."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:7
msgid ""
"SGLang can also be deployed with LWS on Kubernetes for distributed model "
"serving."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:9
msgid ""
"Please see this guide for more details on deploying SGLang on Kubernetes "
"using LWS."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:11
msgid "Here we take the deployment of DeepSeek-R1 as an example."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:13
msgid "Prerequisites"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:15
msgid ""
"At least two Kubernetes nodes, each with two H20 systems and eight GPUs, are "
"required."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:17
msgid ""
"Make sure your K8S cluster has LWS correctly installed. If it hasn't been "
"set up yet, please follow the [installation instructions](https://github.com/"
"kubernetes-sigs/lws/blob/main/site/content/en/docs/installation/_index.md). "
"**Note:** For LWS versions ≤0.5.x, you must use the Downward API to obtain "
"`LWS_WORKER_INDEX`, as native support for this feature was introduced in "
"v0.6.0."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:19
msgid "Basic example"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:21
msgid ""
"For the basic example documentation, refer to [Deploy Distributed Inference "
"Service with SGLang and LWS on GPUs](https://github.com/kubernetes-sigs/lws/"
"tree/main/docs/examples/sglang)."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:23
msgid "However, that document only covers the basic NCCL socket mode."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:25
msgid ""
"In this section, we’ll make some simple modifications to adapt the setup to "
"the RDMA scenario."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:27
msgid "RDMA RoCE case"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:29
#: ../../../references/multi_node_deployment/deploy_on_k8s.md:268
msgid "Check your env:"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:31
msgid ""
"[root@node1 ~]# ibstatus\n"
"Infiniband device 'mlx5_bond_0' port 1 status:\n"
"        default gid:     fe80:0000:0000:0000:0225:9dff:fe64:c79a\n"
"        base lid:        0x0\n"
"        sm lid:          0x0\n"
"        state:           4: ACTIVE\n"
"        phys state:      5: LinkUp\n"
"        rate:            200 Gb/sec (2X NDR)\n"
"        link_layer:      Ethernet\n"
"\n"
"Infiniband device 'mlx5_bond_1' port 1 status:\n"
"        default gid:     fe80:0000:0000:0000:0225:9dff:fe6e:c3ec\n"
"        base lid:        0x0\n"
"        sm lid:          0x0\n"
"        state:           4: ACTIVE\n"
"        phys state:      5: LinkUp\n"
"        rate:            200 Gb/sec (2X NDR)\n"
"        link_layer:      Ethernet\n"
"\n"
"Infiniband device 'mlx5_bond_2' port 1 status:\n"
"        default gid:     fe80:0000:0000:0000:0225:9dff:fe73:0dd7\n"
"        base lid:        0x0\n"
"        sm lid:          0x0\n"
"        state:           4: ACTIVE\n"
"        phys state:      5: LinkUp\n"
"        rate:            200 Gb/sec (2X NDR)\n"
"        link_layer:      Ethernet\n"
"\n"
"Infiniband device 'mlx5_bond_3' port 1 status:\n"
"        default gid:     fe80:0000:0000:0000:0225:9dff:fe36:f7ff\n"
"        base lid:        0x0\n"
"        sm lid:          0x0\n"
"        state:           4: ACTIVE\n"
"        phys state:      5: LinkUp\n"
"        rate:            200 Gb/sec (2X NDR)\n"
"        link_layer:      Ethernet\n"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:70
msgid "Prepare the `lws.yaml` file for deploying on k8s."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:72
msgid ""
"apiVersion: leaderworkerset.x-k8s.io/v1\n"
"kind: LeaderWorkerSet\n"
"metadata:\n"
"  name: sglang\n"
"spec:\n"
"  replicas: 1\n"
"  leaderWorkerTemplate:\n"
"    size: 2\n"
"    restartPolicy: RecreateGroupOnPodRestart\n"
"    leaderTemplate:\n"
"      metadata:\n"
"        labels:\n"
"          role: leader\n"
"      spec:\n"
"        dnsPolicy: ClusterFirstWithHostNet\n"
"        hostNetwork: true\n"
"        hostIPC: true\n"
"        containers:\n"
"          - name: sglang-leader\n"
"            image: sglang:latest\n"
"            securityContext:\n"
"              privileged: true\n"
"            env:\n"
"              - name: NCCL_IB_GID_INDEX\n"
"                value: \"3\"\n"
"            command:\n"
"              - python3\n"
"              - -m\n"
"              - sglang.launch_server\n"
"              - --model-path\n"
"              - /work/models\n"
"              - --mem-fraction-static\n"
"              -  \"0.93\"\n"
"              - --torch-compile-max-bs\n"
"              - \"8\"\n"
"              - --max-running-requests\n"
"              - \"20\"\n"
"              - --tp\n"
"              - \"16\" # Size of Tensor Parallelism\n"
"              - --dist-init-addr\n"
"              - $(LWS_LEADER_ADDRESS):20000\n"
"              - --nnodes\n"
"              - $(LWS_GROUP_SIZE)\n"
"              - --node-rank\n"
"              - $(LWS_WORKER_INDEX)\n"
"              - --trust-remote-code\n"
"              - --host\n"
"              - \"0.0.0.0\"\n"
"              - --port\n"
"              - \"40000\"\n"
"            resources:\n"
"              limits:\n"
"                nvidia.com/gpu: \"8\"\n"
"            ports:\n"
"              - containerPort: 40000\n"
"            readinessProbe:\n"
"              tcpSocket:\n"
"                port: 40000\n"
"              initialDelaySeconds: 15\n"
"              periodSeconds: 10\n"
"            volumeMounts:\n"
"              - mountPath: /dev/shm\n"
"                name: dshm\n"
"              - name: model\n"
"                mountPath: /work/models\n"
"              - name: ib\n"
"                mountPath: /dev/infiniband\n"
"        volumes:\n"
"          - name: dshm\n"
"            emptyDir:\n"
"              medium: Memory\n"
"          - name: model\n"
"            hostPath:\n"
"              path: '< your models dir >' # modify it according your models "
"dir\n"
"          - name: ib\n"
"            hostPath:\n"
"              path: /dev/infiniband\n"
"    workerTemplate:\n"
"      spec:\n"
"        dnsPolicy: ClusterFirstWithHostNet\n"
"        hostNetwork: true\n"
"        hostIPC: true\n"
"        containers:\n"
"          - name: sglang-worker\n"
"            image: sglang:latest\n"
"            securityContext:\n"
"              privileged: true\n"
"            env:\n"
"            - name: NCCL_IB_GID_INDEX\n"
"              value: \"3\"\n"
"            command:\n"
"              - python3\n"
"              - -m\n"
"              - sglang.launch_server\n"
"              - --model-path\n"
"              - /work/models\n"
"              - --mem-fraction-static\n"
"              - \"0.93\"\n"
"              - --torch-compile-max-bs\n"
"              - \"8\"\n"
"              - --max-running-requests\n"
"              - \"20\"\n"
"              - --tp\n"
"              - \"16\" # Size of Tensor Parallelism\n"
"              - --dist-init-addr\n"
"              - $(LWS_LEADER_ADDRESS):20000\n"
"              - --nnodes\n"
"              - $(LWS_GROUP_SIZE)\n"
"              - --node-rank\n"
"              - $(LWS_WORKER_INDEX)\n"
"              - --trust-remote-code\n"
"            resources:\n"
"              limits:\n"
"                nvidia.com/gpu: \"8\"\n"
"            volumeMounts:\n"
"              - mountPath: /dev/shm\n"
"                name: dshm\n"
"              - name: model\n"
"                mountPath: /work/models\n"
"              - name: ib\n"
"                mountPath: /dev/infiniband\n"
"        volumes:\n"
"          - name: dshm\n"
"            emptyDir:\n"
"              medium: Memory\n"
"          - name: ib\n"
"            hostPath:\n"
"              path: /dev/infiniband\n"
"          - name: model\n"
"            hostPath:\n"
"              path: /data1/models/deepseek_v3_moe\n"
"---\n"
"apiVersion: v1\n"
"kind: Service\n"
"metadata:\n"
"  name: sglang-leader\n"
"spec:\n"
"  selector:\n"
"    leaderworkerset.sigs.k8s.io/name: sglang\n"
"    role: leader\n"
"  ports:\n"
"    - protocol: TCP\n"
"      port: 40000\n"
"      targetPort: 40000\n"
"\n"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:220
msgid "Then use  `kubectl apply -f lws.yaml` you will get this output."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:222
msgid ""
"NAME           READY   STATUS    RESTARTS       AGE\n"
"sglang-0       0/1     Running   0              9s\n"
"sglang-0-1     1/1     Running   0              9s\n"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:228
msgid ""
"Wait for the sglang leader (`sglang-0`) status to change to 1/1, which "
"indicates it is `Ready`."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:230
msgid ""
"You can use the command `kubectl logs -f sglang-0` to view the logs of the "
"leader node."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:232
msgid "Once successful, you should see output like this:"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:234
msgid ""
"[2025-02-17 05:27:24 TP1] Capture cuda graph end. Time elapsed: 84.89 s\n"
"[2025-02-17 05:27:24 TP6] max_total_num_tokens=712400, "
"chunked_prefill_size=8192, max_prefill_tokens=16384, "
"max_running_requests=50, context_len=163840\n"
"[2025-02-17 05:27:24 TP0] max_total_num_tokens=712400, "
"chunked_prefill_size=8192, max_prefill_tokens=16384, "
"max_running_requests=50, context_len=163840\n"
"[2025-02-17 05:27:24 TP7] max_total_num_tokens=712400, "
"chunked_prefill_size=8192, max_prefill_tokens=16384, "
"max_running_requests=50, context_len=163840\n"
"[2025-02-17 05:27:24 TP3] max_total_num_tokens=712400, "
"chunked_prefill_size=8192, max_prefill_tokens=16384, "
"max_running_requests=50, context_len=163840\n"
"[2025-02-17 05:27:24 TP2] max_total_num_tokens=712400, "
"chunked_prefill_size=8192, max_prefill_tokens=16384, "
"max_running_requests=50, context_len=163840\n"
"[2025-02-17 05:27:24 TP4] max_total_num_tokens=712400, "
"chunked_prefill_size=8192, max_prefill_tokens=16384, "
"max_running_requests=50, context_len=163840\n"
"[2025-02-17 05:27:24 TP1] max_total_num_tokens=712400, "
"chunked_prefill_size=8192, max_prefill_tokens=16384, "
"max_running_requests=50, context_len=163840\n"
"[2025-02-17 05:27:24 TP5] max_total_num_tokens=712400, "
"chunked_prefill_size=8192, max_prefill_tokens=16384, "
"max_running_requests=50, context_len=163840\n"
"[2025-02-17 05:27:24] INFO:     Started server process [1]\n"
"[2025-02-17 05:27:24] INFO:     Waiting for application startup.\n"
"[2025-02-17 05:27:24] INFO:     Application startup complete.\n"
"[2025-02-17 05:27:24] INFO:     Uvicorn running on http://0.0.0.0:40000 "
"(Press CTRL+C to quit)\n"
"[2025-02-17 05:27:25] INFO:     127.0.0.1:48908 - \"GET /get_model_info "
"HTTP/1.1\" 200 OK\n"
"[2025-02-17 05:27:25 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-"
"token: 0, cache hit rate: 0.00%, token usage: 0.00, #running-req: 0, #queue-"
"req: 0\n"
"[2025-02-17 05:27:32] INFO:     127.0.0.1:48924 - \"POST /generate "
"HTTP/1.1\" 200 OK\n"
"[2025-02-17 05:27:32] The server is fired up and ready to roll!\n"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:254
msgid ""
"If it doesn’t start up successfully, please follow these steps to check for "
"any remaining issues. Thanks!"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:256
msgid "Debug"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:258
msgid "Set `NCCL_DEBUG=TRACE` to check if it is a NCCL communication problem."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:260
msgid "This should resolve most NCCL-related issues."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:262
msgid ""
"***Notice: If you find that NCCL_DEBUG=TRACE is not effective in the "
"container environment, but the process is stuck or you encounter hard-to-"
"diagnose issues, try switching to a different container image. Some images "
"may not handle standard error output properly.***"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:264
msgid "RoCE scenario"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:266
msgid ""
"Please make sure that RDMA devices are available in the cluster environment."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:267
msgid ""
"Please make sure that the nodes in the cluster have Mellanox NICs with RoCE. "
"In this example, we use Mellanox ConnectX 5 model NICs, and the proper OFED "
"driver has been installed. If not, please refer to the document [Install "
"OFED Driver](https://docs.nvidia.com/networking/display/mlnxofedv461000/"
"installing+mellanox+ofed) to install the driver."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:270
msgid ""
"$ lspci -nn | grep Eth | grep Mellanox\n"
"0000:7f:00.0 Ethernet controller [0200]: Mellanox Technologies MT43244 "
"BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n"
"0000:7f:00.1 Ethernet controller [0200]: Mellanox Technologies MT43244 "
"BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n"
"0000:c7:00.0 Ethernet controller [0200]: Mellanox Technologies MT43244 "
"BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n"
"0000:c7:00.1 Ethernet controller [0200]: Mellanox Technologies MT43244 "
"BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n"
"0001:08:00.0 Ethernet controller [0200]: Mellanox Technologies MT43244 "
"BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n"
"0001:08:00.1 Ethernet controller [0200]: Mellanox Technologies MT43244 "
"BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n"
"0001:a2:00.0 Ethernet controller [0200]: Mellanox Technologies MT43244 "
"BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n"
"0001:a2:00.1 Ethernet controller [0200]: Mellanox Technologies MT43244 "
"BlueField-3 integrated ConnectX-7 network controller [15b3:a2dc] (rev 01)\n"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:282
msgid "Check the OFED driver:"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:284
msgid ""
"ofed_info -s\n"
"OFED-internal-23.07-0.5.0:\n"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:289
msgid "Show RDMA link status and check IB devices:"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:291
msgid ""
"$ rdma link show\n"
"8/1: mlx5_bond_0/1: state ACTIVE physical_state LINK_UP netdev reth0\n"
"9/1: mlx5_bond_1/1: state ACTIVE physical_state LINK_UP netdev reth2\n"
"10/1: mlx5_bond_2/1: state ACTIVE physical_state LINK_UP netdev reth4\n"
"11/1: mlx5_bond_3/1: state ACTIVE physical_state LINK_UP netdev reth6\n"
"\n"
"$ ibdev2netdev\n"
"8/1: mlx5_bond_0/1: state ACTIVE physical_state LINK_UP netdev reth0\n"
"9/1: mlx5_bond_1/1: state ACTIVE physical_state LINK_UP netdev reth2\n"
"10/1: mlx5_bond_2/1: state ACTIVE physical_state LINK_UP netdev reth4\n"
"11/1: mlx5_bond_3/1: state ACTIVE physical_state LINK_UP netdev reth6\n"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:305
msgid "Test RoCE network speed on the host:"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:307
msgid ""
"yum install qperf\n"
"# for server：\n"
"execute qperf\n"
"# for client\n"
"qperf -t 60 -cm1 <server_ip>   rc_rdma_write_bw\n"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:315
msgid "Check RDMA accessible in your container:"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:317
msgid ""
"# ibv_devices\n"
"# ibv_devinfo\n"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:322
msgid "Keys to success"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:324
msgid ""
"In the YAML configuration above, pay attention to the NCCL environment "
"variable. For older versions of NCCL, you should check the NCCL_IB_GID_INDEX "
"environment setting."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:325
msgid ""
"NCCL_SOCKET_IFNAME is also crucial, but in a containerized environment, this "
"typically isn’t an issue."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:326
msgid ""
"In some cases, it’s necessary to configure GLOO_SOCKET_IFNAME correctly."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:327
msgid ""
"NCCL_DEBUG is essential for troubleshooting, but I've found that sometimes "
"it doesn't show error logs within containers. This could be related to the "
"Docker image you're using. You may want to try switching images if needed."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:328
msgid ""
"Avoid using Docker images based on Ubuntu 18.04, as they tend to have "
"compatibility issues."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:330
msgid "Remaining issues"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:332
msgid ""
"In Kubernetes, Docker, or Containerd environments, we use hostNetwork to "
"prevent performance degradation."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:333
msgid ""
"We utilize privileged mode, which  isn’t secure. Additionally, in "
"containerized environments, full GPU isolation cannot be achieved."
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:335
msgid "TODO"
msgstr ""

#: ../../../references/multi_node_deployment/deploy_on_k8s.md:337
msgid ""
"Integrated with [k8s-rdma-shared-dev-plugin](https://github.com/Mellanox/k8s-"
"rdma-shared-dev-plugin)."
msgstr ""
