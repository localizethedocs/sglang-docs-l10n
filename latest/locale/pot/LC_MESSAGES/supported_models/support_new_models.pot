# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:37+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../supported_models/support_new_models.md:1
msgid "How to Support New Models"
msgstr ""

#: ../../../supported_models/support_new_models.md:3
msgid ""
"This document explains how to add support for new language models and "
"multimodal large language models (MLLMs) in SGLang. It also covers how to "
"test new models and register external implementations."
msgstr ""

#: ../../../supported_models/support_new_models.md:6
msgid "How to Support a New Language Model"
msgstr ""

#: ../../../supported_models/support_new_models.md:8
msgid ""
"To support a new model in SGLang, you only need to add a single file under "
"the [SGLang Models Directory](https://github.com/sgl-project/sglang/tree/"
"main/python/sglang/srt/models). You can learn from existing model "
"implementations and create a new file for your model. For most models, you "
"should be able to find a similar model to start with (e.g., starting from "
"Llama). Also refer how to [port a Model from vLLM to SGLang](#port-a-model-"
"from-vllm-to-sglang)"
msgstr ""

#: ../../../supported_models/support_new_models.md:14
msgid "How to Support a New Multimodal Large Language Model"
msgstr ""

#: ../../../supported_models/support_new_models.md:16
msgid ""
"To support a new multimodal large language model (MLLM) in SGLang, there are "
"several key components in addition to the standard LLM support:"
msgstr ""

#: ../../../supported_models/support_new_models.md:19
msgid ""
"**Register your new model as multimodal**: Extend `is_multimodal_model` in "
"[model_config.py](https://github.com/sgl-project/sglang/"
"blob/0ab3f437aba729b348a683ab32b35b214456efc7/python/sglang/srt/configs/"
"model_config.py#L561) to return `True` for your model."
msgstr ""

#: ../../../supported_models/support_new_models.md:24
msgid ""
"**Register a new chat-template**: Only when your default chat-template is "
"unable to accept images as input: Register a new chat template in "
"[conversation.py](https://github.com/sgl-project/sglang/tree/main/python/"
"sglang/srt/conversation.py) and the corresponding matching function."
msgstr ""

#: ../../../supported_models/support_new_models.md:27
msgid ""
"**Multimodal Data Processor**: Define a new `Processor` class that inherits "
"from `BaseMultimodalProcessor` and register this processor as your model’s "
"dedicated processor. See [multimodal_processor.py](https://github.com/sgl-"
"project/sglang/tree/main/python/sglang/srt/multimodal/processors) for more "
"details."
msgstr ""

#: ../../../supported_models/support_new_models.md:33
msgid ""
"**Handle Multimodal Tokens**: Implement a `pad_input_ids` function for your "
"new model. In this function, multimodal tokens in the prompt should be "
"expanded (if necessary) and padded with multimodal-data-hashes so that "
"SGLang can recognize different multimodal data with `RadixAttention`."
msgstr ""

#: ../../../supported_models/support_new_models.md:38
msgid ""
"**Handle Image Feature Extraction**: Implement a `get_image_feature` "
"function for your new model, which extracts image features from raw image "
"data and converts them into the embeddings used by the language model."
msgstr ""

#: ../../../supported_models/support_new_models.md:41
msgid ""
"**Adapt to Vision Attention**: Adapt the multi-headed `Attention` of ViT "
"with SGLang’s `VisionAttention`."
msgstr ""

#: ../../../supported_models/support_new_models.md:44
msgid ""
"You can refer to [Qwen2VL](https://github.com/sgl-project/sglang/blob/main/"
"python/sglang/srt/models/qwen2_vl.py) or other mllm implementations. These "
"models demonstrate how to correctly handle both multimodal and textual "
"inputs."
msgstr ""

#: ../../../supported_models/support_new_models.md:47
msgid "Testing and Debugging"
msgstr ""

#: ../../../supported_models/support_new_models.md:49
msgid ""
"Please note all your testing and benchmarking results in PR description."
msgstr ""

#: ../../../supported_models/support_new_models.md:51
msgid "Interactive Debugging"
msgstr ""

#: ../../../supported_models/support_new_models.md:53
msgid ""
"For interactive debugging, compare the outputs of Hugging Face/Transformers "
"and SGLang. The following two commands should give the same text output and "
"very similar prefill logits:"
msgstr ""

#: ../../../supported_models/support_new_models.md:56
msgid "Get the reference output:"
msgstr ""

#: ../../../supported_models/support_new_models.md:57
msgid ""
"python3 scripts/playground/reference_hf.py --model-path [new model] --model-"
"type {text,mllm}\n"
msgstr ""

#: ../../../supported_models/support_new_models.md:60
msgid "Get the SGLang output:"
msgstr ""

#: ../../../supported_models/support_new_models.md:61
msgid "python3 -m sglang.bench_one_batch --correct --model [new model]\n"
msgstr ""

#: ../../../supported_models/support_new_models.md:65
msgid "Add the Model to the Test Suite"
msgstr ""

#: ../../../supported_models/support_new_models.md:67
msgid ""
"To ensure the new model is well maintained, add it to the test suite by "
"including it in the `ALL_OTHER_MODELS` list in the [test_generation_models."
"py](https://github.com/sgl-project/sglang/blob/main/test/srt/models/"
"test_generation_models.py) file, test the new model on your local machine "
"and report the results on demonstrative benchmarks (GSM8K, MMLU, MMMU, MMMU-"
"Pro, etc.) in your PR. \\\\ For VLMs, also include a test in "
"`test_vision_openai_server_{x}.py` (e.g. [test_vision_openai_server_a.py]"
"(https://github.com/sgl-project/sglang/blob/main/test/srt/"
"test_vision_openai_server_a.py), [test_vision_openai_server_b.py](https://"
"github.com/sgl-project/sglang/blob/main/test/srt/test_vision_openai_server_b."
"py))."
msgstr ""

#: ../../../supported_models/support_new_models.md:74
msgid ""
"This is an example command to run to test a new model on your local machine:"
msgstr ""

#: ../../../supported_models/support_new_models.md:76
msgid ""
"ONLY_RUN=Qwen/Qwen2-1.5B python3 -m unittest test_generation_models."
"TestGenerationModels.test_others\n"
msgstr ""

#: ../../../supported_models/support_new_models.md:80
msgid "Benchmark"
msgstr ""

#: ../../../supported_models/support_new_models.md:82
msgid ""
"**(Required) MMMU**: follow MMMU benchmark [README.md](https://github.com/"
"sgl-project/sglang/blob/main/benchmark/mmmu/README.md) to get SGLang vs. HF "
"Transformer accuracy comparison. The accuracy score from SGLang run should "
"not be much lower than that from HF Transformer run. Similarly, follow "
"https://docs.sglang.ai/developer_guide/benchmark_and_profiling.html to get "
"performance comparison: TTFT and throughput must meet or exceed baselines (e."
"g., HF Transformer)."
msgstr ""

#: ../../../supported_models/support_new_models.md:83
msgid ""
"**(Optional) Other evals**: If you ran other evals, please note the results "
"in PR description."
msgstr ""

#: ../../../supported_models/support_new_models.md:85
msgid "Port a Model from vLLM to SGLang"
msgstr ""

#: ../../../supported_models/support_new_models.md:87
msgid ""
"The [vLLM Models Directory](https://github.com/vllm-project/vllm/tree/main/"
"vllm/model_executor/models) is a valuable resource, as vLLM covers many "
"models. SGLang reuses vLLM’s interface and some layers, making it easier to "
"port models from vLLM to SGLang."
msgstr ""

#: ../../../supported_models/support_new_models.md:91
msgid "To port a model from vLLM to SGLang:"
msgstr ""

#: ../../../supported_models/support_new_models.md:93
msgid "Compare these two files for guidance:"
msgstr ""

#: ../../../supported_models/support_new_models.md:94
msgid ""
"[SGLang Llama Implementation](https://github.com/sgl-project/sglang/blob/"
"main/python/sglang/srt/models/llama.py)"
msgstr ""

#: ../../../supported_models/support_new_models.md:95
msgid ""
"[vLLM Llama Implementation](https://github.com/vllm-project/vllm/blob/main/"
"vllm/model_executor/models/llama.py)"
msgstr ""

#: ../../../supported_models/support_new_models.md:96
msgid "The major differences include:"
msgstr ""

#: ../../../supported_models/support_new_models.md:97
msgid ""
"**Replace vLLM’s `Attention` with `RadixAttention`** (ensure you pass "
"`layer_id` to `RadixAttention`)."
msgstr ""

#: ../../../supported_models/support_new_models.md:98
msgid "**Replace vLLM’s `LogitsProcessor` with SGLang’s `LogitsProcessor`.**"
msgstr ""

#: ../../../supported_models/support_new_models.md:99
msgid ""
"**Replace the multi-headed `Attention` of ViT with SGLang’s "
"`VisionAttention`.**"
msgstr ""

#: ../../../supported_models/support_new_models.md:100
msgid ""
"**Replace other vLLM layers** (such as `RMSNorm`, `SiluAndMul`) with SGLang "
"layers."
msgstr ""

#: ../../../supported_models/support_new_models.md:101
msgid "**Remove `Sample`.**"
msgstr ""

#: ../../../supported_models/support_new_models.md:102
msgid ""
"**Change the `forward()` functions** and add a `forward_batch()` method."
msgstr ""

#: ../../../supported_models/support_new_models.md:103
msgid "**Add `EntryClass`** at the end."
msgstr ""

#: ../../../supported_models/support_new_models.md:104
msgid ""
"**Ensure that the new implementation uses only SGLang components** and does "
"not rely on any vLLM components."
msgstr ""

#: ../../../supported_models/support_new_models.md:106
msgid ""
"Note: make sure you add your new model to the supported models list in the "
"supported models documentation."
msgstr ""

#: ../../../supported_models/support_new_models.md:108
msgid "Registering an External Model Implementation"
msgstr ""

#: ../../../supported_models/support_new_models.md:110
msgid ""
"In addition to the methods above, you can register your new model with the "
"`ModelRegistry` before launching the server. This allows you to integrate "
"your model without modifying the source code."
msgstr ""

#: ../../../supported_models/support_new_models.md:113
msgid "For example:"
msgstr ""

#: ../../../supported_models/support_new_models.md:115
msgid ""
"from sglang.srt.models.registry import ModelRegistry\n"
"from sglang.srt.entrypoints.http_server import launch_server\n"
"\n"
"# For a single model, add it to the registry:\n"
"ModelRegistry.models[model_name] = model_class\n"
"\n"
"# For multiple models, you can imitate the import_model_classes() function:\n"
"from functools import lru_cache\n"
"\n"
"@lru_cache()\n"
"def import_new_model_classes():\n"
"    model_arch_name_to_cls = {}\n"
"    # Populate model_arch_name_to_cls with your new model classes.\n"
"    ...\n"
"    return model_arch_name_to_cls\n"
"\n"
"ModelRegistry.models.update(import_new_model_classes())\n"
"\n"
"# Launch the server with your server arguments:\n"
"launch_server(server_args)\n"
msgstr ""

#: ../../../supported_models/support_new_models.md:138
msgid "Example: Implementing and Serving a Llama Wrapper Model"
msgstr ""

#: ../../../supported_models/support_new_models.md:140
msgid ""
"Below is an introductory, step-by-step walkthrough on how to implement a new "
"model end-to-end in SGLang and then run it via the [Offline Engine](https://"
"github.com/sgl-project/sglang/blob/main/docs/basic_usage/offline_engine_api."
"ipynb)."
msgstr ""

#: ../../../supported_models/support_new_models.md:142
msgid "Implementing Our Model"
msgstr ""

#: ../../../supported_models/support_new_models.md:144
msgid ""
"To keep things simple, this new model will be a simple wrapper around [Llama "
"3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct), "
"and our goal will be just to bias the output logits for each `forward` call "
"by taking the square root of each individual logit."
msgstr ""

#: ../../../supported_models/support_new_models.md:146
msgid ""
"Let's start by defining our model in a file called `llama_wrapper.py`. The "
"first step is to import the necessary libraries from SRT, which is SGLang's "
"internal backend."
msgstr ""

#: ../../../supported_models/support_new_models.md:149
msgid ""
"# In the file `llama_wrapper.py`\n"
"\n"
"import torch\n"
"from transformers import LlamaConfig\n"
"from typing import Optional\n"
"from sglang.srt.layers.logits_processor import LogitsProcessorOutput\n"
"from sglang.srt.layers.quantization.base_config import QuantizationConfig\n"
"from sglang.srt.model_executor.forward_batch_info import ForwardBatch, "
"PPProxyTensors\n"
"\n"
"from sglang.srt.models.llama import LlamaForCausalLM\n"
msgstr ""

#: ../../../supported_models/support_new_models.md:162
msgid ""
"Next, we declare a new `class` for our model and have it inherit from "
"`LlamaForCausalLM`, which allows our model to access `LlamaForCausalLM`'s "
"predefined modules and layers, such as `LlamaAttention` and `LlamaMLP`. Note "
"that almost all model implementations take in `config` and `quant_config` as "
"arguments for their `__init__` method; `config` and `quant_config` are "
"passed in via [`model_loader/loader.py`](https://github.com/sgl-project/"
"sglang/blob/bf72b80122fd888bf619d17b96fa3e323ab809fc/python/sglang/srt/"
"model_loader/loader.py#L219). Because we have inherited from "
"`LlamaForCausalLM`, we can pass our parameters directly to its constructor, "
"which will set the member variables for us."
msgstr ""

#: ../../../supported_models/support_new_models.md:166
msgid ""
"class LlamaWrapper(LlamaForCausalLM):\n"
"    def __init__(\n"
"        self,\n"
"        config: LlamaConfig,\n"
"        quant_config: Optional[QuantizationConfig] = None,\n"
"        prefix: str = \"\",\n"
"    ) -> None:\n"
"        super().__init__(config=config, quant_config=quant_config, "
"prefix=prefix)\n"
msgstr ""

#: ../../../supported_models/support_new_models.md:177
msgid ""
"Now, we want to define the `forward` method, which is what will be called at "
"inference time. Note that the signature for `forward` is essentially the "
"same for any model; you can take a look at the other models defined in the "
"[`models` directory](https://github.com/sgl-project/sglang/blob/main/python/"
"sglang/srt/models/) for references. To see where exactly `forward` is called "
"in the SGLang runtime's internals, take a look at [`forward_decode`](https://"
"github.com/sgl-project/sglang/blob/bf72b80122fd888bf619d17b96fa3e323ab809fc/"
"python/sglang/srt/model_executor/model_runner.py#L1705) and "
"[`forward_extend`](https://github.com/sgl-project/sglang/blob/"
"bf72b80122fd888bf619d17b96fa3e323ab809fc/python/sglang/srt/model_executor/"
"model_runner.py#L1724) in the [`ModelRunner` class](https://github.com/sgl-"
"project/sglang/blob/main/python/sglang/srt/model_executor/model_runner.py)."
msgstr ""

#: ../../../supported_models/support_new_models.md:181
msgid ""
"    @torch.no_grad()\n"
"    def forward(\n"
"        self,\n"
"        input_ids: torch.Tensor,\n"
"        positions: torch.Tensor,\n"
"        forward_batch: ForwardBatch,\n"
"        pp_proxy_tensors: Optional[PPProxyTensors] = None,\n"
"        input_embeds: Optional[torch.Tensor] = None,\n"
"        get_embedding: bool = False,\n"
"    ) -> LogitsProcessorOutput:\n"
msgstr ""

#: ../../../supported_models/support_new_models.md:194
msgid ""
"We now call the `__call__` method for `self.model` (which is a member "
"variable that `LlamaForCausalLM` defines in its `__init__` method), which "
"eventually calls `LlamaForCausalLM`'s `forward` method. After that, we feed "
"the `hidden_states` into our model's `LogitsProcessor` (again defined in "
"`LlamaForCausalLM`)."
msgstr ""

#: ../../../supported_models/support_new_models.md:197
msgid ""
"        hidden_states = self.model(\n"
"            input_ids,\n"
"            positions,\n"
"            forward_batch,\n"
"            input_embeds,\n"
"            pp_proxy_tensors=pp_proxy_tensors,\n"
"        )\n"
"\n"
"        res: LogitsProcessorOutput = self.logits_processor(\n"
"            input_ids,\n"
"            hidden_states,\n"
"            self.lm_head,\n"
"            forward_batch,\n"
"        )\n"
msgstr ""

#: ../../../supported_models/support_new_models.md:214
msgid ""
"After receiving the logits for the next token, we can finally perform our "
"biasing step."
msgstr ""

#: ../../../supported_models/support_new_models.md:216
msgid ""
"        orig_logits = res.next_token_logits\n"
"        res.next_token_logits = torch.where(\n"
"            orig_logits > 0,\n"
"            orig_logits.sqrt(),\n"
"            orig_logits\n"
"        )\n"
"\n"
"        return res\n"
msgstr ""

#: ../../../supported_models/support_new_models.md:226
msgid "Now, our `LlamaWrapper` model is created and ready to be served!"
msgstr ""

#: ../../../supported_models/support_new_models.md:228
msgid "Serving Our Model Via SGLang's Offline Engine"
msgstr ""

#: ../../../supported_models/support_new_models.md:230
msgid ""
"The next step of this walkthrough involves hosting our new model offline, so "
"that it can be served locally and without an HTTP server."
msgstr ""

#: ../../../supported_models/support_new_models.md:232
msgid ""
"First, create a new file called `run.py`. Now, we must ensure that SGLang's "
"`ModelRegistry` can find our model. To do this, we first download the "
"model's configuration and weights from Huggingface."
msgstr ""

#: ../../../supported_models/support_new_models.md:236
msgid ""
"# In the file `run.py`\n"
"\n"
"import asyncio\n"
"from functools import lru_cache\n"
"from huggingface_hub import snapshot_download\n"
"from llama_wrapper import LlamaWrapper # Make sure to import our new model!\n"
"import sglang as sgl\n"
"from sglang.srt.models.registry import ModelRegistry\n"
"\n"
"# Make sure to request access to this model on Huggingface, then export "
"your\n"
"# `HF_TOKEN` to download the model snapshot\n"
"llama_dir = snapshot_download(\n"
"    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n"
"    local_dir=\"./llama_ckpt\",\n"
")\n"
msgstr ""

#: ../../../supported_models/support_new_models.md:254
msgid ""
"Now that we have our model on disk, we want to point it to `LlamaWrapper` by "
"changing the `architectures` field in `./llama_ckpt/config.json` to be "
"`LlamaWrapper`. That way, when we pass in the path of our model checkpoint "
"to SGLang, it will know that we want to use \"LlamaWrapper\" instead of "
"\"LlamaForCausalLM\" as our model."
msgstr ""

#: ../../../supported_models/support_new_models.md:257
msgid ""
"{\n"
"  \"architectures\": [\n"
"   #  \"LlamaForCausalLM\"\n"
"    \"LlamaWrapper\"\n"
"  ],\n"
"  ...\n"
"}\n"
msgstr ""

#: ../../../supported_models/support_new_models.md:267
msgid ""
"However, if we don't link our `LlamaWrapper` class to the \"LlamaWrapper\" "
"registry keyword, then SGLang won't be able to find our model. Thus, to "
"register our `LlamaWrapper`, we want to follow the steps in the above "
"section titled \"Registering an External Model Implementation\"."
msgstr ""

#: ../../../supported_models/support_new_models.md:270
msgid ""
"@lru_cache()\n"
"def import_new_model_classes():\n"
"    model_arch_name_to_cls = {\"LlamaWrapper\": LlamaWrapper}\n"
"    return model_arch_name_to_cls\n"
"\n"
"ModelRegistry.models.update(import_new_model_classes())\n"
msgstr ""

#: ../../../supported_models/support_new_models.md:279
msgid ""
"Lastly, when we create our `Engine`, we just pass in the path to the local "
"model directory. Then, our `LlamaWrapper` is ready to be served; for this "
"walkthrough, we will use SGLang `Engine`'s non-streaming asynchronous "
"generation endpoint."
msgstr ""

#: ../../../supported_models/support_new_models.md:282
msgid ""
"def main():\n"
"    llm = sgl.Engine(model_path=\"./llama_ckpt\")\n"
"    sampling_params = {\"temperature\": 0.2, \"top_k\": 5}\n"
"    prompts = [\n"
"        \"Write a short, neutral self-introduction for a fictional "
"character. Hello, my name is\",\n"
"        \"Provide a concise factual statement about France’s capital city. "
"The capital of France is\",\n"
"        \"Explain possible future trends in artificial intelligence. The "
"future of AI is\",\n"
"    ]\n"
"\n"
"    asyncio.run(run_llm(llm, sampling_params, prompts))\n"
"\n"
"    llm.shutdown()\n"
"\n"
"async def run_llm(\n"
"    llm,\n"
"    sampling_params,\n"
"    prompts,\n"
") -> None:\n"
"    outputs = await llm.async_generate(prompts, sampling_params)\n"
"\n"
"    for prompt, output in zip(prompts, outputs):\n"
"        print(f\"\\nPrompt: {prompt}\")\n"
"        print(f\"Generated text: {output['text']}\")\n"
"\n"
"if __name__ == \"__main__\":\n"
"    main()\n"
msgstr ""

#: ../../../supported_models/support_new_models.md:311
msgid ""
"Now, when we call `python run.py`, we will get the outputs of our newly "
"created model!"
msgstr ""

#: ../../../supported_models/support_new_models.md:314
msgid "Documentation"
msgstr ""

#: ../../../supported_models/support_new_models.md:315
msgid ""
"Add to table of supported models in [generative_models.md](https://github."
"com/sgl-project/sglang/blob/main/docs/supported_models/generative_models.md) "
"or [multimodal_language_models.md](https://github.com/sgl-project/sglang/"
"blob/main/docs/supported_models/multimodal_language_models.md)"
msgstr ""

#: ../../../supported_models/support_new_models.md:319
msgid ""
"By following these guidelines, you can add support for new language models "
"and multimodal large language models in SGLang and ensure they are "
"thoroughly tested and easily integrated into the system."
msgstr ""
