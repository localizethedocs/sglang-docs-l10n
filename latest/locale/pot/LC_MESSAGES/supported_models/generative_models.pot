# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-05 08:33+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../supported_models/generative_models.md:1
msgid "Large Language Models"
msgstr ""

#: ../../../supported_models/generative_models.md:3
msgid ""
"These models accept text input and produce text output (e.g., chat "
"completions). They are primarily large language models (LLMs), some with "
"mixture-of-experts (MoE) architectures for scaling."
msgstr ""

#: ../../../supported_models/generative_models.md:5
msgid "Example launch Command"
msgstr ""

#: ../../../supported_models/generative_models.md:7
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model-path meta-llama/Llama-3.2-1B-Instruct \\  # example HF/local path\n"
"  --host 0.0.0.0 \\\n"
"  --port 30000 \\\n"
msgstr ""

#: ../../../supported_models/generative_models.md:14
msgid "Supported models"
msgstr ""

#: ../../../supported_models/generative_models.md:16
msgid "Below the supported models are summarized in a table."
msgstr ""

#: ../../../supported_models/generative_models.md:18
msgid ""
"If you are unsure if a specific architecture is implemented, you can search "
"for it via GitHub. For example, to search for `Qwen3ForCausalLM`, use the "
"expression:"
msgstr ""

#: ../../../supported_models/generative_models.md:20
msgid ""
"repo:sgl-project/sglang path:/^python\\/sglang\\/srt\\/models\\// "
"Qwen3ForCausalLM\n"
msgstr ""

#: ../../../supported_models/generative_models.md:24
msgid "in the GitHub search bar."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "Model Family (Variants)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "Example HuggingFace Identifier"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "Description"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**DeepSeek** (v1, v2, v3/R1)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`deepseek-ai/DeepSeek-R1`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Series of advanced reasoning-optimized models (including a 671B MoE) trained "
"with reinforcement learning; top performance on complex reasoning, math, and "
"code tasks. [SGLang provides Deepseek v3/R1 model-specific optimizations](../"
"basic_usage/deepseek.md) and [Reasoning Parser](../advanced_features/"
"separate_reasoning.ipynb)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**GPT-OSS**"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`openai/gpt-oss-20b`, `openai/gpt-oss-120b`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"OpenAI’s latest GPT-OSS series for complex reasoning, agentic tasks, and "
"versatile developer use cases."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Qwen** (3, 3MoE, 3Next, 2.5, 2 series)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"`Qwen/Qwen3-0.6B`, `Qwen/Qwen3-30B-A3B` `Qwen/Qwen3-Next-80B-A3B-Instruct `"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Alibaba’s latest Qwen3 series for complex reasoning, language understanding, "
"and generation tasks; Support for MoE variants along with previous "
"generation 2.5, 2, etc. [SGLang provides Qwen3 specific reasoning parser](../"
"advanced_features/separate_reasoning.ipynb)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Llama** (2, 3.x, 4 series)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`meta-llama/Llama-4-Scout-17B-16E-Instruct`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Meta's open LLM series, spanning 7B to 400B parameters (Llama 2, 3, and new "
"Llama 4) with well-recognized performance. [SGLang provides Llama-4 model-"
"specific optimizations](../basic_usage/llama4.md)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Mistral** (Mixtral, NeMo, Small3)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`mistralai/Mistral-7B-Instruct-v0.2`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Open 7B LLM by Mistral AI with strong performance; extended into MoE "
"(“Mixtral”) and NeMo Megatron variants for larger scale."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Gemma** (v1, v2, v3)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`google/gemma-3-1b-it`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Google’s family of efficient multilingual models (1B–27B); Gemma 3 offers a "
"128K context window, and its larger (4B+) variants support vision input."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Phi** (Phi-1.5, Phi-2, Phi-3, Phi-4, Phi-MoE series)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`microsoft/Phi-4-multimodal-instruct`, `microsoft/Phi-3.5-MoE-instruct`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Microsoft’s Phi family of small models (1.3B–5.6B); Phi-4-multimodal (5.6B) "
"processes text, images, and speech, Phi-4-mini is a high-accuracy text model "
"and Phi-3.5-MoE is a mixture-of-experts model."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**MiniCPM** (v3, 4B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`openbmb/MiniCPM3-4B`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"OpenBMB’s series of compact LLMs for edge devices; MiniCPM 3 (4B) achieves "
"GPT-3.5-level results in text tasks."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**OLMo** (2, 3)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`allenai/OLMo-2-1124-7B-Instruct`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Allen AI’s series of Open Language Models designed to enable the science of "
"language models."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**OLMoE** (Open MoE)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`allenai/OLMoE-1B-7B-0924`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Allen AI’s open Mixture-of-Experts model (7B total, 1B active parameters) "
"delivering state-of-the-art results with sparse expert activation."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**MiniMax-M2** (M2, M2.1)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`minimax/MiniMax-M2`, `minimax/MiniMax-M2.1`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "MiniMax’s SOTA LLM for coding & agentic workflows."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**StableLM** (3B, 7B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`stabilityai/stablelm-tuned-alpha-7b`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"StabilityAI’s early open-source LLM (3B & 7B) for general text generation; a "
"demonstration model with basic instruction-following ability."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Command-R** (Cohere)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`CohereForAI/c4ai-command-r-v01`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Cohere’s open conversational LLM (Command series) optimized for long "
"context, retrieval-augmented generation, and tool use."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**DBRX** (Databricks)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`databricks/dbrx-instruct`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Databricks’ 132B-parameter MoE model (36B active) trained on 12T tokens; "
"competes with GPT-3.5 quality as a fully open foundation model."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Grok** (xAI)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`xai-org/grok-1`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"xAI’s grok-1 model known for vast size(314B parameters) and high quality; "
"integrated in SGLang for high-performance inference."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**ChatGLM** (GLM-130B family)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`THUDM/chatglm2-6b`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Zhipu AI’s bilingual chat model (6B) excelling at Chinese-English dialogue; "
"fine-tuned for conversational quality and alignment."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**InternLM 2** (7B, 20B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`internlm/internlm2-7b`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Next-gen InternLM (7B and 20B) from SenseTime, offering strong reasoning and "
"ultra-long context support (up to 200K tokens)."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**ExaONE 3** (Korean-English)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"LG AI Research’s Korean-English model (7.8B) trained on 8T tokens; provides "
"high-quality bilingual understanding and generation."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Baichuan 2** (7B, 13B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`baichuan-inc/Baichuan2-13B-Chat`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"BaichuanAI’s second-generation Chinese-English LLM (7B/13B) with improved "
"performance and an open commercial license."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**XVERSE** (MoE)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`xverse/XVERSE-MoE-A36B`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Yuanxiang’s open MoE LLM (XVERSE-MoE-A36B: 255B total, 36B active) "
"supporting ~40 languages; delivers 100B+ dense-level performance via expert "
"routing."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**SmolLM** (135M–1.7B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`HuggingFaceTB/SmolLM-1.7B`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Hugging Face’s ultra-small LLM series (135M–1.7B params) offering "
"surprisingly strong results, enabling advanced AI on mobile/edge devices."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**GLM-4** (Multilingual 9B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`ZhipuAI/glm-4-9b-chat`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Zhipu’s GLM-4 series (up to 9B parameters) – open multilingual models with "
"support for 1M-token context and even a 5.6B multimodal variant (Phi-4V)."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**MiMo** (7B series)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`XiaomiMiMo/MiMo-7B-RL`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Xiaomi's reasoning-optimized model series, leverages Multiple-Token "
"Prediction for faster inference."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**ERNIE-4.5** (4.5, 4.5MoE series)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`baidu/ERNIE-4.5-21B-A3B-PT`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Baidu's ERNIE-4.5 series which consists of MoE with 47B and 3B active "
"parameters, with the largest model having 424B total parameters, as well as "
"a 0.3B dense model."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Arcee AFM-4.5B**"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`arcee-ai/AFM-4.5B-Base`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Arcee's foundational model series for real world reliability and edge "
"deployments."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Persimmon** (8B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`adept/persimmon-8b-chat`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Adept’s open 8B model with a 16K context window and fast inference; trained "
"for broad usability and licensed under Apache 2.0."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Solar** (10.7B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`upstage/SOLAR-10.7B-Instruct-v1.0`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Upstage's 10.7B parameter model, optimized for instruction-following tasks. "
"This architecture incorporates a depth-up scaling methodology, enhancing "
"model performance."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Tele FLM** (52B-1T)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`CofeAI/Tele-FLM`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"BAAI & TeleAI's multilingual model, available in 52-billion and 1-trillion "
"parameter variants. It is a decoder-only transformer trained on ~2T tokens"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Ling** (16.8B–290B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`inclusionAI/Ling-lite`, `inclusionAI/Ling-plus`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"InclusionAI’s open MoE models. Ling-Lite has 16.8B total / 2.75B active "
"parameters, and Ling-Plus has 290B total / 28.8B active parameters. They are "
"designed for high performance on NLP and complex reasoning tasks."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Granite 3.0, 3.1** (IBM)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`ibm-granite/granite-3.1-8b-instruct`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"IBM's open dense foundation models optimized for reasoning, code, and "
"business AI use cases. Integrated with Red Hat and watsonx systems."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Granite 3.0 MoE** (IBM)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`ibm-granite/granite-3.0-3b-a800m-instruct`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"IBM’s Mixture-of-Experts models offering strong performance with cost-"
"efficiency. MoE expert routing designed for enterprise deployment at scale."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Orion** (14B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`OrionStarAI/Orion-14B-Base`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"A series of open-source multilingual large language models by OrionStarAI, "
"pretrained on a 2.5T token multilingual corpus including Chinese, English, "
"Japanese, Korean, etc, and it exhibits superior performance in these "
"languages."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Llama Nemotron Super** (v1, v1.5, NVIDIA)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"`nvidia/Llama-3_3-Nemotron-Super-49B-v1`, `nvidia/Llama-3_3-Nemotron-"
"Super-49B-v1_5`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"The [NVIDIA Nemotron](https://www.nvidia.com/en-us/ai-data-science/"
"foundation-models/nemotron/) family of multimodal models provides state-of-"
"the-art reasoning models specifically designed for enterprise-ready AI "
"agents."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Llama Nemotron Ultra** (v1, NVIDIA)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`nvidia/Llama-3_1-Nemotron-Ultra-253B-v1`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**NVIDIA Nemotron Nano 2.0**"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`nvidia/NVIDIA-Nemotron-Nano-9B-v2`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"The [NVIDIA Nemotron](https://www.nvidia.com/en-us/ai-data-science/"
"foundation-models/nemotron/) family of multimodal models provides state-of-"
"the-art reasoning models specifically designed for enterprise-ready AI "
"agents. `Nemotron-Nano-9B-v2` is a hybrid Mamba-Transformer language model "
"designed to increase throughput for reasoning workloads while achieving "
"state-of-the-art accuracy compared to similarly-sized models."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**StarCoder2** (3B-15B)"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`bigcode/starcoder2-7b`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"StarCoder2 is a family of open large language models (LLMs) specialized for "
"code generation and understanding. It is the successor to StarCoder, jointly "
"developed by the BigCode project (a collaboration between Hugging Face, "
"ServiceNow Research, and other contributors)."
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "**Jet-Nemotron**"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid "`jet-ai/Jet-Nemotron-2B`"
msgstr ""

#: ../../../supported_models/generative_models.md:0
msgid ""
"Jet-Nemotron is a new family of hybrid-architecture language models that "
"surpass state-of-the-art open-source full-attention language models, while "
"achieving significant efficiency gains."
msgstr ""
