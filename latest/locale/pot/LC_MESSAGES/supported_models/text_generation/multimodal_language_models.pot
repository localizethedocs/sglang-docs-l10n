# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2026, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-02-16 08:46+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../supported_models/text_generation/multimodal_language_models.md:1
msgid "Multimodal Language Models"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:3
msgid ""
"These models accept multi-modal inputs (e.g., images and text) and generate "
"text output. They augment language models with multimodal encoders."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:5
msgid "Example launch Command"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:7
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model-path meta-llama/Llama-3.2-11B-Vision-Instruct \\  # example HF/"
"local path\n"
"  --host 0.0.0.0 \\\n"
"  --port 30000 \\\n"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:14
msgid ""
"See the [OpenAI APIs section](https://docs.sglang.io/basic_usage/"
"openai_api_vision.html) for how to send multimodal requests."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:16
msgid "Supported models"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:18
msgid "Below the supported models are summarized in a table."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:20
msgid ""
"If you are unsure if a specific architecture is implemented, you can search "
"for it via GitHub. For example, to search for "
"`Qwen2_5_VLForConditionalGeneration`, use the expression:"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:22
msgid ""
"repo:sgl-project/sglang path:/^python\\/sglang\\/srt\\/models\\// "
"Qwen2_5_VLForConditionalGeneration\n"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:26
msgid "in the GitHub search bar."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "Model Family (Variants)"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "Example HuggingFace Identifier"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "Description"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "Notes"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**Qwen-VL**"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`Qwen/Qwen3-VL-235B-A22B-Instruct`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Alibaba's vision-language extension of Qwen; for example, Qwen2.5-VL (7B and "
"larger variants) can analyze and converse about image content."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**DeepSeek-VL2**"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`deepseek-ai/deepseek-vl2`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Vision-language variant of DeepSeek (with a dedicated image processor), "
"enabling advanced multimodal reasoning on image and text inputs."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**DeepSeek-OCR / OCR-2**"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`deepseek-ai/DeepSeek-OCR-2`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"OCR-focused DeepSeek models for document understanding and text extraction."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "Use `--trust-remote-code`."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**Janus-Pro** (1B, 7B)"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`deepseek-ai/Janus-Pro-7B`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"DeepSeek's open-source multimodal model capable of both image understanding "
"and generation. Janus-Pro employs a decoupled architecture for separate "
"visual encoding paths, enhancing performance in both tasks."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**MiniCPM-V / MiniCPM-o**"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`openbmb/MiniCPM-V-2_6`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"MiniCPM-V (2.6, ~8B) supports image inputs, and MiniCPM-o adds audio/video; "
"these multimodal LLMs are optimized for end-side deployment on mobile/edge "
"devices."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**Llama 3.2 Vision** (11B)"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`meta-llama/Llama-3.2-11B-Vision-Instruct`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Vision-enabled variant of Llama 3 (11B) that accepts image inputs for visual "
"question answering and other multimodal tasks."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**LLaVA** (v1.5 & v1.6)"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "*e.g.* `liuhaotian/llava-v1.5-13b`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Open vision-chat models that add an image encoder to LLaMA/Vicuna (e.g. "
"LLaMA2 13B) for following multimodal instruction prompts."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**LLaVA-NeXT** (8B, 72B)"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`lmms-lab/llava-next-72b`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Improved LLaVA models (with an 8B Llama3 version and a 72B version) offering "
"enhanced visual instruction-following and accuracy on multimodal benchmarks."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**LLaVA-OneVision**"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`lmms-lab/llava-onevision-qwen2-7b-ov`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Enhanced LLaVA variant integrating Qwen as the backbone; supports multiple "
"images (and even video frames) as inputs via an OpenAI Vision API-compatible "
"format."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**Gemma 3 (Multimodal)**"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`google/gemma-3-4b-it`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Gemma 3's larger models (4B, 12B, 27B) accept images (each image encoded as "
"256 tokens) alongside text in a combined 128K-token context."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**Kimi-VL** (A3B)"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`moonshotai/Kimi-VL-A3B-Instruct`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Kimi-VL is a multimodal model that can understand and generate text from "
"images."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**Mistral-Small-3.1-24B**"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`mistralai/Mistral-Small-3.1-24B-Instruct-2503`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Mistral 3.1 is a multimodal model that can generate text from text or images "
"input. It also supports tool calling and structured output."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**Phi-4-multimodal-instruct**"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`microsoft/Phi-4-multimodal-instruct`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Phi-4-multimodal-instruct is the multimodal variant of the Phi-4-mini model, "
"enhanced with LoRA for improved multimodal capabilities. It supports text, "
"vision and audio modalities in SGLang."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**MiMo-VL** (7B)"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`XiaomiMiMo/MiMo-VL-7B-RL`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Xiaomi's compact yet powerful vision-language model featuring a native "
"resolution ViT encoder for fine-grained visual details, an MLP projector for "
"cross-modal alignment, and the MiMo-7B language model optimized for complex "
"reasoning tasks."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**GLM-4.5V** (106B) /  **GLM-4.1V**(9B)"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`zai-org/GLM-4.5V`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with "
"Scalable Reinforcement Learning"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "Use `--chat-template glm-4v`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**GLM-OCR**"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`zai-org/GLM-OCR`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "GLM-OCR: A fast and accurate general OCR model"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**DotsVLM** (General/OCR)"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`rednote-hilab/dots.vlm1.inst`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"RedNote's vision-language model built on a 1.2B vision encoder and DeepSeek "
"V3 LLM, featuring NaViT vision encoder trained from scratch with dynamic "
"resolution support and enhanced OCR capabilities through structured image "
"data training."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**DotsVLM-OCR**"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`rednote-hilab/dots.ocr`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Specialized OCR variant of DotsVLM optimized for optical character "
"recognition tasks with enhanced text extraction and document understanding "
"capabilities."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "Don't use `--trust-remote-code`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**NVILA** (8B, 15B, Lite-2B, Lite-8B, Lite-15B)"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`Efficient-Large-Model/NVILA-8B`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`chatml`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"NVILA explores the full stack efficiency of multi-modal design, achieving "
"cheaper training, faster deployment and better performance."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**NVIDIA Nemotron Nano 2.0 VL**"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`nvidia/NVIDIA-Nemotron-Nano-12B-v2-VL-BF16`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"NVIDIA Nemotron Nano v2 VL enables multi-image reasoning and video "
"understanding, along with strong document intelligence, visual Q&A and "
"summarization capabilities. It builds on Nemotron Nano V2, a hybrid Mamba-"
"Transformer LLM, in order to achieve higher inference throughput in long "
"document and video scenarios."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Use `--trust-remote-code`. You may need to adjust `--max-mamba-cache-size` "
"[default is 512] to fit memory constraints."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**Ernie4.5-VL**"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`baidu/ERNIE-4.5-VL-28B-A3B-PT`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Baidu's vision-language models(28B,424B). Support image and video "
"comprehension, and also support thinking."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**JetVLM**"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"JetVLM is an vision-language model designed for high-performance multimodal "
"understanding and generation tasks built upon Jet-Nemotron."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "Coming soon"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**Step3-VL** (10B)"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`stepfun-ai/Step3-VL-10B`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"StepFun's lightweight open-source 10B parameter VLM for multimodal "
"intelligence, excelling in visual perception, complex reasoning, and human "
"alignment."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**Qwen3-Omni**"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`Qwen/Qwen3-Omni-30B-A3B-Instruct`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Alibaba's omni-modal MoE model. Currently supports the **Thinker** component "
"(multimodal understanding for text, images, audio, and video), while the "
"**Talker** component (audio generation) is not yet supported."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:56
msgid "Video Input Support"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:58
msgid ""
"SGLang supports video input for Vision-Language Models (VLMs), enabling "
"temporal reasoning tasks such as video question answering, captioning, and "
"holistic scene understanding. Video clips are decoded, key frames are "
"sampled, and the resulting tensors are batched together with the text "
"prompt, allowing multimodal inference to integrate visual and linguistic "
"context."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "Model Family"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "Example Identifier"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "Video notes"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**Qwen-VL** (Qwen2-VL, Qwen2.5-VL, Qwen3-VL, Qwen3-Omni)"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"The processor gathers `video_data`, runs Qwen's frame sampler, and merges "
"the resulting features with text tokens before inference."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**GLM-4v** (4.5V, 4.1V, MOE)"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"Video clips are read with Decord, converted to tensors, and passed to the "
"model alongside metadata for rotary-position handling."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**NVILA** (Full & Lite)"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"The runtime samples eight frames per clip and attaches them to the "
"multimodal request when `video_data` is present."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "**LLaVA video variants** (LLaVA-NeXT-Video, LLaVA-OneVision)"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid "`lmms-lab/LLaVA-NeXT-Video-7B`"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"The processor routes video prompts to the LlavaVid video-enabled "
"architecture, and the provided example shows how to query it with `sgl."
"video(...)` clips."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:0
msgid ""
"The processor samples at 2 FPS, at a max of 128 frames, as per model "
"training. The model uses [EVS](../../python/sglang/srt/multimodal/evs/README."
"md), a pruning method that removes redundant tokens from video embeddings. "
"By default `video_pruning_rate=0.7`. Change this by providing: `--json-model-"
"override-args '{\"video_pruning_rate\": 0.0}'` to disable EVS, for example."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:69
msgid ""
"Use `sgl.video(path, num_frames)` when building prompts to attach clips from "
"your SGLang programs."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:71
msgid "Example OpenAI-compatible request that sends a video clip:"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:73
msgid ""
"import requests\n"
"\n"
"url = \"http://localhost:30000/v1/chat/completions\"\n"
"\n"
"data = {\n"
"    \"model\": \"Qwen/Qwen3-VL-30B-A3B-Instruct\",\n"
"    \"messages\": [\n"
"        {\n"
"            \"role\": \"user\",\n"
"            \"content\": [\n"
"                {\"type\": \"text\", \"text\": \"Whatâ€™s happening in this "
"video?\"},\n"
"                {\n"
"                    \"type\": \"video_url\",\n"
"                    \"video_url\": {\n"
"                        \"url\": \"https://github.com/sgl-project/sgl-test-"
"files/raw/refs/heads/main/videos/jobs_presenting_ipod.mp4\"\n"
"                    },\n"
"                },\n"
"            ],\n"
"        }\n"
"    ],\n"
"    \"max_tokens\": 300,\n"
"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"print(response.text)\n"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:101
msgid "Usage Notes"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:103
msgid "Performance Optimization"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:105
msgid ""
"For multimodal models, you can use the `--keep-mm-feature-on-device` flag to "
"optimize for latency at the cost of increased GPU memory usage:"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:107
msgid ""
"**Default behavior**: Multimodal feature tensors are moved to CPU after "
"processing to save GPU memory"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:108
msgid ""
"**With `--keep-mm-feature-on-device`**: Feature tensors remain on GPU, "
"reducing device-to-host copy overhead and improving latency, but consuming "
"more GPU memory"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:110
msgid ""
"Use this flag when you have sufficient GPU memory and want to minimize "
"latency for multimodal inference."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:112
msgid "Multimodal Inputs Limitation"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:114
msgid ""
"**Use `--mm-process-config '{\"image\":{\"max_pixels\":1048576},\"video\":"
"{\"fps\":3,\"max_pixels\":602112,\"max_frames\":60}}'`**: To set `image`, "
"`video`, and `audio` input limits."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:116
msgid ""
"This can reduce GPU memory usage, improve inference speed, and help to avoid "
"OOM, but may impact model performance, thus set a proper value based on your "
"specific use case. Currently, only `qwen_vl` supports this config. Please "
"refer to [qwen_vl processor](https://github.com/sgl-project/sglang/blob/main/"
"python/sglang/srt/multimodal/processors/qwen_vl.py) for understanding the "
"meaning of each parameter."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:118
msgid "Bidirectional Attention in Multimodal Model Serving"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:119
msgid "**Note for serving the Gemma-3 multimodal model**:"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:121
msgid ""
"As mentioned in [Welcome Gemma 3: Google's all new multimodal, multilingual, "
"long context open LLM ](https://huggingface.co/blog/gemma3#multimodality), "
"Gemma-3 employs bidirectional attention between image tokens during the "
"prefill phase. Currently, SGLang only supports bidirectional attention when "
"using the Triton Attention Backend. Note, however, that SGLang's current "
"bidirectional attention implementation is incompatible with both CUDA Graph "
"and Chunked Prefill."
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:124
msgid ""
"To enable bidirectional attention, you can use the `TritonAttnBackend` while "
"disabling CUDA Graph and Chunked Prefill. Example launch command:"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:125
msgid ""
"python -m sglang.launch_server \\\n"
"  --model-path google/gemma-3-4b-it \\\n"
"  --host 0.0.0.0 --port 30000 \\\n"
"  --enable-multimodal \\\n"
"  --dtype bfloat16 --triton-attention-reduce-in-fp32 \\\n"
"  --attention-backend triton \\ # Use Triton attention backend\n"
"  --disable-cuda-graph \\ # Disable Cuda Graph\n"
"  --chunked-prefill-size -1 # Disable Chunked Prefill\n"
msgstr ""

#: ../../../supported_models/text_generation/multimodal_language_models.md:136
msgid ""
"If higher serving performance is required and a certain degree of accuracy "
"loss is acceptable, you may choose to use other attention backends, and you "
"can also enable features like CUDA Graph and Chunked Prefill for better "
"performance, but note that the model will fall back to using causal "
"attention instead of bidirectional attention."
msgstr ""
