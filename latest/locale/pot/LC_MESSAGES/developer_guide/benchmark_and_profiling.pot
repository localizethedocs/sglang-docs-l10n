# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-10 08:34+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../developer_guide/benchmark_and_profiling.md:1
msgid "Benchmark and Profiling"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:3
msgid "Benchmark"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:5
msgid ""
"Benchmark the latency of running a single static batch without a server. The "
"arguments are the same as for `launch_server.py`. Note that this is a "
"simplified test script without a dynamic batching server, so it may run out "
"of memory for a batch size that a real server can handle. A real server "
"truncates the prefill into several batches, while this simplified script "
"does not."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:7
msgid "Without a server (do not need to launch a server)"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:8
msgid ""
"python -m sglang.bench_one_batch --model-path meta-llama/Meta-Llama-3.1-8B-"
"Instruct --batch 32 --input-len 256 --output-len 32\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:11
msgid ""
"With a server (please use `sglang.launch_server` to launch a server first "
"and run the following command.)"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:12
msgid ""
"python -m sglang.bench_one_batch_server --base-url http://127.0.0.1:30000 --"
"model-path meta-llama/Meta-Llama-3.1-8B-Instruct --batch-size 32 --input-len "
"256 --output-len 32\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:17
msgid ""
"Benchmark offline processing. This script will start an offline engine and "
"run the benchmark."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:19
msgid ""
"python3 -m sglang.bench_offline_throughput --model-path meta-llama/Meta-"
"Llama-3.1-8B-Instruct --num-prompts 10\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:23
msgid ""
"Benchmark online serving. Please use `sglang.launch_server` to launch a "
"server first and run the following command."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:25
msgid "python3 -m sglang.bench_serving --backend sglang --num-prompt 10\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:29
msgid "Profile with PyTorch Profiler"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:31
msgid ""
"[Pytorch Profiler](https://pytorch.org/tutorials/recipes/recipes/"
"profiler_recipe.html) is a convenient basic tool to inspect kernel execution "
"time, call stack, and kernel overlap and occupancy."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:33
msgid "Profile a server with `sglang.bench_serving`"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:35
msgid ""
"# set trace path\n"
"export SGLANG_TORCH_PROFILER_DIR=/root/sglang/profile_log\n"
"\n"
"# start server\n"
"python -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-"
"Instruct\n"
"\n"
"# send profiling request from client\n"
"python -m sglang.bench_serving --backend sglang --model meta-llama/"
"Llama-3.1-8B-Instruct --num-prompts 10 --sharegpt-output-len 100 --profile\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:46
msgid ""
"Please make sure that the `SGLANG_TORCH_PROFILER_DIR` should be set at both "
"server and client side, otherwise the trace file cannot be generated "
"correctly . A secure way will be setting `SGLANG_TORCH_PROFILER_DIR` in the "
"`.*rc` file of shell (e.g. `~/.bashrc` for bash shells)."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:48
msgid ""
"For more details, please refer to [Bench Serving Guide](./bench_serving.md)."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:50
msgid "Profile In PD Disaggregation Mode"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:52
msgid ""
"When profiling in PD disaggregation mode, prefill and decode workers **must "
"be profiled separately** due to torch profiler limitations. The "
"`bench_serving` command provides dedicated options for this:"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:54
msgid "Profile Prefill Workers"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:56
msgid ""
"# set trace path\n"
"export SGLANG_TORCH_PROFILER_DIR=/root/sglang/profile_log\n"
"\n"
"# start prefill and decode servers (see PD disaggregation docs for setup)\n"
"python -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct "
"--disaggregation-mode prefill\n"
"python -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct "
"--disaggregation-mode decode --port 30001 --base-gpu-id 1\n"
"\n"
"# start router\n"
"python -m sglang_router.launch_router --pd-disaggregation --prefill "
"http://127.0.0.1:30000 --decode http://127.0.0.1:30001 --host 0.0.0.0 --port "
"8000\n"
"\n"
"# send profiling request targeting prefill workers\n"
"python -m sglang.bench_serving --backend sglang --model meta-llama/"
"Llama-3.1-8B-Instruct --num-prompts 10 --sharegpt-output-len 100 --profile --"
"pd-separated --profile-prefill-url http://127.0.0.1:30000\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:71
msgid "Profile Decode Workers"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:73
msgid ""
"# send profiling request targeting decode workers\n"
"python -m sglang.bench_serving --backend sglang --model meta-llama/"
"Llama-3.1-8B-Instruct --num-prompts 10 --sharegpt-output-len 100 --profile --"
"pd-separated --profile-decode-url http://127.0.0.1:30001\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:78
msgid "Important Notes"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:80
msgid ""
"`--profile-prefill-url` and `--profile-decode-url` are **mutually "
"exclusive** - you cannot profile both at the same time"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:81
msgid "Both options support multiple worker URLs for multi-instance setups:"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:82
msgid ""
"# Profile multiple prefill workers\n"
"python -m sglang.bench_serving --backend sglang --model meta-llama/"
"Llama-3.1-8B-Instruct --num-prompts 10 --profile --pd-separated --profile-"
"prefill-url http://127.0.0.1:30000 http://127.0.0.1:30002\n"
"\n"
"# Profile multiple decode workers\n"
"python -m sglang.bench_serving --backend sglang --model meta-llama/"
"Llama-3.1-8B-Instruct --num-prompts 10 --profile --pd-separated --profile-"
"decode-url http://127.0.0.1:30001 http://127.0.0.1:30003\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:89
msgid ""
"Make sure `SGLANG_TORCH_PROFILER_DIR` is set on all worker nodes before "
"starting the servers"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:90
msgid ""
"For more details on setting up PD disaggregation, see [PD Disaggregation "
"Guide](../advanced_features/pd_disaggregation.md)"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:92
msgid "Profile a server with `sglang.bench_offline_throughput`"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:93
msgid ""
"export SGLANG_TORCH_PROFILER_DIR=/root/sglang/profile_log\n"
"\n"
"# profile one batch with bench_one_batch.py\n"
"# batch size can be controlled with --batch argument\n"
"python3 -m sglang.bench_one_batch --model-path meta-llama/Llama-3.1-8B-"
"Instruct --batch 32 --input-len 1024 --output-len 10 --profile\n"
"\n"
"# profile multiple batches with bench_offline_throughput.py\n"
"python -m sglang.bench_offline_throughput --model-path meta-llama/"
"Llama-3.1-8B-Instruct --dataset-name random --num-prompts 10 --profile --mem-"
"frac=0.8\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:104
msgid "Profile a server with `sglang.profiler`"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:106
msgid ""
"When the server is running (e.g., processing a decoding request), you can "
"start live profiling immediately by sending a profile request to the server."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:108
msgid "You can do this by running `python3 -m sglang.profiler`. For example:"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:110
msgid ""
"# Terminal 1: Send a generation request\n"
"python3 -m sglang.test.send_one\n"
"\n"
"# Terminal 2: Before the above request finishes, quickly launch the "
"following command in a separate terminal.\n"
"# It will generate a profile of the above request for several decoding "
"batches.\n"
"python3 -m sglang.profiler\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:119
msgid "You can also combine the above operations into a single command"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:121
msgid "python3 -m sglang.test.send_one --profile\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:125
msgid "Profiler Trace Merger for Distributed Traces"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:127
msgid ""
"SGLang now supports automatic merging of profiling traces from distributed "
"setups with multiple parallelism types (TP, DP, PP, EP). This feature is "
"particularly useful for analyzing performance across distributed runs."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:129
msgid "Multi-Node Profiling and Shared Storage Considerations"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:131
msgid ""
"Single-node profiler output merging is completely supported. When profiling "
"in distributed environments spanning multiple nodes, shared storage (e.g., "
"NFS, Lustre) should be accessible by all nodes for the output directory to "
"enable merging of trace files."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:133
msgid ""
"If there is no shared storage accessible across nodes, automatic merging of "
"trace files during profiling is not supported directly as of now."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:135
msgid "HTTP API Usage"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:137
msgid ""
"# Start profiling with automatic trace merging enabled\n"
"curl -X POST <BASE_URL>/start_profile \\\n"
"  -H \"Content-Type: application/json\" \\\n"
"  -d '{\n"
"    \"output_dir\": \"/tmp/profiles\", # where to store profile traces\n"
"    \"num_steps\": 10,\n"
"    \"activities\": [\"CPU\", \"GPU\"],\n"
"    \"merge_profiles\": true # optional argument to merge profile traces "
"(default=False)\n"
"  }'\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:149
msgid "Command Line Usage"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:151
msgid ""
"# Start profiling with merge enabled\n"
"python -m sglang.profiler \\\n"
"  --num-steps 10 \\\n"
"  --cpu \\\n"
"  --gpu \\\n"
"  --output-dir /tmp/profiles \\\n"
"  --merge-profiles # optional argument to merge profile traces "
"(default=False)\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:161
msgid "Output Files"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:163
msgid "The profile merger generates:"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:164
msgid ""
"Individual rank trace files: `{profile_id}-TP-{tp}-DP-{dp}-PP-{pp}-EP-{ep}."
"trace.json.gz`"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:165
msgid "Merged trace file: `merged-{profile_id}.trace.json.gz`"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:167
msgid "Possible PyTorch bugs"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:168
msgid ""
"If in any cases you encounter the following error (for example, using qwen "
"2.5 VL):"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:169
msgid ""
"RuntimeError: !stack.empty() INTERNAL ASSERT FAILED at \"/pytorch/torch/csrc/"
"autograd/profiler_python.cpp\":983, please report a bug to PyTorch. Python "
"replay stack is empty.\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:172
msgid ""
"This is likely a PyTorch Bug reported in [Bug: vLLM Profiler](https://github."
"com/vllm-project/vllm/issues/18240) and [Bug: torch.profiler.profile]"
"(https://github.com/pytorch/pytorch/issues/101632). As a workaround, you may "
"disable `with_stack` with an environment variable such as follows:"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:173
msgid ""
"export SGLANG_PROFILE_WITH_STACK=False\n"
"python -m sglang.bench_offline_throughput --model-path meta-llama/"
"Llama-3.1-8B-Instruct --dataset-name random --num-prompts 10 --profile --mem-"
"frac=0.8\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:178
msgid "View traces"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:180
msgid "Trace files can be loaded and visualized from:"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:182
msgid "https://ui.perfetto.dev/ (any browser)"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:183
msgid "chrome://tracing (Chrome browser only)"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:185
msgid ""
"If browser cannot open trace file due to its large size, client can generate "
"a small trace file (<100MB) by controlling number of prompts and lengths of "
"prompt outputs. For example, when profiling a server,"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:189
msgid ""
"python -m sglang.bench_serving --backend sglang --model meta-llama/"
"Llama-3.1-8B-Instruct --num-prompts 2 --sharegpt-output-len 100 --profile\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:193
msgid ""
"This command sets the number of prompts to 2 with `--num-prompts` argument "
"and limits the length of output sequences to 100 with `--sharegpt-output-"
"len` argument, which can generate a small trace file for browser to open "
"smoothly."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:195
msgid ""
"Additionally, if you want to locate the SGLang Python source code through "
"the cuda kernel in Trace, you need to disable CUDA Graph when starting the "
"service. This can be done by using the `--disable-cuda-graph` parameter in "
"the command to start the service."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:197
msgid "Profile with Nsight"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:199
msgid ""
"[Nsight systems](https://docs.nvidia.com/nsight-systems/) is an advanced "
"tool that exposes more profiling details, such as register and shared memory "
"usage, annotated code regions and low-level CUDA APIs and events."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:201
msgid "Prerequisite:"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:203
msgid ""
"Install using apt, or run inside a [NVIDIA Docker container](https://catalog."
"ngc.nvidia.com/orgs/nvidia/containers/pytorch/tags) or [SGLang Docker "
"container](https://github.com/sgl-project/sglang/tree/main/docker)."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:205
msgid ""
"# install nsys\n"
"# https://docs.nvidia.com/nsight-systems/InstallationGuide/index.html\n"
"apt update\n"
"apt install -y --no-install-recommends gnupg\n"
"echo \"deb http://developer.download.nvidia.com/devtools/repos/"
"ubuntu$(source /etc/lsb-release; echo \"$DISTRIB_RELEASE\" | tr -d .)/$(dpkg "
"--print-architecture) /\" | tee /etc/apt/sources.list.d/nvidia-devtools."
"list\n"
"apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/"
"repos/ubuntu1804/x86_64/7fa2af80.pub\n"
"apt update\n"
"apt install nsight-systems-cli\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:216
msgid "To profile a single batch, use"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:218
msgid ""
"nsys profile --trace-fork-before-exec=true --cuda-graph-trace=node python3 -"
"m sglang.bench_one_batch --model meta-llama/Meta-Llama-3-8B --batch-size 64 "
"--input-len 512\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:222
msgid "To profile a server, e.g."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:224
msgid ""
"# launch the server, set the delay and duration times according to needs\n"
"# after the duration time has been used up, server will be killed by nsys\n"
"\n"
"nsys profile --trace-fork-before-exec=true --cuda-graph-trace=node -o sglang."
"out --delay 60 --duration 70 python3 -m sglang.launch_server --model-path "
"meta-llama/Llama-3.1-8B-Instruct --disable-radix-cache\n"
"\n"
"# client\n"
"python3 -m sglang.bench_serving --backend sglang --num-prompts 1000 --"
"dataset-name random --random-input 1024 --random-output 512\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:234
msgid ""
"In practice, we recommend users to set `--duration` argument to a large "
"value. Whenever user wants the server to stop profiling. Firstly run:"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:236
msgid "nsys sessions list\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:240
msgid "to get the session id in the form of `profile-XXXXX`, then run:"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:242
msgid "nsys stop --session=profile-XXXXX\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:246
msgid "to manually kill the profiler and generate `nsys-rep` files instantly."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:248
msgid "Use NVTX to annotate code regions, e.g. to see their execution time."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:250
msgid ""
"# install nvtx\n"
"pip install nvtx\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:255
msgid ""
"# code snippets\n"
"import nvtx\n"
"with nvtx.annotate(\"description\", color=\"color\"):\n"
"    # some critical code\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:262
msgid "Other tips"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:264
msgid ""
"You can benchmark a model using dummy weights by only providing the config."
"json file. This allows for quick testing of model variants without training. "
"To do so, add `--load-format dummy` to the above commands and then you only "
"need a correct `config.json` under the checkpoint folder."
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:265
msgid ""
"You can benchmark a model with modified configs (e.g., less layers) by using "
"`--json-model-override-args`. For example, you can benchmark a model with "
"only 2 layers and 2 kv heads using:"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:267
msgid ""
"python -m sglang.bench_one_batch --model-path meta-llama/Meta-Llama-3.1-8B-"
"Instruct --batch 32 --input-len 256 --output-len 32 --load-format dummy --"
"json-model-override-args '{\"num_hidden_layers\": 1, "
"\"num_key_value_heads\": 1}'\n"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:271
msgid ""
"You can use `--python-backtrace=cuda` to see python call stack for all CUDA "
"kernels, as in PyTorch Profiler. (Caveat: this can cause inaccurately long "
"kernel runtimes for CUDA event based timing)"
msgstr ""

#: ../../../developer_guide/benchmark_and_profiling.md:272
msgid ""
"For more arguments see [Nsight Systems User Guide](https://docs.nvidia.com/"
"nsight-systems/UserGuide/index.html)."
msgstr ""
