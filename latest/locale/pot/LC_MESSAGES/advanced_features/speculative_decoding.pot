# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-10 08:34+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../advanced_features/speculative_decoding.ipynb:2
msgid ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:9
msgid "Speculative Decoding"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:11
msgid ""
"SGLang now provides an EAGLE-based (EAGLE-2/EAGLE-3) speculative decoding "
"option. Our implementation aims to maximize speed and efficiency and is "
"considered to be among the fastest in open-source LLM engines."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:14
msgid "Performance Highlights"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:16
msgid ""
"Please see below for the huge improvements on throughput for LLaMA-Instruct "
"3.1 8B tested on MT bench that can be achieved via EAGLE3 decoding. For "
"further details please see the `EAGLE3 paper <https://arxiv.org/"
"pdf/2503.01840>`__."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:19
msgid "Method"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:19
msgid "Throughput (tokens/s)"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:21
msgid "SGLang (w/o speculative, 1x H100)"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:21
msgid "158.34 tokens/s"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:22
msgid "SGLang + EAGLE-2 (1x H100)"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:22
msgid "244.10 tokens/s"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:23
msgid "SGLang + EAGLE-3 (1x H100)"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:23
msgid "373.25 tokens/s"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:36
msgid "EAGLE Decoding"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:38
msgid ""
"To enable EAGLE speculative decoding the following parameters are relevant:"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:40
msgid ""
"``speculative_draft_model_path``: Specifies draft model. This parameter is "
"required."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:41
msgid ""
"``speculative_num_steps``: Depth of autoregressive drafting. Increases "
"speculation range but risks rejection cascades. Default is 5."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:42
msgid ""
"``speculative_eagle_topk``: Branching factor per step. Improves candidate "
"diversity, will lead to higher acceptance rate, but more lead to higher "
"memory/compute consumption. Default is 4."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:43
msgid ""
"``speculative_num_draft_tokens``: Maximum parallel verification capacity. "
"Allows deeper tree evaluation but will lead to higher GPU memory usage. "
"Default is 8."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:45
msgid "These parameters are the same for EAGLE-2 and EAGLE-3."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:47
msgid ""
"You can find the best combinations of these parameters with "
"`bench_speculative.py <https://github.com/sgl-project/sglang/blob/main/"
"scripts/playground/bench_speculative.py>`__."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:49
msgid ""
"In the documentation below, we set ``--cuda-graph-max-bs`` to be a small "
"value for faster engine startup. For your own workloads, please tune the "
"above parameters together with ``--cuda-graph-max-bs``, ``--max-running-"
"requests``, ``--mem-fraction-static`` for the best performance."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:61
msgid "EAGLE-2 decoding"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:63
msgid ""
"You can enable EAGLE-2 decoding by setting ``--speculative-algorithm EAGLE`` "
"and choosing an appropriate model."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:-1
msgid "[ ]:"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:-1
msgid ""
"from sglang.test.doc_patch import launch_server_cmd\n"
"from sglang.utils import wait_for_server, print_highlight, "
"terminate_process\n"
"\n"
"import openai"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:-1
msgid ""
"server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model meta-llama/Llama-2-7b-chat-hf  --"
"speculative-algorithm EAGLE \\\n"
"    --speculative-draft-model-path lmsys/sglang-EAGLE-llama2-chat-7B --"
"speculative-num-steps 3 \\\n"
"    --speculative-eagle-topk 4 --speculative-num-draft-tokens 16 --cuda-"
"graph-max-bs 8 --log-level warning\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:-1
msgid ""
"client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", "
"api_key=\"None\")\n"
"\n"
"response = client.chat.completions.create(\n"
"    model=\"meta-llama/Llama-2-7b-chat-hf\",\n"
"    messages=[\n"
"        {\"role\": \"user\", \"content\": \"List 3 countries and their "
"capitals.\"},\n"
"    ],\n"
"    temperature=0,\n"
"    max_tokens=64,\n"
")\n"
"\n"
"print_highlight(f\"Response: {response}\")"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:-1
msgid "terminate_process(server_process)"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:133
msgid "EAGLE-2 Decoding with ``torch.compile``"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:135
msgid ""
"You can also enable ``torch.compile`` for further optimizations and "
"optionally set ``--torch-compile-max-bs``:"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:-1
msgid ""
"server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model meta-llama/Llama-2-7b-chat-hf  --"
"speculative-algorithm EAGLE \\\n"
"    --speculative-draft-model-path lmsys/sglang-EAGLE-llama2-chat-7B --"
"speculative-num-steps 5 \\\n"
"        --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --mem-"
"fraction 0.6 \\\n"
"            --enable-torch-compile --torch-compile-max-bs 2 --log-level "
"warning\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:194
msgid "EAGLE-2 Decoding via Frequency-Ranked Speculative Sampling"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:196
msgid ""
"By employing a truncated high-frequency token vocabulary in the draft model, "
"Eagle speculative decoding reduces ``lm_head`` computational overhead while "
"accelerating the pipeline without quality degradation. For more details, "
"checkout `the paper <https://arxiv.org/pdf/arXiv:2502.14856>`__."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:198
msgid ""
"In our implementation, set ``--speculative-token-map`` to enable the "
"optimization. You can get the high-frequency token in FR-Spec from `this "
"model <https://huggingface.co/thunlp/LLaMA3-Instruct-8B-FR-Spec>`__. Or you "
"can obtain high-frequency token by directly downloading these token from "
"`this repo <https://github.com/thunlp/FR-Spec/tree/main?tab=readme-ov-"
"file#prepare-fr-spec-vocabulary-subset>`__."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:200
msgid ""
"Thanks for the contribution from `Weilin Zhao <https://github.com/"
"Achazwl>`__ and `Zhousx <https://github.com/Zhou-sx>`__."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:-1
msgid ""
"server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model meta-llama/Meta-Llama-3-8B-Instruct "
"--speculative-algorithm EAGLE \\\n"
"    --speculative-draft-model-path lmsys/sglang-EAGLE-LLaMA3-Instruct-8B --"
"speculative-num-steps 5 \\\n"
"    --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --"
"speculative-token-map thunlp/LLaMA3-Instruct-8B-FR-Spec/freq_32768.pt \\\n"
"    --mem-fraction 0.7 --cuda-graph-max-bs 2 --dtype float16  --log-level "
"warning\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:-1
msgid ""
"client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", "
"api_key=\"None\")\n"
"\n"
"response = client.chat.completions.create(\n"
"    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n"
"    messages=[\n"
"        {\"role\": \"user\", \"content\": \"List 3 countries and their "
"capitals.\"},\n"
"    ],\n"
"    temperature=0,\n"
"    max_tokens=64,\n"
")\n"
"\n"
"print_highlight(f\"Response: {response}\")"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:259
msgid "EAGLE-3 Decoding"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:261
msgid ""
"You can enable EAGLE-3 decoding by setting ``--speculative-algorithm "
"EAGLE3`` and choosing an appropriate model."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:-1
msgid ""
"server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model meta-llama/Llama-3.1-8B-Instruct  --"
"speculative-algorithm EAGLE3 \\\n"
"    --speculative-draft-model-path jamesliu1/sglang-EAGLE3-Llama-3.1-"
"Instruct-8B --speculative-num-steps 5 \\\n"
"        --speculative-eagle-topk 8 --speculative-num-draft-tokens 32 --mem-"
"fraction 0.6 \\\n"
"        --cuda-graph-max-bs 2 --dtype float16 --log-level warning\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:-1
msgid ""
"client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", "
"api_key=\"None\")\n"
"\n"
"response = client.chat.completions.create(\n"
"    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n"
"    messages=[\n"
"        {\"role\": \"user\", \"content\": \"List 3 countries and their "
"capitals.\"},\n"
"    ],\n"
"    temperature=0,\n"
"    max_tokens=64,\n"
")\n"
"\n"
"print_highlight(f\"Response: {response}\")"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:320
msgid "Multi Token Prediction"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:322
msgid ""
"We support `MTP(Multi-Token Prediction) <https://arxiv.org/"
"pdf/2404.19737>`__ in SGLang by using speculative decoding. We use Xiaomi/"
"MiMo-7B-RL model as example here (deepseek mtp usage refer to `deepseek doc "
"<../basic_usage/deepseek.md#multi-token-prediction>`__)"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:-1
msgid ""
"server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"    python3 -m sglang.launch_server --model-path XiaomiMiMo/MiMo-7B-RL --"
"host 0.0.0.0 --trust-remote-code \\\n"
"    --speculative-algorithm EAGLE --speculative-num-steps 1 --speculative-"
"eagle-topk 1 --speculative-num-draft-tokens 2 \\\n"
"    --mem-fraction 0.5 --log-level warning\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:-1
msgid ""
"import requests\n"
"\n"
"url = f\"http://localhost:{port}/v1/chat/completions\"\n"
"\n"
"data = {\n"
"    \"model\": \"XiaomiMiMo/MiMo-7B-RL\",\n"
"    \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital "
"of France?\"}],\n"
"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"print_highlight(response.json())"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:379
msgid "References"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:381
msgid "EAGLE process is as follows:"
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:383
msgid ""
"Within EAGLE the draft model predicts the next feature vector, i.e. the last "
"hidden state of the original LLM, using the feature sequence :math:"
"`(f_1, ..., f_k)` and the token sequence :math:`(t_2, ..., t_{k+1})`."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:384
msgid ""
"The next token is then sampled from :math:`p_{k+2}=\\text{LMHead}(f_{k+1})`. "
"Afterwards, the two sequences are extended in a tree style—branching out "
"multiple potential continuations, with the branching factor per step "
"controlled by the ``speculative_eagle_topk`` parameter—to ensure a more "
"coherent connection of context, and are given as input again."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:385
msgid ""
"EAGLE-2 additionally uses the draft model to evaluate how probable certain "
"branches in the draft tree are, dynamically stopping the expansion of "
"unlikely branches. After the expansion phase, reranking is employed to "
"select only the top ``speculative_num_draft_tokens`` final nodes as draft "
"tokens."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:386
msgid ""
"EAGLE-3 removes the feature prediction objective, incorporates low and mid-"
"layer features, and is trained in an on-policy manner."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:388
msgid ""
"This enhances drafting accuracy by operating on the features instead of "
"tokens for more regular inputs and passing the tokens from the next timestep "
"additionally to minimize randomness effects from sampling. Furthermore the "
"dynamic adjustment of the draft tree and selection of reranked final nodes "
"increases acceptance rate of draft tokens further. For more details see "
"`EAGLE-2 <https://arxiv.org/abs/2406.16858>`__ and `EAGLE-3 <https://arxiv."
"org/abs/2503.01840>`__ paper."
msgstr ""

#: ../../../advanced_features/speculative_decoding.ipynb:390
msgid ""
"For guidance how to train your own EAGLE model please see the `EAGLE repo "
"<https://github.com/SafeAILab/EAGLE/tree/main?tab=readme-ov-file#train>`__."
msgstr ""
