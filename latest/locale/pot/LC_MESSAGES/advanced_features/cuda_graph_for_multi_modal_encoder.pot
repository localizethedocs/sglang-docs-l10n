# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2026, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-05 08:33+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:1
msgid "Cuda Graph for Multi-Modal Encoder in SGLang"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:3
msgid "Motivation"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:5
msgid ""
"In multimodal reasoning services, the visual encoder (ViT / Vision "
"Transformer) typically has a few characteristic traits:"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:7
msgid ""
"Many layers, fragmented operators: Each layer includes LN, QKV projections, "
"attention, MLP, residual connections, etc., resulting in extremely frequent "
"kernel launches."
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:9
msgid ""
"Server-side “small batch / low latency” is common: The batch size is very "
"small (sometimes it looks like 1 after “flattening” the batch), so kernel "
"launch overhead accounts for a large portion of end-to-end latency."
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:11
msgid ""
"Input token count (number of patches) varies frequently: Different image/"
"video resolutions and different batch composition lead to different sequence "
"lengths S — and this is precisely the biggest obstacle for CUDA Graph "
"(unstable shapes)."
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:14
msgid ""
"The value of CUDA Graph: It captures a long sequence of GPU kernels with "
"fixed shapes and fixed memory addresses into a graph; later, for the same "
"shapes, it can replay the graph directly, dramatically reducing launch "
"overhead and making GPU scheduling more compact."
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:16
msgid ""
"This led us to seek a CUDA Graph enabled feature for ViT in order to improve "
"ViT performance."
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:18
msgid "Design and Restrictions"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:20
msgid ""
"The new CUDA Graph enabled ViT logic is built on ViTCudaGraphRunner. This "
"runner captures the \"blocks + merger + deepstack merger (optional)\" part "
"of a vision transformer into a CUDA graph and replays it for identical "
"shapes. See the following design consideration and restrictions for more "
"details."
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:22
msgid "Dynamic inputs to fit static constraints of CUDA Graph"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:24
msgid ""
"Variable sequence length S is very common in ViT. While CUDA Graph requires "
"fixed shapes. The solution is to build a graph cache by S(e.g., graph_key = "
"S). The first time create a new S, and then capture a graph; afterwards, "
"replay it."
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:26
msgid ""
"If there are many distinct S values, we need to increase VRAM usage which is "
"graph-private memory pools for many graphs."
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:28
msgid "Stable addresses"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:30
msgid "Everything \"parameter-like\" becomes a static buffer:"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:32
msgid "block_input / block_ws / block_output"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:33
msgid "cu_full_len / cu_window_len and their kk variants"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:34
msgid "sin_cos_ws"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:36
msgid ""
"In this way to solve the underlying requirement: during replay, not allowed "
"to swap tensors, can only modify tensor contents."
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:38
msgid "Attention backend arguments"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:39
msgid "Attention backend arguments are fixed inside the graph:"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:41
msgid ""
"TritonAttn expects [cu_seqlens, cu_seqlens_kk, max_len] FA3 expects "
"[cu_seqlens, max_len]"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:44
msgid ""
"max_len is frozen as an int constant. cu_seqlens is cached into a dict "
"during create_graph(), and its contents are not updated during subsequent "
"replays."
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:47
msgid ""
"For the same graph_key = S, you not only require the input shape to match, "
"but also require the segmentation pattern in cu_seqlens (and window seqlens) "
"to be identical. Otherwise, attention will segment the sequence incorrectly."
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:49
msgid "Rotary buffer management"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:50
msgid ""
"The feature reallocates a larger sin_cos_ws when seq_len increases. The "
"max_content_len is used to make sure the maximum size of the allocated "
"rotary buffer."
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:54
msgid "Command Example"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:55
msgid ""
"You can enable CUDA Graph for ViT by setting env variable "
"`SGLANG_VIT_ENABLE_CUDA_GRAPH=1`, for example:"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:56
msgid ""
"SGLANG_VIT_ENABLE_CUDA_GRAPH=1 \\\n"
"python3 -m sglang.launch_server \\\n"
"  --model Qwen/Qwen3-VL-8B-Instruct\n"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:61
msgid ""
"Or you can run CUDA Graph for ViT together with Piecewise CUDA Graph feature "
"by both setting env variable `SGLANG_VIT_ENABLE_CUDA_GRAPH=1` and setting `--"
"enable-piecewise-cuda-graph`, for example:"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:62
msgid ""
"SGLANG_VIT_ENABLE_CUDA_GRAPH=1 \\\n"
"python3 -m sglang.launch_server \\\n"
"  --model Qwen/Qwen3-VL-8B-Instruct \\\n"
"  --piecewise-cuda-graph-max-tokens 4096 \\\n"
"  --enable-piecewise-cuda-graph \\\n"
"  --piecewise-cuda-graph-compiler eager\n"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:71
msgid "Known supported models"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:72
msgid "Qwen2.5-VL (https://github.com/sgl-project/sglang/pull/14422)"
msgstr ""

#: ../../../advanced_features/cuda_graph_for_multi_modal_encoder.md:73
msgid "Qwen3-VL (https://github.com/sgl-project/sglang/pull/15320)"
msgstr ""
