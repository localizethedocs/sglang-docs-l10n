# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:37+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../supported_models/modelscope.md:1
msgid "Use Models From ModelScope"
msgstr ""

#: ../../../supported_models/modelscope.md:3
msgid ""
"To use a model from [ModelScope](https://www.modelscope.cn), set the "
"environment variable `SGLANG_USE_MODELSCOPE`."
msgstr ""

#: ../../../supported_models/modelscope.md:5
msgid "export SGLANG_USE_MODELSCOPE=true\n"
msgstr ""

#: ../../../supported_models/modelscope.md:9
msgid ""
"We take [Qwen2-7B-Instruct](https://www.modelscope.cn/models/qwen/qwen2-7b-"
"instruct) as an example."
msgstr ""

#: ../../../supported_models/modelscope.md:11
msgid "Launch the Server:"
msgstr ""

#: ../../../supported_models/modelscope.md:12
msgid ""
"python -m sglang.launch_server --model-path qwen/Qwen2-7B-Instruct --port "
"30000\n"
msgstr ""

#: ../../../supported_models/modelscope.md:16
msgid "Or start it by docker:"
msgstr ""

#: ../../../supported_models/modelscope.md:18
msgid ""
"docker run --gpus all \\\n"
"    -p 30000:30000 \\\n"
"    -v ~/.cache/modelscope:/root/.cache/modelscope \\\n"
"    --env \"SGLANG_USE_MODELSCOPE=true\" \\\n"
"    --ipc=host \\\n"
"    lmsysorg/sglang:latest \\\n"
"    python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-7B-Instruct --"
"host 0.0.0.0 --port 30000\n"
msgstr ""

#: ../../../supported_models/modelscope.md:28
msgid ""
"Note that modelscope uses a different cache directory than huggingface. You "
"may need to set it manually to avoid running out of disk space."
msgstr ""
