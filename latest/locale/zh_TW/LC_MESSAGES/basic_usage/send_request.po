# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-02-16 08:46+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../basic_usage/send_request.ipynb:2
msgid ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"
msgstr ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"

#: ../../../basic_usage/send_request.ipynb:9
msgid "Sending Requests"
msgstr "發送請求"

#: ../../../basic_usage/send_request.ipynb:11
msgid ""
"This notebook provides a quick-start guide to use SGLang in chat completions "
"after installation. Once your server is running, API documentation is "
"available at ``http://localhost:30000/docs`` (Swagger UI), ``http://"
"localhost:30000/redoc`` (ReDoc), or ``http://localhost:30000/openapi.json`` "
"(OpenAPI spec, useful for AI agents). Replace ``30000`` with your port if "
"using a different one."
msgstr ""

#: ../../../basic_usage/send_request.ipynb:13
msgid ""
"For Vision Language Models, see `OpenAI APIs - Vision <openai_api_vision."
"ipynb>`__."
msgstr ""

#: ../../../basic_usage/send_request.ipynb:14
msgid ""
"For Embedding Models, see `OpenAI APIs - Embedding <openai_api_embeddings."
"ipynb>`__ and `Encode (embedding model) <native_api.html#Encode-(embedding-"
"model)>`__."
msgstr ""

#: ../../../basic_usage/send_request.ipynb:15
msgid ""
"For Reward Models, see `Classify (reward model) <native_api.html#Classify-"
"(reward-model)>`__."
msgstr ""

#: ../../../basic_usage/send_request.ipynb:27
msgid "Launch A Server"
msgstr ""

#: ../../../basic_usage/send_request.ipynb:-1
msgid "[ ]:"
msgstr ""

#: ../../../basic_usage/send_request.ipynb:-1
msgid ""
"from sglang.test.doc_patch import launch_server_cmd\n"
"from sglang.utils import wait_for_server, print_highlight, "
"terminate_process\n"
"\n"
"# This is equivalent to running the following command in your terminal\n"
"# python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct --"
"host 0.0.0.0\n"
"\n"
"server_process, port = launch_server_cmd(\"\"\"\n"
"python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct \\\n"
" --host 0.0.0.0 --log-level warning\n"
"\"\"\")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\", process=server_process)"
msgstr ""

#: ../../../basic_usage/send_request.ipynb:59
msgid "Using cURL"
msgstr "使用 cURL"

#: ../../../basic_usage/send_request.ipynb:-1
msgid ""
"import subprocess, json\n"
"\n"
"curl_command = f\"\"\"\n"
"curl -s http://localhost:{port}/v1/chat/completions \\\n"
"  -H \"Content-Type: application/json\" \\\n"
"  -d '{{\"model\": \"qwen/qwen2.5-0.5b-instruct\", \"messages\": "
"[{{\"role\": \"user\", \"content\": \"What is the capital of France?"
"\"}}]}}'\n"
"\"\"\"\n"
"\n"
"response = json.loads(subprocess.check_output(curl_command, shell=True))\n"
"print_highlight(response)"
msgstr ""
"import subprocess, json\n"
"\n"
"curl_command = f\"\"\"\n"
"curl -s http://localhost:{port}/v1/chat/completions \\\n"
"  -H \"Content-Type: application/json\" \\\n"
"  -d '{{\"model\": \"qwen/qwen2.5-0.5b-instruct\", \"messages\": "
"[{{\"role\": \"user\", \"content\": \"What is the capital of France?"
"\"}}]}}'\n"
"\"\"\"\n"
"\n"
"response = json.loads(subprocess.check_output(curl_command, shell=True))\n"
"print_highlight(response)"

#: ../../../basic_usage/send_request.ipynb:89
msgid "Using Python Requests"
msgstr "使用 Python Requests"

#: ../../../basic_usage/send_request.ipynb:-1
msgid ""
"import requests\n"
"\n"
"url = f\"http://localhost:{port}/v1/chat/completions\"\n"
"\n"
"data = {\n"
"    \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n"
"    \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital "
"of France?\"}],\n"
"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"print_highlight(response.json())"
msgstr ""
"import requests\n"
"\n"
"url = f\"http://localhost:{port}/v1/chat/completions\"\n"
"\n"
"data = {\n"
"    \"model\": \"qwen/qwen2.5-0.5b-instruct\",\n"
"    \"messages\": [{\"role\": \"user\", \"content\": \"What is the capital "
"of France?\"}],\n"
"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"print_highlight(response.json())"

#: ../../../basic_usage/send_request.ipynb:120
msgid "Using OpenAI Python Client"
msgstr "使用 OpenAI Python 客戶端"

#: ../../../basic_usage/send_request.ipynb:-1
msgid ""
"import openai\n"
"\n"
"client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", "
"api_key=\"None\")\n"
"\n"
"response = client.chat.completions.create(\n"
"    model=\"qwen/qwen2.5-0.5b-instruct\",\n"
"    messages=[\n"
"        {\"role\": \"user\", \"content\": \"List 3 countries and their "
"capitals.\"},\n"
"    ],\n"
"    temperature=0,\n"
"    max_tokens=64,\n"
")\n"
"print_highlight(response)"
msgstr ""

#: ../../../basic_usage/send_request.ipynb:153
#: ../../../basic_usage/send_request.ipynb:228
msgid "Streaming"
msgstr ""

#: ../../../basic_usage/send_request.ipynb:-1
msgid ""
"import openai\n"
"\n"
"client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", "
"api_key=\"None\")\n"
"\n"
"# Use stream=True for streaming responses\n"
"response = client.chat.completions.create(\n"
"    model=\"qwen/qwen2.5-0.5b-instruct\",\n"
"    messages=[\n"
"        {\"role\": \"user\", \"content\": \"List 3 countries and their "
"capitals.\"},\n"
"    ],\n"
"    temperature=0,\n"
"    max_tokens=64,\n"
"    stream=True,\n"
")\n"
"\n"
"# Handle the streaming output\n"
"for chunk in response:\n"
"    if chunk.choices[0].delta.content:\n"
"        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
msgstr ""

#: ../../../basic_usage/send_request.ipynb:192
msgid "Using Native Generation APIs"
msgstr ""

#: ../../../basic_usage/send_request.ipynb:194
msgid ""
"You can also use the native ``/generate`` endpoint with requests, which "
"provides more flexibility. An API reference is available at `Sampling "
"Parameters <sampling_params.md>`__."
msgstr ""

#: ../../../basic_usage/send_request.ipynb:-1
msgid ""
"import requests\n"
"\n"
"response = requests.post(\n"
"    f\"http://localhost:{port}/generate\",\n"
"    json={\n"
"        \"text\": \"The capital of France is\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 32,\n"
"        },\n"
"    },\n"
")\n"
"\n"
"print_highlight(response.json())"
msgstr ""

#: ../../../basic_usage/send_request.ipynb:-1
msgid ""
"import requests, json\n"
"\n"
"response = requests.post(\n"
"    f\"http://localhost:{port}/generate\",\n"
"    json={\n"
"        \"text\": \"The capital of France is\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 32,\n"
"        },\n"
"        \"stream\": True,\n"
"    },\n"
"    stream=True,\n"
")\n"
"\n"
"prev = 0\n"
"for chunk in response.iter_lines(decode_unicode=False):\n"
"    chunk = chunk.decode(\"utf-8\")\n"
"    if chunk and chunk.startswith(\"data:\"):\n"
"        if chunk == \"data: [DONE]\":\n"
"            break\n"
"        data = json.loads(chunk[5:].strip(\"\\n\"))\n"
"        output = data[\"text\"]\n"
"        print(output[prev:], end=\"\", flush=True)\n"
"        prev = len(output)"
msgstr ""

#: ../../../basic_usage/send_request.ipynb:-1
msgid "terminate_process(server_process)"
msgstr ""
