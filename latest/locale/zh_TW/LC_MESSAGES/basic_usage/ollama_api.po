# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-05 08:33+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../basic_usage/ollama_api.md:1
msgid "Ollama-Compatible API"
msgstr ""

#: ../../../basic_usage/ollama_api.md:3
msgid ""
"SGLang provides Ollama API compatibility, allowing you to use the Ollama CLI "
"and Python library with SGLang as the inference backend."
msgstr ""

#: ../../../basic_usage/ollama_api.md:5
msgid "Prerequisites"
msgstr ""

#: ../../../basic_usage/ollama_api.md:7
msgid ""
"# Install the Ollama Python library (for Python client usage)\n"
"pip install ollama\n"
msgstr ""

#: ../../../basic_usage/ollama_api.md:12
msgid ""
"**Note**: You don't need the Ollama server installed - SGLang acts as the "
"backend. You only need the `ollama` CLI or Python library as the client."
msgstr ""

#: ../../../basic_usage/ollama_api.md:14
msgid "Endpoints"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "Endpoint"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "Method"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "Description"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "`/`"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "GET, HEAD"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "Health check for Ollama CLI"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "`/api/tags`"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "GET"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "List available models"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "`/api/chat`"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "POST"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "Chat completions (streaming & non-streaming)"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "`/api/generate`"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "Text generation (streaming & non-streaming)"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "`/api/show`"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "Model information"
msgstr ""

#: ../../../basic_usage/ollama_api.md:24
msgid "Quick Start"
msgstr ""

#: ../../../basic_usage/ollama_api.md:26
msgid "1. Launch SGLang Server"
msgstr ""

#: ../../../basic_usage/ollama_api.md:28
msgid ""
"python -m sglang.launch_server \\\n"
"    --model Qwen/Qwen2.5-1.5B-Instruct \\\n"
"    --port 30001 \\\n"
"    --host 0.0.0.0\n"
msgstr ""

#: ../../../basic_usage/ollama_api.md:35
msgid ""
"**Note**: The model name used with `ollama run` must match exactly what you "
"passed to `--model`."
msgstr ""

#: ../../../basic_usage/ollama_api.md:37
msgid "2. Use Ollama CLI"
msgstr ""

#: ../../../basic_usage/ollama_api.md:39
msgid ""
"# List available models\n"
"OLLAMA_HOST=http://localhost:30001 ollama list\n"
"\n"
"# Interactive chat\n"
"OLLAMA_HOST=http://localhost:30001 ollama run \"Qwen/Qwen2.5-1.5B-"
"Instruct\"\n"
msgstr ""

#: ../../../basic_usage/ollama_api.md:47
msgid "If connecting to a remote server behind a firewall:"
msgstr ""

#: ../../../basic_usage/ollama_api.md:49
msgid ""
"# SSH tunnel\n"
"ssh -L 30001:localhost:30001 user@gpu-server -N &\n"
"\n"
"# Then use Ollama CLI as above\n"
"OLLAMA_HOST=http://localhost:30001 ollama list\n"
msgstr ""

#: ../../../basic_usage/ollama_api.md:57
msgid "3. Use Ollama Python Library"
msgstr ""

#: ../../../basic_usage/ollama_api.md:59
msgid ""
"import ollama\n"
"\n"
"client = ollama.Client(host='http://localhost:30001')\n"
"\n"
"# Non-streaming\n"
"response = client.chat(\n"
"    model='Qwen/Qwen2.5-1.5B-Instruct',\n"
"    messages=[{'role': 'user', 'content': 'Hello!'}]\n"
")\n"
"print(response['message']['content'])\n"
"\n"
"# Streaming\n"
"stream = client.chat(\n"
"    model='Qwen/Qwen2.5-1.5B-Instruct',\n"
"    messages=[{'role': 'user', 'content': 'Tell me a story'}],\n"
"    stream=True\n"
")\n"
"for chunk in stream:\n"
"    print(chunk['message']['content'], end='', flush=True)\n"
msgstr ""

#: ../../../basic_usage/ollama_api.md:81
msgid "Smart Router"
msgstr ""

#: ../../../basic_usage/ollama_api.md:83
msgid ""
"For intelligent routing between local Ollama (fast) and remote SGLang "
"(powerful) using an LLM judge, see the [Smart Router documentation](https://"
"github.com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/ollama/"
"README.md)."
msgstr ""

#: ../../../basic_usage/ollama_api.md:85
msgid "Summary"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "Component"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "Purpose"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "**Ollama API**"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "Familiar CLI/API that developers already know"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "**SGLang Backend**"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "High-performance inference engine"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid "**Smart Router**"
msgstr ""

#: ../../../basic_usage/ollama_api.md:0
msgid ""
"Intelligent routing - fast local for simple tasks, powerful remote for "
"complex tasks"
msgstr ""
