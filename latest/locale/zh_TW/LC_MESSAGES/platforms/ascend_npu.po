# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-05 08:33+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../platforms/ascend_npu.md:2
msgid "SGLang installation with NPUs support"
msgstr ""

#: ../../../platforms/ascend_npu.md:4
msgid ""
"You can install SGLang using any of the methods below. Please go through "
"`System Settings` section to ensure the clusters are roaring at max "
"performance. Feel free to leave an issue [here at sglang](https://github.com/"
"sgl-project/sglang/issues) if you encounter any issues or have any problems."
msgstr ""

#: ../../../platforms/ascend_npu.md:6
msgid "Component Version Mapping For SGLang"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "Component"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "Version"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "Obtain Way"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "HDK"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "25.2.1"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid ""
"[link](https://support.huawei.com/carrier/productNewOffering?"
"col=product&path=PBI1-262732867/PBI1-262735886/PBI1-262735910/PBI1-261410188/"
"PBI1-252764743&pVR=PBI1-263550357&pC=PBI1-264360782&pSPC=PBI1-266220744&resTab=SW)"
msgstr ""

#: ../../../platforms/ascend_npu.md:0 ../../../platforms/ascend_npu.md:40
msgid "CANN"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "8.3.rc2"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "[Obtain Images](#obtain-cann-image)"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "Pytorch Adapter"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "7.3.0"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "[link](https://gitcode.com/Ascend/pytorch/releases)"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "MemFabric"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "0.1.0"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "[link](https://gitcode.com/Ascend/memfabric_hybrid/releases)"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "Triton"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "3.2.0.dev2025112116"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid ""
"[link](https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/sglang/"
"triton_ascend/triton_ascend-3.2.0.dev2025112116-cp311-cp311-"
"manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl)"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "Bisheng"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "20251121"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid ""
"[link](https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/sglang/"
"triton_ascend/Ascend-BiSheng-toolkit_aarch64_20251121.run)"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "SGLang NPU Kernel"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "NA"
msgstr ""

#: ../../../platforms/ascend_npu.md:0
msgid "[link](https://github.com/sgl-project/sgl-kernel-npu/releases)"
msgstr ""

#: ../../../platforms/ascend_npu.md:17
msgid "<a id=\"obtain-cann-image\"></a>"
msgstr ""

#: ../../../platforms/ascend_npu.md:17
msgid "<a id=\"obtain-cann-image\">"
msgstr ""

#: ../../../platforms/ascend_npu.md:17
msgid "</a>"
msgstr ""

#: ../../../platforms/ascend_npu.md:18
msgid "Obtain CANN Image"
msgstr ""

#: ../../../platforms/ascend_npu.md:19
msgid ""
"You can obtain the dependency of a specified version of CANN through an "
"image."
msgstr ""

#: ../../../platforms/ascend_npu.md:20
msgid ""
"# for Atlas 800I A3 and Ubuntu OS\n"
"docker pull quay.io/ascend/cann:8.3.rc2-a3-ubuntu22.04-py3.11\n"
"# for Atlas 800I A2 and Ubuntu OS\n"
"docker pull quay.io/ascend/cann:8.3.rc2-910b-ubuntu22.04-py3.11\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:27
msgid "Preparing the Running Environment"
msgstr ""

#: ../../../platforms/ascend_npu.md:29
msgid "Method 1: Installing from source with prerequisites"
msgstr ""

#: ../../../platforms/ascend_npu.md:31
msgid "Python Version"
msgstr ""

#: ../../../platforms/ascend_npu.md:33
msgid ""
"Only `python==3.11` is supported currently. If you don't want to break "
"system pre-installed python, try installing with [conda](https://github.com/"
"conda/conda)."
msgstr ""

#: ../../../platforms/ascend_npu.md:35
msgid ""
"conda create --name sglang_npu python=3.11\n"
"conda activate sglang_npu\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:42
msgid ""
"Prior to start work with SGLang on Ascend you need to install CANN Toolkit, "
"Kernels operator package and NNAL version 8.3.RC2 or higher, check the "
"[installation guide](https://www.hiascend.com/document/detail/zh/"
"CANNCommunityEdition/83RC1/softwareinst/instg/instg_0008.html?"
"Mode=PmIns&InstallType=local&OS=openEuler&Software=cannToolKit)"
msgstr ""

#: ../../../platforms/ascend_npu.md:44
msgid "MemFabric Adaptor"
msgstr ""

#: ../../../platforms/ascend_npu.md:46
msgid ""
"If you want to use PD disaggregation mode, you need to install MemFabric "
"Adaptor. MemFabric Adaptor is a drop-in replacement of Mooncake Transfer "
"Engine that enables KV cache transfer on Ascend NPU clusters."
msgstr ""

#: ../../../platforms/ascend_npu.md:48
msgid "pip install mf-adapter==1.0.0\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:52
msgid "Pytorch and Pytorch Framework Adaptor on Ascend"
msgstr ""

#: ../../../platforms/ascend_npu.md:54
msgid ""
"At the moment NPUGraph optimizations are supported only in `torch_npu==2.6.0."
"post3` that requires 'torch==2.6.0'."
msgstr ""

#: ../../../platforms/ascend_npu.md:56
msgid ""
"_TODO: NPUGraph optimizations will be supported in future releases of "
"'torch_npu' 2.7.1, 2.8.0 and 2.9.0_"
msgstr ""

#: ../../../platforms/ascend_npu.md:58
msgid ""
"PYTORCH_VERSION=2.6.0\n"
"TORCHVISION_VERSION=0.21.0\n"
"TORCH_NPU_VERSION=2.6.0.post3\n"
"pip install torch==$PYTORCH_VERSION torchvision==$TORCHVISION_VERSION --"
"index-url https://download.pytorch.org/whl/cpu\n"
"pip install torch_npu==$TORCH_NPU_VERSION\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:66
msgid ""
"While there is no released versions of 'torch_npu' for 'torch==2.7.1' and "
"'torch==2.8.0' we provide custom builds of 'torch_npu'. PLATFORM can be "
"'aarch64' or 'x86_64'"
msgstr ""

#: ../../../platforms/ascend_npu.md:68
msgid ""
"PLATFORM=\"aarch64\"\n"
"PYTORCH_VERSION=2.8.0\n"
"TORCHVISION_VERSION=0.23.0\n"
"pip install torch==$PYTORCH_VERSION torchvision==$TORCHVISION_VERSION --"
"index-url https://download.pytorch.org/whl/cpu\n"
"wget https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/sglang/torch_npu/"
"torch_npu-${PYTORCH_VERSION}.post2.dev20251120-cp311-cp311-"
"manylinux_2_28_${PLATFORM}.whl\n"
"pip install torch_npu-${PYTORCH_VERSION}.post2.dev20251120-cp311-cp311-"
"manylinux_2_28_${PLATFORM}.whl\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:77
msgid ""
"If you are using other versions of `torch` and install `torch_npu`, check "
"[installation guide](https://github.com/Ascend/pytorch/blob/master/README.md)"
msgstr ""

#: ../../../platforms/ascend_npu.md:79
msgid "Triton on Ascend"
msgstr ""

#: ../../../platforms/ascend_npu.md:81
msgid "We provide our own implementation of Triton for Ascend."
msgstr ""

#: ../../../platforms/ascend_npu.md:83
msgid ""
"BISHENG_NAME=\"Ascend-BiSheng-toolkit_aarch64_20251121.run\"\n"
"BISHENG_URL=\"https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/sglang/"
"triton_ascend/${BISHENG_NAME}\"\n"
"wget -O \"${BISHENG_NAME}\" \"${BISHENG_URL}\" && chmod a+x "
"\"${BISHENG_NAME}\" && \"./${BISHENG_NAME}\" --install && rm "
"\"${BISHENG_NAME}\"\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:88
msgid "pip install triton-ascend==3.2.0rc4\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:91
msgid ""
"For installation of Triton on Ascend nightly builds or from sources, follow "
"[installation guide](https://gitcode.com/Ascend/triton-ascend/blob/master/"
"docs/sources/getting-started/installation.md)"
msgstr ""

#: ../../../platforms/ascend_npu.md:93
msgid "SGLang Kernels NPU"
msgstr ""

#: ../../../platforms/ascend_npu.md:94
msgid ""
"We provide SGL kernels for Ascend NPU, check [installation guide](https://"
"github.com/sgl-project/sgl-kernel-npu/blob/main/python/sgl_kernel_npu/README."
"md)."
msgstr ""

#: ../../../platforms/ascend_npu.md:96
msgid "DeepEP-compatible Library"
msgstr ""

#: ../../../platforms/ascend_npu.md:97
msgid ""
"We provide a DeepEP-compatible Library as a drop-in replacement of deepseek-"
"ai's DeepEP library, check the [installation guide](https://github.com/sgl-"
"project/sgl-kernel-npu/blob/main/python/deep_ep/README.md)."
msgstr ""

#: ../../../platforms/ascend_npu.md:99
msgid "CustomOps"
msgstr ""

#: ../../../platforms/ascend_npu.md:100
msgid ""
"_TODO: to be removed once merged into sgl-kernel-npu._ Additional package "
"with custom operations. DEVICE_TYPE can be \"a3\" for Atlas A3 server or "
"\"910b\" for Atlas A2 server."
msgstr ""

#: ../../../platforms/ascend_npu.md:103
msgid ""
"DEVICE_TYPE=\"a3\"\n"
"wget https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/ops/CANN-"
"custom_ops-8.2.0.0-$DEVICE_TYPE-linux.aarch64.run\n"
"chmod a+x ./CANN-custom_ops-8.2.0.0-$DEVICE_TYPE-linux.aarch64.run\n"
"./CANN-custom_ops-8.2.0.0-$DEVICE_TYPE-linux.aarch64.run --quiet --install-"
"path=/usr/local/Ascend/ascend-toolkit/latest/opp\n"
"wget https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/ops/"
"custom_ops-1.0.$DEVICE_TYPE-cp311-cp311-linux_aarch64.whl\n"
"pip install ./custom_ops-1.0.$DEVICE_TYPE-cp311-cp311-linux_aarch64.whl\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:112
msgid "Installing SGLang from source"
msgstr ""

#: ../../../platforms/ascend_npu.md:114
msgid ""
"# Use the last release branch\n"
"git clone -b v0.5.6.post2 https://github.com/sgl-project/sglang.git\n"
"cd sglang\n"
"mv python/pyproject_other.toml python/pyproject.toml\n"
"pip install -e python[srt_npu]\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:122
msgid "Method 2: Using Docker Image"
msgstr ""

#: ../../../platforms/ascend_npu.md:123
msgid "Obtain Image"
msgstr ""

#: ../../../platforms/ascend_npu.md:124
msgid ""
"You can download the SGLang image or build an image based on Dockerfile to "
"obtain the Ascend NPU image."
msgstr ""

#: ../../../platforms/ascend_npu.md:125
msgid "Download SGLang image"
msgstr ""

#: ../../../platforms/ascend_npu.md:126
msgid ""
"dockerhub: docker.io/lmsysorg/sglang:$tag\n"
"# Main-based tag, change main to specific version like v0.5.6,\n"
"# you can get image for specific version\n"
"Atlas 800I A3 : {main}-cann8.3.rc2-a3\n"
"Atlas 800I A2: {main}-cann8.3.rc2-910b\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:133
msgid "Build an image based on Dockerfile"
msgstr ""

#: ../../../platforms/ascend_npu.md:134
msgid ""
"# Clone the SGLang repository\n"
"git clone https://github.com/sgl-project/sglang.git\n"
"cd sglang/docker\n"
"\n"
"# Build the docker image\n"
"docker build -t <image_name> -f npu.Dockerfile .\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:143
msgid "Create Docker"
msgstr ""

#: ../../../platforms/ascend_npu.md:144
msgid ""
"__Notice:__ `--privileged` and `--network=host` are required by RDMA, which "
"is typically needed by Ascend NPU clusters."
msgstr ""

#: ../../../platforms/ascend_npu.md:146
msgid ""
"__Notice:__ The following docker command is based on Atlas 800I A3 machines. "
"If you are using Atlas 800I A2, make sure only `davinci[0-7]` are mapped "
"into container."
msgstr ""

#: ../../../platforms/ascend_npu.md:148
msgid ""
"\n"
"alias drun='docker run -it --rm --privileged --network=host --ipc=host --shm-"
"size=16g \\\n"
"    --device=/dev/davinci0 --device=/dev/davinci1 --device=/dev/davinci2 --"
"device=/dev/davinci3 \\\n"
"    --device=/dev/davinci4 --device=/dev/davinci5 --device=/dev/davinci6 --"
"device=/dev/davinci7 \\\n"
"    --device=/dev/davinci8 --device=/dev/davinci9 --device=/dev/davinci10 --"
"device=/dev/davinci11 \\\n"
"    --device=/dev/davinci12 --device=/dev/davinci13 --device=/dev/davinci14 "
"--device=/dev/davinci15 \\\n"
"    --device=/dev/davinci_manager --device=/dev/hisi_hdc \\\n"
"    --volume /usr/local/sbin:/usr/local/sbin --volume /usr/local/Ascend/"
"driver:/usr/local/Ascend/driver \\\n"
"    --volume /usr/local/Ascend/firmware:/usr/local/Ascend/firmware \\\n"
"    --volume /etc/ascend_install.info:/etc/ascend_install.info \\\n"
"    --volume /var/queue_schedule:/var/queue_schedule --volume ~/.cache/:/"
"root/.cache/'\n"
"\n"
"# Add HF_TOKEN env for download model by SGLang.\n"
"drun --env \"HF_TOKEN=<secret>\" \\\n"
"    <image_name> \\\n"
"    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-"
"Instruct --attention-backend ascend\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:167
msgid "System Settings"
msgstr ""

#: ../../../platforms/ascend_npu.md:169
msgid "CPU performance power scheme"
msgstr ""

#: ../../../platforms/ascend_npu.md:171
msgid ""
"The default power scheme on Ascend hardware is `ondemand` which could affect "
"performance, changing it to `performance` is recommended."
msgstr ""

#: ../../../platforms/ascend_npu.md:173
msgid ""
"echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/"
"scaling_governor\n"
"\n"
"# Make sure changes are applied successfully\n"
"cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor # shows "
"performance\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:180
msgid "Disable NUMA balancing"
msgstr ""

#: ../../../platforms/ascend_npu.md:182
msgid ""
"sudo sysctl -w kernel.numa_balancing=0\n"
"# Check\n"
"cat /proc/sys/kernel/numa_balancing # shows 0\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:188
msgid "Prevent swapping out system memory"
msgstr ""

#: ../../../platforms/ascend_npu.md:190
msgid ""
"sudo sysctl -w vm.swappiness=10\n"
"\n"
"# Check\n"
"cat /proc/sys/vm/swappiness # shows 10\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:197
msgid "Running SGLang Service"
msgstr ""

#: ../../../platforms/ascend_npu.md:198
msgid "Running Service For Large Language Models"
msgstr ""

#: ../../../platforms/ascend_npu.md:199 ../../../platforms/ascend_npu.md:261
msgid "PD Mixed Scene"
msgstr ""

#: ../../../platforms/ascend_npu.md:200
msgid ""
"# Enabling CPU Affinity\n"
"export SGLANG_SET_CPU_AFFINITY=1\n"
"python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-"
"Instruct --attention-backend ascend\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:206
msgid "PD Separation Scene"
msgstr ""

#: ../../../platforms/ascend_npu.md:207
msgid "Launch Prefill Server"
msgstr ""

#: ../../../platforms/ascend_npu.md:208
msgid ""
"# Enabling CPU Affinity\n"
"export SGLANG_SET_CPU_AFFINITY=1\n"
"\n"
"# PIP: recommended to config first Prefill Server IP\n"
"# PORT: one free port\n"
"# all sglang servers need to be config the same PIP and PORT,\n"
"export ASCEND_MF_STORE_URL=\"tcp://PIP:PORT\"\n"
"# if you are Atlas 800I A2 hardware and use rdma for kv cache transfer, add "
"this parameter\n"
"export ASCEND_MF_TRANSFER_PROTOCOL=\"device_rdma\"\n"
"python3 -m sglang.launch_server \\\n"
"    --model-path meta-llama/Llama-3.1-8B-Instruct \\\n"
"    --disaggregation-mode prefill \\\n"
"    --disaggregation-transfer-backend ascend \\\n"
"    --disaggregation-bootstrap-port 8995 \\\n"
"    --attention-backend ascend \\\n"
"    --device npu \\\n"
"    --base-gpu-id 0 \\\n"
"    --tp-size 1 \\\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:229
msgid "Launch Decode Server"
msgstr ""

#: ../../../platforms/ascend_npu.md:230
msgid ""
"# PIP: recommended to config first Prefill Server IP\n"
"# PORT: one free port\n"
"# all sglang servers need to be config the same PIP and PORT,\n"
"export ASCEND_MF_STORE_URL=\"tcp://PIP:PORT\"\n"
"# if you are Atlas 800I A2 hardware and use rdma for kv cache transfer, add "
"this parameter\n"
"export ASCEND_MF_TRANSFER_PROTOCOL=\"device_rdma\"\n"
"python3 -m sglang.launch_server \\\n"
"    --model-path meta-llama/Llama-3.1-8B-Instruct \\\n"
"    --disaggregation-mode decode \\\n"
"    --disaggregation-transfer-backend ascend \\\n"
"    --attention-backend ascend \\\n"
"    --device npu \\\n"
"    --base-gpu-id 1 \\\n"
"    --tp-size 1 \\\n"
"    --host 127.0.0.1 \\\n"
"    --port 8001\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:249
msgid "Launch Router"
msgstr ""

#: ../../../platforms/ascend_npu.md:250
msgid ""
"python3 -m sglang_router.launch_router \\\n"
"    --pd-disaggregation \\\n"
"    --policy cache_aware \\\n"
"    --prefill http://127.0.0.1:8000 8995 \\\n"
"    --decode http://127.0.0.1:8001 \\\n"
"    --host 127.0.0.1 \\\n"
"    --port 6688\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:260
msgid "Running Service For Multimodal Language Models"
msgstr ""

#: ../../../platforms/ascend_npu.md:262
msgid ""
"python3 -m sglang.launch_server \\\n"
"    --model-path Qwen3-VL-30B-A3B-Instruct \\\n"
"    --host 127.0.0.1 \\\n"
"    --port 8000 \\\n"
"    --tp 4 \\\n"
"    --device npu \\\n"
"    --attention-backend ascend \\\n"
"    --mm-attention-backend ascend_attn \\\n"
"    --disable-radix-cache \\\n"
"    --trust-remote-code \\\n"
"    --enable-multimodal \\\n"
"    --sampling-backend ascend\n"
msgstr ""
