# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-22 08:33+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../platforms/ascend_npu.md:2
msgid "SGLang installation with NPUs support"
msgstr ""

#: ../../../platforms/ascend_npu.md:4
msgid ""
"You can install SGLang using any of the methods below. Please go through "
"`System Settings` section to ensure the clusters are roaring at max "
"performance. Feel free to leave an issue [here at sglang](https://github.com/"
"sgl-project/sglang/issues) if you encounter any issues or have any problems."
msgstr ""

#: ../../../platforms/ascend_npu.md:6
msgid "Installing SGLang"
msgstr ""

#: ../../../platforms/ascend_npu.md:8
msgid "Method 1: Installing from source with prerequisites"
msgstr ""

#: ../../../platforms/ascend_npu.md:10
msgid "Python Version"
msgstr ""

#: ../../../platforms/ascend_npu.md:12
msgid ""
"Only `python==3.11` is supported currently. If you don't want to break "
"system pre-installed python, try installing with [conda](https://github.com/"
"conda/conda)."
msgstr ""

#: ../../../platforms/ascend_npu.md:14
msgid ""
"conda create --name sglang_npu python=3.11\n"
"conda activate sglang_npu\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:19
msgid "CANN"
msgstr ""

#: ../../../platforms/ascend_npu.md:21
msgid ""
"Prior to start work with SGLang on Ascend you need to install CANN Toolkit, "
"Kernels operator package and NNAL version 8.3.RC2 or higher, check the "
"[installation guide](https://www.hiascend.com/document/detail/zh/"
"CANNCommunityEdition/83RC1/softwareinst/instg/instg_0008.html?"
"Mode=PmIns&InstallType=local&OS=openEuler&Software=cannToolKit)"
msgstr ""

#: ../../../platforms/ascend_npu.md:23
msgid "MemFabric Adaptor"
msgstr ""

#: ../../../platforms/ascend_npu.md:25
msgid ""
"If you want to use PD disaggregation mode, you need to install MemFabric "
"Adaptor. MemFabric Adaptor is a drop-in replacement of Mooncake Transfer "
"Engine that enables KV cache transfer on Ascend NPU clusters."
msgstr ""

#: ../../../platforms/ascend_npu.md:27
msgid "pip install mf-adapter==1.0.0\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:31
msgid "Pytorch and Pytorch Framework Adaptor on Ascend"
msgstr ""

#: ../../../platforms/ascend_npu.md:33
msgid ""
"At the moment NPUGraph optimizations are supported only in `torch_npu==2.6.0."
"post3` that requires 'torch==2.6.0'. _TODO: NPUGraph optimizations will be "
"supported in future releases of 'torch_npu' 2.7.1, 2.8.0 and 2.9.0_"
msgstr ""

#: ../../../platforms/ascend_npu.md:36
msgid ""
"PYTORCH_VERSION=2.6.0\n"
"TORCHVISION_VERSION=0.21.0\n"
"TORCH_NPU_VERSION=2.6.0.post3\n"
"pip install torch==$PYTORCH_VERSION torchvision==$TORCHVISION_VERSION --"
"index-url https://download.pytorch.org/whl/cpu\n"
"pip install torch_npu==$TORCH_NPU_VERSION\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:44
msgid ""
"While there is no resleased versions of 'torch_npu' for 'torch==2.7.1' and "
"'torch==2.8.0' we provide custom builds of 'torch_npu'. PLATFORM can be "
"'aarch64' or 'x86_64'"
msgstr ""

#: ../../../platforms/ascend_npu.md:46
msgid ""
"PLATFORM=\"aarch64\"\n"
"PYTORCH_VERSION=2.8.0\n"
"TORCHVISION_VERSION=0.23.0\n"
"pip install torch==$PYTORCH_VERSION torchvision==$TORCHVISION_VERSION --"
"index-url https://download.pytorch.org/whl/cpu\n"
"wget https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/sglang/torch_npu/"
"torch_npu-${PYTORCH_VERSION}.post2.dev20251120-cp311-cp311-"
"manylinux_2_28_${PLATFORM}.whl\n"
"pip install torch_npu-${PYTORCH_VERSION}.post2.dev20251120-cp311-cp311-"
"manylinux_2_28_${PLATFORM}.whl\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:55
msgid ""
"If you are using other versions of 'torch' install 'torch_npu' from sources, "
"check [installation guide](https://github.com/Ascend/pytorch/blob/master/"
"README.md)"
msgstr ""

#: ../../../platforms/ascend_npu.md:57
msgid "Triton on Ascend"
msgstr ""

#: ../../../platforms/ascend_npu.md:59
msgid "We provide our own implementation of Triton for Ascend."
msgstr ""

#: ../../../platforms/ascend_npu.md:61
msgid ""
"BISHENG_NAME=\"Ascend-BiSheng-toolkit_aarch64_20251121.run\"\n"
"BISHENG_URL=\"https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/sglang/"
"triton_ascend/${BISHENG_NAME}\"\n"
"wget -O \"${BISHENG_NAME}\" \"${BISHENG_URL}\" && chmod a+x "
"\"${BISHENG_NAME}\" && \"./${BISHENG_NAME}\" --install && rm "
"\"${BISHENG_NAME}\"\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:66
msgid "pip install triton-ascend==3.2.0rc4\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:69
msgid ""
"For installation of Triton on Ascend nightly builds or from sources, follow "
"[installation guide](https://gitcode.com/Ascend/triton-ascend/blob/master/"
"docs/sources/getting-started/installation.md)"
msgstr ""

#: ../../../platforms/ascend_npu.md:71
msgid "SGLang Kernels NPU"
msgstr ""

#: ../../../platforms/ascend_npu.md:72
msgid ""
"We provide our own set of SGL kernels, check [installation guide](https://"
"github.com/sgl-project/sgl-kernel-npu/blob/main/python/sgl_kernel_npu/README."
"md)."
msgstr ""

#: ../../../platforms/ascend_npu.md:74
msgid "DeepEP-compatible Library"
msgstr ""

#: ../../../platforms/ascend_npu.md:75
msgid ""
"We provide a DeepEP-compatible Library as a drop-in replacement of deepseek-"
"ai's DeepEP library, check the [installation guide](https://github.com/sgl-"
"project/sgl-kernel-npu/blob/main/python/deep_ep/README.md)."
msgstr ""

#: ../../../platforms/ascend_npu.md:77
msgid "CustomOps"
msgstr ""

#: ../../../platforms/ascend_npu.md:78
msgid ""
"_TODO: to be removed once merged into sgl-kernel-npu._ Additional package "
"with custom operations. DEVICE_TYPE can be \"a3\" for Atlas A3 server or "
"\"910b\" for Atlas A2 server."
msgstr ""

#: ../../../platforms/ascend_npu.md:81
msgid ""
"DEVICE_TYPE=\"a3\"\n"
"wget https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/ops/CANN-"
"custom_ops-8.2.0.0-$DEVICE_TYPE-linux.aarch64.run\n"
"chmod a+x ./CANN-custom_ops-8.2.0.0-$DEVICE_TYPE-linux.aarch64.run\n"
"./CANN-custom_ops-8.2.0.0-$DEVICE_TYPE-linux.aarch64.run --quiet --install-"
"path=/usr/local/Ascend/ascend-toolkit/latest/opp\n"
"wget https://sglang-ascend.obs.cn-east-3.myhuaweicloud.com/ops/"
"custom_ops-1.0.$DEVICE_TYPE-cp311-cp311-linux_aarch64.whl\n"
"pip install ./custom_ops-1.0.$DEVICE_TYPE-cp311-cp311-linux_aarch64.whl\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:90
msgid "Installing SGLang from source"
msgstr ""

#: ../../../platforms/ascend_npu.md:92
msgid ""
"# Use the last release branch\n"
"git clone -b v0.5.6.post2 https://github.com/sgl-project/sglang.git\n"
"cd sglang\n"
"mv python/pyproject_other.toml python/pyproject.toml\n"
"pip install -e python[srt_npu]\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:100
msgid "Method 2: Using docker"
msgstr ""

#: ../../../platforms/ascend_npu.md:102
msgid ""
"__Notice:__ `--privileged` and `--network=host` are required by RDMA, which "
"is typically needed by Ascend NPU clusters."
msgstr ""

#: ../../../platforms/ascend_npu.md:104
msgid ""
"__Notice:__ The following docker command is based on Atlas 800I A3 machines. "
"If you are using Atlas 800I A2, make sure only `davinci[0-7]` are mapped "
"into container."
msgstr ""

#: ../../../platforms/ascend_npu.md:106
msgid ""
"# Clone the SGLang repository\n"
"git clone https://github.com/sgl-project/sglang.git\n"
"cd sglang/docker\n"
"\n"
"# Build the docker image\n"
"docker build -t <image_name> -f npu.Dockerfile .\n"
"\n"
"alias drun='docker run -it --rm --privileged --network=host --ipc=host --shm-"
"size=16g \\\n"
"    --device=/dev/davinci0 --device=/dev/davinci1 --device=/dev/davinci2 --"
"device=/dev/davinci3 \\\n"
"    --device=/dev/davinci4 --device=/dev/davinci5 --device=/dev/davinci6 --"
"device=/dev/davinci7 \\\n"
"    --device=/dev/davinci8 --device=/dev/davinci9 --device=/dev/davinci10 --"
"device=/dev/davinci11 \\\n"
"    --device=/dev/davinci12 --device=/dev/davinci13 --device=/dev/davinci14 "
"--device=/dev/davinci15 \\\n"
"    --device=/dev/davinci_manager --device=/dev/hisi_hdc \\\n"
"    --volume /usr/local/sbin:/usr/local/sbin --volume /usr/local/Ascend/"
"driver:/usr/local/Ascend/driver \\\n"
"    --volume /usr/local/Ascend/firmware:/usr/local/Ascend/firmware \\\n"
"    --volume /etc/ascend_install.info:/etc/ascend_install.info \\\n"
"    --volume /var/queue_schedule:/var/queue_schedule --volume ~/.cache/:/"
"root/.cache/'\n"
"\n"
"drun --env \"HF_TOKEN=<secret>\" \\\n"
"    <image_name> \\\n"
"    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-"
"Instruct --attention-backend ascend --host 0.0.0.0 --port 30000\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:130
msgid "System Settings"
msgstr ""

#: ../../../platforms/ascend_npu.md:132
msgid "CPU performance power scheme"
msgstr ""

#: ../../../platforms/ascend_npu.md:134
msgid ""
"The default power scheme on Ascend hardware is `ondemand` which could affect "
"performance, changing it to `performance` is recommended."
msgstr ""

#: ../../../platforms/ascend_npu.md:136
msgid ""
"echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/"
"scaling_governor\n"
"\n"
"# Make sure changes are applied successfully\n"
"cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor # shows "
"performance\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:143
msgid "Disable NUMA balancing"
msgstr ""

#: ../../../platforms/ascend_npu.md:145
msgid ""
"sudo sysctl -w kernel.numa_balancing=0\n"
"\n"
"# Check\n"
"cat /proc/sys/kernel/numa_balancing # shows 0\n"
msgstr ""

#: ../../../platforms/ascend_npu.md:152
msgid "Prevent swapping out system memory"
msgstr ""

#: ../../../platforms/ascend_npu.md:154
msgid ""
"sudo sysctl -w vm.swappiness=10\n"
"\n"
"# Check\n"
"cat /proc/sys/vm/swappiness # shows 10\n"
msgstr ""
