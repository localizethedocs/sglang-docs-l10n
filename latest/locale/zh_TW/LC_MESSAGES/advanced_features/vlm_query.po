# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-10 08:34+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../advanced_features/vlm_query.ipynb:2
msgid ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:9
msgid "Query Vision Language Model"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:21
msgid "Querying Qwen-VL"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:-1
msgid "[ ]:"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:-1
msgid ""
"import nest_asyncio\n"
"\n"
"nest_asyncio.apply()  # Run this first.\n"
"\n"
"model_path = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n"
"chat_template = \"qwen2-vl\""
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:-1
msgid ""
"# Lets create a prompt.\n"
"\n"
"from io import BytesIO\n"
"import requests\n"
"from PIL import Image\n"
"\n"
"from sglang.srt.parser.conversation import chat_templates\n"
"\n"
"image = Image.open(\n"
"    BytesIO(\n"
"        requests.get(\n"
"            \"https://github.com/sgl-project/sglang/blob/main/test/lang/"
"example_image.png?raw=true\"\n"
"        ).content\n"
"    )\n"
")\n"
"\n"
"conv = chat_templates[chat_template].copy()\n"
"conv.append_message(conv.roles[0], f\"What's shown here: {conv.image_token}?"
"\")\n"
"conv.append_message(conv.roles[1], \"\")\n"
"conv.image_data = [image]\n"
"\n"
"print(conv.get_prompt())\n"
"image"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:78
#: ../../../advanced_features/vlm_query.ipynb:224
msgid "Query via the offline Engine API"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:-1
msgid ""
"from sglang import Engine\n"
"\n"
"llm = Engine(\n"
"    model_path=model_path, chat_template=chat_template, "
"mem_fraction_static=0.8\n"
")"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:-1
msgid ""
"out = llm.generate(prompt=conv.get_prompt(), image_data=[image])\n"
"print(out[\"text\"])"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:113
#: ../../../advanced_features/vlm_query.ipynb:269
msgid "Query via the offline Engine API, but send precomputed embeddings"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:-1
msgid ""
"# Compute the image embeddings using Huggingface.\n"
"\n"
"from transformers import AutoProcessor\n"
"from transformers import Qwen2_5_VLForConditionalGeneration\n"
"\n"
"processor = AutoProcessor.from_pretrained(model_path, use_fast=True)\n"
"vision = (\n"
"    Qwen2_5_VLForConditionalGeneration.from_pretrained(model_path).eval()."
"visual.cuda()\n"
")"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:-1
msgid ""
"processed_prompt = processor(\n"
"    images=[image], text=conv.get_prompt(), return_tensors=\"pt\"\n"
")\n"
"input_ids = processed_prompt[\"input_ids\"][0].detach().cpu().tolist()\n"
"precomputed_embeddings = vision(\n"
"    processed_prompt[\"pixel_values\"].cuda(), "
"processed_prompt[\"image_grid_thw\"].cuda()\n"
")\n"
"\n"
"mm_item = dict(\n"
"    modality=\"IMAGE\",\n"
"    image_grid_thw=processed_prompt[\"image_grid_thw\"],\n"
"    precomputed_embeddings=precomputed_embeddings,\n"
")\n"
"out = llm.generate(input_ids=input_ids, image_data=[mm_item])\n"
"print(out[\"text\"])"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:165
msgid "Querying Llama 4 (Vision)"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:-1
msgid ""
"import nest_asyncio\n"
"\n"
"nest_asyncio.apply()  # Run this first.\n"
"\n"
"model_path = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n"
"chat_template = \"llama-4\""
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:-1
msgid ""
"# Lets create a prompt.\n"
"\n"
"from io import BytesIO\n"
"import requests\n"
"from PIL import Image\n"
"\n"
"from sglang.srt.parser.conversation import chat_templates\n"
"\n"
"image = Image.open(\n"
"    BytesIO(\n"
"        requests.get(\n"
"            \"https://github.com/sgl-project/sglang/blob/main/test/lang/"
"example_image.png?raw=true\"\n"
"        ).content\n"
"    )\n"
")\n"
"\n"
"conv = chat_templates[chat_template].copy()\n"
"conv.append_message(conv.roles[0], f\"What's shown here: {conv.image_token}?"
"\")\n"
"conv.append_message(conv.roles[1], \"\")\n"
"conv.image_data = [image]\n"
"\n"
"print(conv.get_prompt())\n"
"print(f\"Image size: {image.size}\")\n"
"\n"
"image"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:-1
msgid ""
"from sglang.test.test_utils import is_in_ci\n"
"\n"
"if not is_in_ci():\n"
"    from sglang import Engine\n"
"\n"
"    llm = Engine(\n"
"        model_path=model_path,\n"
"        trust_remote_code=True,\n"
"        enable_multimodal=True,\n"
"        mem_fraction_static=0.8,\n"
"        tp_size=4,\n"
"        attention_backend=\"fa3\",\n"
"        context_length=65536,\n"
"    )"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:-1
msgid ""
"if not is_in_ci():\n"
"    out = llm.generate(prompt=conv.get_prompt(), image_data=[image])\n"
"    print(out[\"text\"])"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:-1
msgid ""
"if not is_in_ci():\n"
"    # Compute the image embeddings using Huggingface.\n"
"\n"
"    from transformers import AutoProcessor\n"
"    from transformers import Llama4ForConditionalGeneration\n"
"\n"
"    processor = AutoProcessor.from_pretrained(model_path, use_fast=True)\n"
"    model = Llama4ForConditionalGeneration.from_pretrained(\n"
"        model_path, torch_dtype=\"auto\"\n"
"    ).eval()\n"
"    vision = model.vision_model.cuda()\n"
"    multi_modal_projector = model.multi_modal_projector.cuda()"
msgstr ""

#: ../../../advanced_features/vlm_query.ipynb:-1
msgid ""
"if not is_in_ci():\n"
"    processed_prompt = processor(\n"
"        images=[image], text=conv.get_prompt(), return_tensors=\"pt\"\n"
"    )\n"
"    print(f'{processed_prompt[\"pixel_values\"].shape=}')\n"
"    input_ids = processed_prompt[\"input_ids\"][0].detach().cpu().tolist()\n"
"\n"
"    image_outputs = vision(\n"
"        processed_prompt[\"pixel_values\"].to(\"cuda\"), "
"output_hidden_states=False\n"
"    )\n"
"    image_features = image_outputs.last_hidden_state\n"
"    vision_flat = image_features.view(-1, image_features.size(-1))\n"
"    precomputed_embeddings = multi_modal_projector(vision_flat)\n"
"\n"
"    mm_item = dict(modality=\"IMAGE\", "
"precomputed_embeddings=precomputed_embeddings)\n"
"    out = llm.generate(input_ids=input_ids, image_data=[mm_item])\n"
"    print(out[\"text\"])"
msgstr ""
