# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:37+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../advanced_features/lora.ipynb:2
msgid ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"
msgstr ""

#: ../../../advanced_features/lora.ipynb:9
msgid "LoRA Serving"
msgstr ""

#: ../../../advanced_features/lora.ipynb:20
msgid ""
"SGLang enables the use of `LoRA adapters <https://arxiv.org/"
"abs/2106.09685>`__ with a base model. By incorporating techniques from `S-"
"LoRA <https://arxiv.org/pdf/2311.03285>`__ and `Punica <https://arxiv.org/"
"pdf/2310.18547>`__, SGLang can efficiently support multiple LoRA adapters "
"for different sequences within a single batch of inputs."
msgstr ""

#: ../../../advanced_features/lora.ipynb:32
msgid "Arguments for LoRA Serving"
msgstr ""

#: ../../../advanced_features/lora.ipynb:43
msgid "The following server arguments are relevant for multi-LoRA serving:"
msgstr ""

#: ../../../advanced_features/lora.ipynb:45
msgid ""
"``enable_lora``: Enable LoRA support for the model. This argument is "
"automatically set to True if ``--lora-paths`` is provided for backward "
"compatibility."
msgstr ""

#: ../../../advanced_features/lora.ipynb:47
msgid ""
"``lora_paths``: The list of LoRA adapters to load. Each adapter must be "
"specified in one of the following formats: \\| = \\| JSON with schema "
"{\"lora_name\":str,\"lora_path\":str,\"pinned\":bool}."
msgstr ""

#: ../../../advanced_features/lora.ipynb:49
msgid ""
"``max_loras_per_batch``: Maximum number of adaptors used by each batch. This "
"argument can affect the amount of GPU memory reserved for multi-LoRA "
"serving, so it should be set to a smaller value when memory is scarce. "
"Defaults to be 8."
msgstr ""

#: ../../../advanced_features/lora.ipynb:51
msgid ""
"``max_loaded_loras``: If specified, it limits the maximum number of LoRA "
"adapters loaded in CPU memory at a time. The value must be greater than or "
"equal to ``max-loras-per-batch``."
msgstr ""

#: ../../../advanced_features/lora.ipynb:53
msgid ""
"``lora_eviction_policy``: LoRA adapter eviction policy when GPU memory pool "
"is full. ``lru``: Least Recently Used (default, better cache efficiency). "
"``fifo``: First-In-First-Out."
msgstr ""

#: ../../../advanced_features/lora.ipynb:55
msgid ""
"``lora_backend``: The backend of running GEMM kernels for Lora modules. "
"Currently we support Triton LoRA backend (``triton``) and Chunked SGMV "
"backend (``csgmv``). In the future, faster backend built upon Cutlass or "
"Cuda kernels will be added."
msgstr ""

#: ../../../advanced_features/lora.ipynb:57
msgid ""
"``max_lora_rank``: The maximum LoRA rank that should be supported. If not "
"specified, it will be automatically inferred from the adapters provided in "
"``--lora-paths``. This argument is needed when you expect to dynamically "
"load adapters of larger LoRA rank after server startup."
msgstr ""

#: ../../../advanced_features/lora.ipynb:59
msgid ""
"``lora_target_modules``: The union set of all target modules where LoRA "
"should be applied (e.g., ``q_proj``, ``k_proj``, ``gate_proj``). If not "
"specified, it will be automatically inferred from the adapters provided in "
"``--lora-paths``. This argument is needed when you expect to dynamically "
"load adapters of different target modules after server startup. You can also "
"set it to ``all`` to enable LoRA for all supported modules. However, "
"enabling LoRA on additional modules introduces a minor performance overhead. "
"If your application is performance-sensitive, we recommend only specifying "
"the modules for which you plan to load adapters."
msgstr ""

#: ../../../advanced_features/lora.ipynb:62
msgid ""
"``--max-lora-chunk-size``: Maximum chunk size for the ChunkedSGMV LoRA "
"backend. Only used when --lora-backend is 'csgmv'. Choosing a larger value "
"might improve performance. Please tune this value based on your hardware and "
"workload as needed. Defaults to 16."
msgstr ""

#: ../../../advanced_features/lora.ipynb:64
msgid ""
"``tp_size``: LoRA serving along with Tensor Parallelism is supported by "
"SGLang. ``tp_size`` controls the number of GPUs for tensor parallelism. More "
"details on the tensor sharding strategy can be found in `S-Lora <https://"
"arxiv.org/pdf/2311.03285>`__ paper."
msgstr ""

#: ../../../advanced_features/lora.ipynb:66
msgid ""
"From client side, the user needs to provide a list of strings as input "
"batch, and a list of adaptor names that each input sequence corresponds to."
msgstr ""

#: ../../../advanced_features/lora.ipynb:78
msgid "Usage"
msgstr ""

#: ../../../advanced_features/lora.ipynb:81
msgid "Serving Single Adaptor"
msgstr ""

#: ../../../advanced_features/lora.ipynb:92
msgid "**Note:** SGLang supports LoRA adapters through two APIs:"
msgstr ""

#: ../../../advanced_features/lora.ipynb:94
msgid ""
"**OpenAI-Compatible API** (``/v1/chat/completions``, ``/v1/completions``): "
"Use the ``model:adapter-name`` syntax. See `OpenAI API with LoRA <../"
"basic_usage/openai_api_completions.ipynb#Using-LoRA-Adapters>`__ for "
"examples."
msgstr ""

#: ../../../advanced_features/lora.ipynb:96
msgid ""
"**Native API** (``/generate``): Pass ``lora_path`` in the request body "
"(shown below)."
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid "[ ]:"
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid ""
"import json\n"
"import requests\n"
"\n"
"from sglang.test.doc_patch import launch_server_cmd\n"
"from sglang.utils import wait_for_server, terminate_process"
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid ""
"server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-"
"Instruct \\\n"
"    --enable-lora \\\n"
"    --lora-paths lora0=algoprog/fact-generation-llama-3.1-8b-instruct-lora "
"\\\n"
"    --max-loras-per-batch 1 \\\n"
"    --log-level warning \\\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid ""
"url = f\"http://127.0.0.1:{port}\"\n"
"json_data = {\n"
"    \"text\": [\n"
"        \"List 3 countries and their capitals.\",\n"
"        \"List 3 countries and their capitals.\",\n"
"    ],\n"
"    \"sampling_params\": {\"max_new_tokens\": 32, \"temperature\": 0},\n"
"    # The first input uses lora0, and the second input uses the base model\n"
"    \"lora_path\": [\"lora0\", None],\n"
"}\n"
"response = requests.post(\n"
"    url + \"/generate\",\n"
"    json=json_data,\n"
")\n"
"print(f\"Output 0: {response.json()[0]['text']}\")\n"
"print(f\"Output 1: {response.json()[1]['text']}\")"
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid "terminate_process(server_process)"
msgstr ""

#: ../../../advanced_features/lora.ipynb:173
msgid "Serving Multiple Adaptors"
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid ""
"server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model-path meta-llama/Meta-Llama-3.1-8B-"
"Instruct \\\n"
"    --enable-lora \\\n"
"    --lora-paths lora0=algoprog/fact-generation-llama-3.1-8b-instruct-lora "
"\\\n"
"    lora1=Nutanix/Meta-Llama-3.1-8B-Instruct_lora_4_alpha_16 \\\n"
"    --max-loras-per-batch 2 \\\n"
"    --log-level warning \\\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid ""
"url = f\"http://127.0.0.1:{port}\"\n"
"json_data = {\n"
"    \"text\": [\n"
"        \"List 3 countries and their capitals.\",\n"
"        \"List 3 countries and their capitals.\",\n"
"    ],\n"
"    \"sampling_params\": {\"max_new_tokens\": 32, \"temperature\": 0},\n"
"    # The first input uses lora0, and the second input uses lora1\n"
"    \"lora_path\": [\"lora0\", \"lora1\"],\n"
"}\n"
"response = requests.post(\n"
"    url + \"/generate\",\n"
"    json=json_data,\n"
")\n"
"print(f\"Output 0: {response.json()[0]['text']}\")\n"
"print(f\"Output 1: {response.json()[1]['text']}\")"
msgstr ""

#: ../../../advanced_features/lora.ipynb:238
msgid "Dynamic LoRA loading"
msgstr ""

#: ../../../advanced_features/lora.ipynb:249
msgid ""
"Instead of specifying all adapters during server startup via ``--lora-"
"paths``. You can also load & unload LoRA adapters dynamically via the ``/"
"load_lora_adapter`` and ``/unload_lora_adapter`` API."
msgstr ""

#: ../../../advanced_features/lora.ipynb:251
msgid ""
"When using dynamic LoRA loading, it's recommended to explicitly specify both "
"``--max-lora-rank`` and ``--lora-target-modules`` at startup. For backward "
"compatibility, SGLang will infer these values from ``--lora-paths`` if they "
"are not explicitly provided. However, in that case, you would have to ensure "
"that all dynamically loaded adapters share the same shape (rank and target "
"modules) as those in the initial ``--lora-paths`` or are strictly "
"\"smaller\"."
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid ""
"lora0 = \"Nutanix/Meta-Llama-3.1-8B-Instruct_lora_4_alpha_16\"  # rank - 4, "
"target modules - q_proj, k_proj, v_proj, o_proj, gate_proj\n"
"lora1 = \"algoprog/fact-generation-llama-3.1-8b-instruct-lora\"  # rank - "
"64, target modules - q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, "
"down_proj\n"
"lora0_new = \"philschmid/code-llama-3-1-8b-text-to-sql-lora\"  # rank - 256, "
"target modules - q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, "
"down_proj\n"
"\n"
"\n"
"# The `--target-lora-modules` param below is technically not needed, as the "
"server will infer it from lora0 which already has all the target modules "
"specified.\n"
"# We are adding it here just to demonstrate usage.\n"
"server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"    python3 -m sglang.launch_server --model-path meta-llama/Meta-"
"Llama-3.1-8B-Instruct \\\n"
"    --enable-lora \\\n"
"    --cuda-graph-max-bs 2 \\\n"
"    --max-loras-per-batch 2 \\\n"
"    --max-lora-rank 256\n"
"    --lora-target-modules all\n"
"    --log-level warning\n"
"    \"\"\"\n"
")\n"
"\n"
"url = f\"http://127.0.0.1:{port}\"\n"
"wait_for_server(url)"
msgstr ""

#: ../../../advanced_features/lora.ipynb:291
msgid "Load adapter lora0"
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid ""
"response = requests.post(\n"
"    url + \"/load_lora_adapter\",\n"
"    json={\n"
"        \"lora_name\": \"lora0\",\n"
"        \"lora_path\": lora0,\n"
"    },\n"
")\n"
"\n"
"if response.status_code == 200:\n"
"    print(\"LoRA adapter loaded successfully.\", response.json())\n"
"else:\n"
"    print(\"Failed to load LoRA adapter.\", response.json())"
msgstr ""

#: ../../../advanced_features/lora.ipynb:322
msgid "Load adapter lora1:"
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid ""
"response = requests.post(\n"
"    url + \"/load_lora_adapter\",\n"
"    json={\n"
"        \"lora_name\": \"lora1\",\n"
"        \"lora_path\": lora1,\n"
"    },\n"
")\n"
"\n"
"if response.status_code == 200:\n"
"    print(\"LoRA adapter loaded successfully.\", response.json())\n"
"else:\n"
"    print(\"Failed to load LoRA adapter.\", response.json())"
msgstr ""

#: ../../../advanced_features/lora.ipynb:353
msgid "Check inference output:"
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid ""
"url = f\"http://127.0.0.1:{port}\"\n"
"json_data = {\n"
"    \"text\": [\n"
"        \"List 3 countries and their capitals.\",\n"
"        \"List 3 countries and their capitals.\",\n"
"    ],\n"
"    \"sampling_params\": {\"max_new_tokens\": 32, \"temperature\": 0},\n"
"    # The first input uses lora0, and the second input uses lora1\n"
"    \"lora_path\": [\"lora0\", \"lora1\"],\n"
"}\n"
"response = requests.post(\n"
"    url + \"/generate\",\n"
"    json=json_data,\n"
")\n"
"print(f\"Output from lora0: \\n{response.json()[0]['text']}\\n\")\n"
"print(f\"Output from lora1 (updated): \\n{response.json()[1]['text']}\\n\")"
msgstr ""

#: ../../../advanced_features/lora.ipynb:388
msgid "Unload lora0 and replace it with a different adapter:"
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid ""
"response = requests.post(\n"
"    url + \"/unload_lora_adapter\",\n"
"    json={\n"
"        \"lora_name\": \"lora0\",\n"
"    },\n"
")\n"
"\n"
"response = requests.post(\n"
"    url + \"/load_lora_adapter\",\n"
"    json={\n"
"        \"lora_name\": \"lora0\",\n"
"        \"lora_path\": lora0_new,\n"
"    },\n"
")\n"
"\n"
"if response.status_code == 200:\n"
"    print(\"LoRA adapter loaded successfully.\", response.json())\n"
"else:\n"
"    print(\"Failed to load LoRA adapter.\", response.json())"
msgstr ""

#: ../../../advanced_features/lora.ipynb:426
msgid "Check output again:"
msgstr ""

#: ../../../advanced_features/lora.ipynb:462
msgid "OpenAI-compatible API usage"
msgstr ""

#: ../../../advanced_features/lora.ipynb:464
msgid ""
"You can use LoRA adapters via the OpenAI-compatible APIs by specifying the "
"adapter in the ``model`` field using the ``base-model:adapter-name`` syntax "
"(for example, ``qwen/qwen2.5-0.5b-instruct:adapter_a``). For more details "
"and examples, see the “Using LoRA Adapters” section in the OpenAI API "
"documentation: `openai_api_completions.ipynb <../basic_usage/"
"openai_api_completions.ipynb>`__."
msgstr ""

#: ../../../advanced_features/lora.ipynb:485
msgid "LoRA GPU Pinning"
msgstr ""

#: ../../../advanced_features/lora.ipynb:496
msgid ""
"Another advanced option is to specify adapters as ``pinned`` during loading. "
"When an adapter is pinned, it is permanently assigned to one of the "
"available GPU pool slots (as configured by ``--max-loras-per-batch``) and "
"will not be evicted from GPU memory during runtime. Instead, it remains "
"resident until it is explicitly unloaded."
msgstr ""

#: ../../../advanced_features/lora.ipynb:498
msgid ""
"This can improve performance in scenarios where the same adapter is "
"frequently used across requests, by avoiding repeated memory transfers and "
"reinitialization overhead. However, since GPU pool slots are limited, "
"pinning adapters reduces the flexibility of the system to dynamically load "
"other adapters on demand. If too many adapters are pinned, it may lead to "
"degraded performance, or in the most extreme case (``Number of pinned "
"adapters == max-loras-per-batch``), halt all unpinned requests. Therefore, "
"currently SGLang limits maximal number of pinned adapters to ``max-loras-per-"
"batch - 1`` to prevent unexpected starvations."
msgstr ""

#: ../../../advanced_features/lora.ipynb:501
msgid ""
"In the example below, we start a server with ``lora1`` loaded as pinned, "
"``lora2`` and ``lora3`` loaded as regular (unpinned) adapters. Please note "
"that, we intentionally specify ``lora2`` and ``lora3`` in two different "
"formats to demonstrate that both are supported."
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid ""
"server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"    python3 -m sglang.launch_server --model-path meta-llama/Meta-"
"Llama-3.1-8B-Instruct \\\n"
"    --enable-lora \\\n"
"    --cuda-graph-max-bs 8 \\\n"
"    --max-loras-per-batch 3 \\\n"
"    --max-lora-rank 256 \\\n"
"    --lora-target-modules all \\\n"
"    --lora-paths \\\n"
"        {\"lora_name\":\"lora0\",\"lora_path\":\"Nutanix/Meta-Llama-3.1-8B-"
"Instruct_lora_4_alpha_16\",\"pinned\":true} \\\n"
"        {\"lora_name\":\"lora1\",\"lora_path\":\"algoprog/fact-generation-"
"llama-3.1-8b-instruct-lora\"} \\\n"
"        lora2=philschmid/code-llama-3-1-8b-text-to-sql-lora\n"
"    --log-level warning\n"
"    \"\"\"\n"
")\n"
"\n"
"\n"
"url = f\"http://127.0.0.1:{port}\"\n"
"wait_for_server(url)"
msgstr ""

#: ../../../advanced_features/lora.ipynb:539
msgid ""
"You can also specify adapter as pinned during dynamic adapter loading. In "
"the example below, we reload ``lora2`` as pinned adapter:"
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid ""
"response = requests.post(\n"
"    url + \"/unload_lora_adapter\",\n"
"    json={\n"
"        \"lora_name\": \"lora1\",\n"
"    },\n"
")\n"
"\n"
"response = requests.post(\n"
"    url + \"/load_lora_adapter\",\n"
"    json={\n"
"        \"lora_name\": \"lora1\",\n"
"        \"lora_path\": \"algoprog/fact-generation-llama-3.1-8b-instruct-"
"lora\",\n"
"        \"pinned\": True,  # Pin the adapter to GPU\n"
"    },\n"
")"
msgstr ""

#: ../../../advanced_features/lora.ipynb:573
msgid "Verify that the results are expected:"
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid ""
"url = f\"http://127.0.0.1:{port}\"\n"
"json_data = {\n"
"    \"text\": [\n"
"        \"List 3 countries and their capitals.\",\n"
"        \"List 3 countries and their capitals.\",\n"
"        \"List 3 countries and their capitals.\",\n"
"    ],\n"
"    \"sampling_params\": {\"max_new_tokens\": 32, \"temperature\": 0},\n"
"    # The first input uses lora0, and the second input uses lora1\n"
"    \"lora_path\": [\"lora0\", \"lora1\", \"lora2\"],\n"
"}\n"
"response = requests.post(\n"
"    url + \"/generate\",\n"
"    json=json_data,\n"
")\n"
"print(f\"Output from lora0 (pinned): \\n{response.json()[0]['text']}\\n\")\n"
"print(f\"Output from lora1 (pinned): \\n{response.json()[1]['text']}\\n\")\n"
"print(f\"Output from lora2 (not pinned): \\n{response.json()[2]"
"['text']}\\n\")"
msgstr ""

#: ../../../advanced_features/lora.ipynb:620
msgid "Choosing LoRA Backend"
msgstr ""

#: ../../../advanced_features/lora.ipynb:622
msgid ""
"SGLang supports two LoRA backends that you can choose from using the ``--"
"lora-backend`` argument:"
msgstr ""

#: ../../../advanced_features/lora.ipynb:624
msgid "``triton``: Default basic Triton-based backend."
msgstr ""

#: ../../../advanced_features/lora.ipynb:625
msgid ""
"``csgmv``: Chunked SGMV backend optimized for high concurrency scenarios."
msgstr ""

#: ../../../advanced_features/lora.ipynb:627
msgid ""
"The ``csgmv`` backend was recently introduced to improve performance "
"especially at high-concurrency scenarios. Our benchmark shows that it "
"achieves 20% to 80% latency improvements over the basic triton backend. "
"Currently it is at preview phase, we expect to make it our the default LoRA "
"backend in future release. Before that, you can adopt it by manually setting "
"the ``--lora-backend`` server config."
msgstr ""

#: ../../../advanced_features/lora.ipynb:-1
msgid ""
"server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"    python3 -m sglang.launch_server \\\n"
"    --model-path meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"    --enable-lora \\\n"
"    --lora-backend csgmv \\\n"
"    --max-loras-per-batch 16 \\\n"
"    --lora-paths lora1=path/to/lora1 lora2=path/to/lora2\n"
"    \"\"\"\n"
")"
msgstr ""

#: ../../../advanced_features/lora.ipynb:666
msgid "Future Works"
msgstr ""

#: ../../../advanced_features/lora.ipynb:668
msgid ""
"The development roadmap for LoRA-related features can be found in this "
"`issue <https://github.com/sgl-project/sglang/issues/2929>`__. Other "
"features, including Embedding Layer, Unified Paging, Cutlass backend are "
"still under development."
msgstr ""
