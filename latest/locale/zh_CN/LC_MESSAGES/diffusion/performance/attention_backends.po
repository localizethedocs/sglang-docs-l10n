# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2026, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-02-16 08:46+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../diffusion/performance/attention_backends.md:1
msgid "Attention Backends"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:3
msgid ""
"This document describes the attention backends available in sglang diffusion "
"(`sglang.multimodal_gen`) and how to select them."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:5
msgid "Overview"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:7
msgid ""
"Attention backends are defined by `AttentionBackendEnum` (`sglang."
"multimodal_gen.runtime.platforms.interface.AttentionBackendEnum`) and "
"selected via the CLI flag `--attention-backend`."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:9
msgid ""
"Backend selection is performed by the shared attention layers (e.g. "
"`LocalAttention` / `USPAttention` / `UlyssesAttention` in `sglang."
"multimodal_gen.runtime.layers.attention.layer`) and therefore applies to any "
"model component using these layers (e.g. diffusion transformer / DiT and "
"encoders)."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:11
msgid ""
"When using the diffusers backend, `--attention-backend` is passed through to "
"diffusers' `set_attention_backend` (e.g., `flash`, `_flash_3_hub`, `sage`, "
"`xformers`, `native`)."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:14
msgid ""
"**CUDA**: prefers FlashAttention (FA3/FA4) when supported; otherwise falls "
"back to PyTorch SDPA."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:15
msgid ""
"**ROCm**: uses FlashAttention when available; otherwise falls back to "
"PyTorch SDPA."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:16
msgid "**MPS**: always uses PyTorch SDPA."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:18
msgid "Backend options"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:20
msgid ""
"For SGLang-native pipelines, the CLI accepts the lowercase names of "
"`AttentionBackendEnum`. The table below lists the backends implemented by "
"the built-in platforms. `fa3`/`fa4` are accepted as aliases for `fa`."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "CLI value"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Enum value"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Notes"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`fa` / `fa3` / `fa4`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`FA`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid ""
"FlashAttention. `fa3/fa4` are normalized to `fa` during argument parsing "
"(`ServerArgs.__post_init__`)."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`torch_sdpa`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`TORCH_SDPA`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "PyTorch `scaled_dot_product_attention`."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`sliding_tile_attn`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`SLIDING_TILE_ATTN`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid ""
"Sliding Tile Attention (STA). Requires `st_attn`. Configure via `--attention-"
"backend-config`."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`sage_attn`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`SAGE_ATTN`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid ""
"Requires `sageattention`. Upstream SageAttention CUDA extensions target SM80/"
"SM86/SM89/SM90/SM120 (compute capability 8.0/8.6/8.9/9.0/12.0); see upstream "
"`setup.py`: https://github.com/thu-ml/SageAttention/blob/main/setup.py."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`sage_attn_3`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`SAGE_ATTN_3`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Requires SageAttention3 installed per upstream instructions."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`video_sparse_attn`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`VIDEO_SPARSE_ATTN`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Requires `vsa`. Configure `sparsity` via `--attention-backend-config`."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`vmoba_attn`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`VMOBA_ATTN`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid ""
"Requires `kernel.attn.vmoba_attn.vmoba`. Configure via `--attention-backend-"
"config`."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`aiter`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`AITER`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Requires `aiter`."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`sparse_video_gen_2_attn`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`SPARSE_VIDEO_GEN_2_ATTN`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid ""
"Requires `svg`. See installation instructions at https://github.com/svg-"
"project/Sparse-VideoGen."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:34
msgid "Selection priority"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:36
msgid "The selection order in `runtime/layers/attention/selector.py` is:"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:38
msgid ""
"`global_force_attn_backend(...)` / "
"`global_force_attn_backend_context_manager(...)`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:39
msgid "CLI `--attention-backend` (`ServerArgs.attention_backend`)"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:40
msgid "Auto selection (platform capability, dtype, and installed packages)"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:42
msgid "Configuration"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:44
msgid ""
"Some backends require additional configuration. You can pass these "
"parameters via `--attention-backend-config`. This argument accepts:"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:45
msgid "A path to a JSON or YAML configuration file."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:46
msgid "A JSON string (e.g., `'{\"sparsity\": 0.5}'`)."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:47
msgid "Key-value pairs (e.g., `\"sparsity=0.5,enable_x=true\"`)."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:49
msgid "Supported Configuration Parameters"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:51
msgid "**Sliding Tile Attention (`sliding_tile_attn`)**"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Parameter"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Type"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Description"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Default"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`mask_strategy_file_path`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`str`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "**Required.** Path to the mask strategy JSON file."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "-"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`sta_mode`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Mode of STA."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`STA_inference`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`skip_time_steps`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`int`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid ""
"Number of steps to use full attention before switching to sparse attention."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`15`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:59
msgid "**Video Sparse Attention (`video_sparse_attn`)**"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`sparsity`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`float`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Validation sparsity (0.0 - 1.0)."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`0.0`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:65
msgid "**V-MoBA (`vmoba_attn`)**"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`temporal_chunk_size`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Chunk size for temporal dimension."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`temporal_topk`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Top-K tokens to select in temporal dimension."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`spatial_chunk_size`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`list[int]`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Chunk size for spatial dimension (H, W)."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`spatial_topk`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Top-K tokens to select in spatial dimension."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`st_chunk_size`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Chunk size for spatiotemporal dimension (T, H, W)."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`st_topk`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Top-K tokens to select in spatiotemporal dimension."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`moba_select_mode`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Selection mode (e.g., `threshold`)."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`threshold`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`moba_threshold`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Threshold value for selection."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`0.25`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`moba_threshold_type`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Type of thresholding (e.g., `query_head`)."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`query_head`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`first_full_step`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Number of initial steps to use full attention."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`12`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`first_full_layer`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Number of initial layers to use full attention."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`0`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`temporal_layer`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Number of temporal layers."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`1`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`spatial_layer`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Number of spatial layers."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`st_layer`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Number of spatiotemporal layers."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:84
msgid "Platform support matrix"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Backend"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "CUDA"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "ROCm"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "MPS"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "`fa`"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "✅"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "❌"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid ""
"CUDA requires SM80+ and fp16/bf16. FlashAttention is only used when the "
"required runtime is installed; otherwise it falls back to `torch_sdpa`."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "Most compatible option across platforms."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid ""
"CUDA-only. Requires `st_attn`. Configure via `--attention-backend-config`."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "CUDA-only (optional dependency)."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid ""
"CUDA-only. Requires `vsa`. Configure `sparsity` via `--attention-backend-"
"config`."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid ""
"CUDA-only. Requires `kernel.attn.vmoba_attn.vmoba`. Configure via `--"
"attention-backend-config`."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:0
msgid "CUDA-only. Requires `svg`."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:98
msgid "Usage"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:100
msgid "Select a backend via CLI"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:102
msgid ""
"sglang generate \\\n"
"  --model-path <MODEL_PATH_OR_ID> \\\n"
"  --prompt \"...\" \\\n"
"  --attention-backend fa\n"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:109
msgid ""
"sglang generate \\\n"
"  --model-path <MODEL_PATH_OR_ID> \\\n"
"  --prompt \"...\" \\\n"
"  --attention-backend torch_sdpa\n"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:116
msgid "Using Sliding Tile Attention (STA)"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:118
msgid ""
"# Pass the mask strategy file path via config\n"
"sglang generate \\\n"
"  --model-path <MODEL_PATH_OR_ID> \\\n"
"  --prompt \"...\" \\\n"
"  --attention-backend sliding_tile_attn \\\n"
"  --attention-backend-config \"mask_strategy_file_path=/abs/path/to/"
"mask_strategy.json\"\n"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:127
msgid "Notes for ROCm / MPS"
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:129
msgid ""
"ROCm: use `--attention-backend torch_sdpa` or `fa` depending on what is "
"available in your environment."
msgstr ""

#: ../../../diffusion/performance/attention_backends.md:130
msgid "MPS: the platform implementation always uses `torch_sdpa`."
msgstr ""
