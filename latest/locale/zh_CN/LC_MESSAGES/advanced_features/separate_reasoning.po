# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-10 08:34+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../advanced_features/separate_reasoning.ipynb:2
msgid ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:9
msgid "Reasoning Parser"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:11
msgid ""
"SGLang supports parsing reasoning content out from \"normal\" content for "
"reasoning models such as `DeepSeek R1 <https://huggingface.co/deepseek-ai/"
"DeepSeek-R1>`__."
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:14
msgid "Supported Models & Parsers"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:17
msgid "Model"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:17
msgid "Reasoning tags"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:17
msgid "Parser"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:17
msgid "Notes"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:19
msgid ""
"`DeepSeek‑R1 series <https://huggingface.co/collections/deepseek-ai/deepseek-"
"r1-678e1e131c0169c0bc89728d>`__"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:19
#: ../../../advanced_features/separate_reasoning.ipynb:22
#: ../../../advanced_features/separate_reasoning.ipynb:26
#: ../../../advanced_features/separate_reasoning.ipynb:29
msgid "``<think>`` … ``</think>``"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:19
msgid "``deepseek-r1``"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:19
msgid "Supports all variants (R1, R1-0528, R1-Distill)"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:22
msgid ""
"`DeepSeek‑V3 series <https://huggingface.co/deepseek-ai/DeepSeek-V3.1>`__"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:22
msgid "``deepseek-v3``"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:22
msgid ""
"Including `DeepSeek‑V3.2 <https://huggingface.co/deepseek-ai/DeepSeek-V3.2-"
"Exp>`__. Supports ``thinking`` parameter"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:26
msgid ""
"`Standard Qwen3 models <https://huggingface.co/collections/Qwen/"
"qwen3-67dd247413f0e2e4f653967f>`__"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:26
msgid "``qwen3``"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:26
msgid "Supports ``enable_thinking`` parameter"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:29
msgid ""
"`Qwen3-Thinking models <https://huggingface.co/Qwen/Qwen3-235B-A22B-"
"Thinking-2507>`__"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:29
msgid "``qwen3`` or ``qwen3-thinking``"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:29
msgid "Always generates thinking content"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:31
msgid "`Kimi models <https://huggingface.co/moonshotai/models>`__"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:31
msgid "``◁think▷`` … ``◁/think▷``"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:31
msgid "``kimi``"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:31
msgid "Uses special thinking delimiters"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:33
msgid "`GPT OSS <https://huggingface.co/openai/gpt-oss-120b>`__"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:33
msgid "``<\\|channel\\|>analysis<\\|message\\|>`` … ``<\\|end\\|>``"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:33
msgid "``gpt-oss``"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:33
msgid "N/A"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:37
msgid "Model-Specific Behaviors"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:39
msgid "**DeepSeek-R1 Family:**"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:41
msgid ""
"DeepSeek-R1: No ``<think>`` start tag, jumps directly to thinking content"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:42
msgid ""
"DeepSeek-R1-0528: Generates both ``<think>`` start and ``</think>`` end tags"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:43
msgid "Both are handled by the same ``deepseek-r1`` parser"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:45
msgid "**DeepSeek-V3 Family:**"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:47
msgid ""
"DeepSeek-V3.1/V3.2: Hybrid model supporting both thinking and non-thinking "
"modes, use the ``deepseek-v3`` parser and ``thinking`` parameter (NOTE: not "
"``enable_thinking``)"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:49
msgid "**Qwen3 Family:**"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:51
msgid ""
"Standard Qwen3 (e.g., Qwen3-2507): Use ``qwen3`` parser, supports "
"``enable_thinking`` in chat templates"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:52
msgid ""
"Qwen3-Thinking (e.g., Qwen3-235B-A22B-Thinking-2507): Use ``qwen3`` or "
"``qwen3-thinking`` parser, always thinks"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:54
msgid "**Kimi:**"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:56
msgid "Kimi: Uses special ``◁think▷`` and ``◁/think▷`` tags"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:58
msgid "**GPT OSS:**"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:60
msgid ""
"GPT OSS: Uses special ``<|channel|>analysis<|message|>`` and ``<|end|>`` tags"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:72
msgid "Usage"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:75
msgid "Launching the Server"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:86
msgid "Specify the ``--reasoning-parser`` option."
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:-1
msgid "[ ]:"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:-1
msgid ""
"import requests\n"
"from openai import OpenAI\n"
"from sglang.test.doc_patch import launch_server_cmd\n"
"from sglang.utils import wait_for_server, print_highlight, "
"terminate_process\n"
"\n"
"server_process, port = launch_server_cmd(\n"
"    \"python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-R1-"
"Distill-Qwen-7B --host 0.0.0.0 --reasoning-parser deepseek-r1 --log-level "
"warning\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:115
msgid ""
"Note that ``--reasoning-parser`` defines the parser used to interpret "
"responses."
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:127
msgid "OpenAI Compatible API"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:129
msgid ""
"Using the OpenAI compatible API, the contract follows the `DeepSeek API "
"design <https://api-docs.deepseek.com/guides/reasoning_model>`__ established "
"with the release of DeepSeek-R1:"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:131
msgid "``reasoning_content``: The content of the CoT."
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:132
msgid "``content``: The content of the final answer."
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:-1
msgid ""
"# Initialize OpenAI-like client\n"
"client = OpenAI(api_key=\"None\", base_url=f\"http://0.0.0.0:{port}/v1\")\n"
"model_name = client.models.list().data[0].id\n"
"\n"
"messages = [\n"
"    {\n"
"        \"role\": \"user\",\n"
"        \"content\": \"What is 1+3?\",\n"
"    }\n"
"]"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:162
msgid "Non-Streaming Request"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:-1
msgid ""
"response_non_stream = client.chat.completions.create(\n"
"    model=model_name,\n"
"    messages=messages,\n"
"    temperature=0.6,\n"
"    top_p=0.95,\n"
"    stream=False,  # Non-streaming\n"
"    extra_body={\"separate_reasoning\": True},\n"
")\n"
"print_highlight(\"==== Reasoning ====\")\n"
"print_highlight(response_non_stream.choices[0].message.reasoning_content)\n"
"\n"
"print_highlight(\"==== Text ====\")\n"
"print_highlight(response_non_stream.choices[0].message.content)"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:195
msgid "Streaming Request"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:-1
msgid ""
"response_stream = client.chat.completions.create(\n"
"    model=model_name,\n"
"    messages=messages,\n"
"    temperature=0.6,\n"
"    top_p=0.95,\n"
"    stream=True,  # Non-streaming\n"
"    extra_body={\"separate_reasoning\": True},\n"
")\n"
"\n"
"reasoning_content = \"\"\n"
"content = \"\"\n"
"for chunk in response_stream:\n"
"    if chunk.choices[0].delta.content:\n"
"        content += chunk.choices[0].delta.content\n"
"    if chunk.choices[0].delta.reasoning_content:\n"
"        reasoning_content += chunk.choices[0].delta.reasoning_content\n"
"\n"
"print_highlight(\"==== Reasoning ====\")\n"
"print_highlight(reasoning_content)\n"
"\n"
"print_highlight(\"==== Text ====\")\n"
"print_highlight(content)"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:236
msgid ""
"Optionally, you can buffer the reasoning content to the last reasoning chunk "
"(or the first chunk after the reasoning content)."
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:-1
msgid ""
"response_stream = client.chat.completions.create(\n"
"    model=model_name,\n"
"    messages=messages,\n"
"    temperature=0.6,\n"
"    top_p=0.95,\n"
"    stream=True,  # Non-streaming\n"
"    extra_body={\"separate_reasoning\": True, \"stream_reasoning\": False},\n"
")\n"
"\n"
"reasoning_content = \"\"\n"
"content = \"\"\n"
"for chunk in response_stream:\n"
"    if chunk.choices[0].delta.content:\n"
"        content += chunk.choices[0].delta.content\n"
"    if chunk.choices[0].delta.reasoning_content:\n"
"        reasoning_content += chunk.choices[0].delta.reasoning_content\n"
"\n"
"print_highlight(\"==== Reasoning ====\")\n"
"print_highlight(reasoning_content)\n"
"\n"
"print_highlight(\"==== Text ====\")\n"
"print_highlight(content)"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:277
msgid ""
"The reasoning separation is enable by default when specify . **To disable "
"it, set the ``separate_reasoning`` option to ``False`` in request.**"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:-1
msgid ""
"response_non_stream = client.chat.completions.create(\n"
"    model=model_name,\n"
"    messages=messages,\n"
"    temperature=0.6,\n"
"    top_p=0.95,\n"
"    stream=False,  # Non-streaming\n"
"    extra_body={\"separate_reasoning\": False},\n"
")\n"
"\n"
"print_highlight(\"==== Original Output ====\")\n"
"print_highlight(response_non_stream.choices[0].message.content)"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:308
msgid "SGLang Native API"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:-1
msgid ""
"from transformers import AutoTokenizer\n"
"\n"
"tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-"
"Qwen-7B\")\n"
"input = tokenizer.apply_chat_template(\n"
"    messages,\n"
"    tokenize=False,\n"
"    add_generation_prompt=True,\n"
")\n"
"\n"
"gen_url = f\"http://localhost:{port}/generate\"\n"
"gen_data = {\n"
"    \"text\": input,\n"
"    \"sampling_params\": {\n"
"        \"skip_special_tokens\": False,\n"
"        \"max_new_tokens\": 1024,\n"
"        \"temperature\": 0.6,\n"
"        \"top_p\": 0.95,\n"
"    },\n"
"}\n"
"gen_response = requests.post(gen_url, json=gen_data).json()[\"text\"]\n"
"\n"
"print_highlight(\"==== Original Output ====\")\n"
"print_highlight(gen_response)\n"
"\n"
"parse_url = f\"http://localhost:{port}/separate_reasoning\"\n"
"separate_reasoning_data = {\n"
"    \"text\": gen_response,\n"
"    \"reasoning_parser\": \"deepseek-r1\",\n"
"}\n"
"separate_reasoning_response_json = requests.post(\n"
"    parse_url, json=separate_reasoning_data\n"
").json()\n"
"print_highlight(\"==== Reasoning ====\")\n"
"print_highlight(separate_reasoning_response_json[\"reasoning_text\"])\n"
"print_highlight(\"==== Text ====\")\n"
"print_highlight(separate_reasoning_response_json[\"text\"])"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:-1
msgid "terminate_process(server_process)"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:373
msgid "Offline Engine API"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:-1
msgid ""
"import sglang as sgl\n"
"from sglang.srt.parser.reasoning_parser import ReasoningParser\n"
"from sglang.utils import print_highlight\n"
"\n"
"llm = sgl.Engine(model_path=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n"
"tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-"
"Qwen-7B\")\n"
"input = tokenizer.apply_chat_template(\n"
"    messages,\n"
"    tokenize=False,\n"
"    add_generation_prompt=True,\n"
")\n"
"sampling_params = {\n"
"    \"max_new_tokens\": 1024,\n"
"    \"skip_special_tokens\": False,\n"
"    \"temperature\": 0.6,\n"
"    \"top_p\": 0.95,\n"
"}\n"
"result = llm.generate(prompt=input, sampling_params=sampling_params)\n"
"\n"
"generated_text = result[\"text\"]  # Assume there is only one prompt\n"
"\n"
"print_highlight(\"==== Original Output ====\")\n"
"print_highlight(generated_text)\n"
"\n"
"parser = ReasoningParser(\"deepseek-r1\")\n"
"reasoning_text, text = parser.parse_non_stream(generated_text)\n"
"print_highlight(\"==== Reasoning ====\")\n"
"print_highlight(reasoning_text)\n"
"print_highlight(\"==== Text ====\")\n"
"print_highlight(text)"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:-1
msgid "llm.shutdown()"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:432
msgid "Supporting New Reasoning Model Schemas"
msgstr ""

#: ../../../advanced_features/separate_reasoning.ipynb:434
msgid ""
"For future reasoning models, you can implement the reasoning parser as a "
"subclass of ``BaseReasoningFormatDetector`` in ``python/sglang/srt/"
"reasoning_parser.py`` and specify the reasoning parser for new reasoning "
"model schemas accordingly."
msgstr ""
