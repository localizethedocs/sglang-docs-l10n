# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2026, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-02-16 08:46+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../supported_models/extending/transformers_fallback.md:1
msgid "Transformers fallback in SGLang"
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:3
msgid ""
"`sglang` can fall back to using models that are available in `transformers`. "
"This works for most decoder-style language models and support for vision-"
"language models is coming soon!"
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:5
msgid "Example launch Command"
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:7
msgid ""
"By default, we will use sglang implementation if it is available. Otherwise, "
"we will fall back to transformers one. However, you can switch the "
"implementation by setting `--model-impl` to `transformers`."
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:9
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model-path meta-llama/Llama-3.2-1B-Instruct \\\n"
"  --host 0.0.0.0 \\\n"
"  --port 30000 \\\n"
"  --model-impl transformers\n"
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:17
msgid "Supported features"
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:19
msgid "Quantization"
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:21
msgid ""
"Transformers fall back has supported most of available quantization in "
"SGLang (except GGUF). See [Quantization page](../advanced_features/"
"quantization.md) for more information about supported quantization in SGLang."
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:23
msgid "Remote code"
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:25
msgid ""
"This fallback also means that any model on the hub that can be used in "
"`transformers` with `trust_remote_code=True` that correctly implements "
"attention can be used in production!"
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:27
msgid "A model just needs the following two things:"
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:29
msgid ""
"from transformers import PreTrainedModel\n"
"from torch import nn\n"
"\n"
"class MyAttention(nn.Module):\n"
"\n"
"  def forward(self, hidden_states, **kwargs): # <- kwargs are required\n"
"\n"
"    ...\n"
"    attention_interface = ALL_ATTENTION_FUNCTIONS[self.config."
"_attn_implementation]\n"
"    attn_output, attn_weights = attention_interface(\n"
"      self,\n"
"      query_states,\n"
"      key_states,\n"
"      value_states,\n"
"      **kwargs,\n"
"    )\n"
"    ...\n"
"\n"
"class MyModel(PreTrainedModel):\n"
"  _supports_attention_backend = True\n"
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:52
msgid "Here is what happens in the background:"
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:54
msgid "The config is loaded"
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:55
msgid ""
"`MyModel` python class is loaded from the `auto_map`, and we check that the "
"model `_supports_attention_backend`."
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:56
msgid ""
"The `TransformersModel` backend is used. See `/srt/models/transformers`, "
"which leverages `self.config._attn_implementation = \"sglang\"`, thus the "
"need to use `ALL_ATTENTION_FUNCTIONS`."
msgstr ""

#: ../../../supported_models/extending/transformers_fallback.md:58
msgid "That's it!"
msgstr ""
