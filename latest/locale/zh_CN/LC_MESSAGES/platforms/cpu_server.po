# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang latest\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-08 08:37+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../platforms/cpu_server.md:1
msgid "CPU Servers"
msgstr ""

#: ../../../platforms/cpu_server.md:3
msgid ""
"The document addresses how to set up the [SGLang](https://github.com/sgl-"
"project/sglang) environment and run LLM inference on CPU servers. SGLang is "
"enabled and optimized on the CPUs equipped with Intel® AMX® Instructions, "
"which are 4th generation or newer Intel® Xeon® Scalable Processors."
msgstr ""

#: ../../../platforms/cpu_server.md:7
msgid "Optimized Model List"
msgstr ""

#: ../../../platforms/cpu_server.md:9
msgid ""
"A list of popular LLMs are optimized and run efficiently on CPU, including "
"the most notable open-source models like Llama series, Qwen series, and "
"DeepSeek series like DeepSeek-R1 and DeepSeek-V3.1-Terminus."
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "Model Name"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "BF16"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "W8A8_INT8"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "FP8"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "DeepSeek-R1"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[meituan/DeepSeek-R1-Channel-INT8](https://huggingface.co/meituan/DeepSeek-"
"R1-Channel-INT8)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[deepseek-ai/DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "DeepSeek-V3.1-Terminus"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[IntervitensInc/DeepSeek-V3.1-Terminus-Channel-int8](https://huggingface.co/"
"IntervitensInc/DeepSeek-V3.1-Terminus-Channel-int8)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[deepseek-ai/DeepSeek-V3.1-Terminus](https://huggingface.co/deepseek-ai/"
"DeepSeek-V3.1-Terminus)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "Llama-3.2-3B"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[meta-llama/Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/"
"Llama-3.2-3B-Instruct)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[RedHatAI/Llama-3.2-3B-quantized.w8a8](https://huggingface.co/RedHatAI/"
"Llama-3.2-3B-Instruct-quantized.w8a8)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "Llama-3.1-8B"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/"
"Llama-3.1-8B-Instruct)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[RedHatAI/Meta-Llama-3.1-8B-quantized.w8a8](https://huggingface.co/RedHatAI/"
"Meta-Llama-3.1-8B-quantized.w8a8)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "QwQ-32B"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[RedHatAI/QwQ-32B-quantized.w8a8](https://huggingface.co/RedHatAI/QwQ-32B-"
"quantized.w8a8)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "DeepSeek-Distilled-Llama"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[RedHatAI/DeepSeek-R1-Distill-Llama-70B-quantized.w8a8](https://huggingface."
"co/RedHatAI/DeepSeek-R1-Distill-Llama-70B-quantized.w8a8)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "Qwen3-235B"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[Qwen/Qwen3-235B-A22B-FP8](https://huggingface.co/Qwen/Qwen3-235B-A22B-FP8)"
msgstr ""

#: ../../../platforms/cpu_server.md:23
msgid ""
"**Note:** The model identifiers listed in the table above have been verified "
"on 6th Gen Intel® Xeon® P-core platforms."
msgstr ""

#: ../../../platforms/cpu_server.md:26
msgid "Installation"
msgstr ""

#: ../../../platforms/cpu_server.md:28
msgid "Install Using Docker"
msgstr ""

#: ../../../platforms/cpu_server.md:30
msgid ""
"It is recommended to use Docker for setting up the SGLang environment. A "
"[Dockerfile](https://github.com/sgl-project/sglang/blob/main/docker/xeon."
"Dockerfile) is provided to facilitate the installation. Replace `<secret>` "
"below with your [HuggingFace access token](https://huggingface.co/docs/hub/"
"en/security-tokens)."
msgstr ""

#: ../../../platforms/cpu_server.md:34
msgid ""
"# Clone the SGLang repository\n"
"git clone https://github.com/sgl-project/sglang.git\n"
"cd sglang/docker\n"
"\n"
"# Build the docker image\n"
"docker build -t sglang-cpu:latest -f xeon.Dockerfile .\n"
"\n"
"# Initiate a docker container\n"
"docker run \\\n"
"    -it \\\n"
"    --privileged \\\n"
"    --ipc=host \\\n"
"    --network=host \\\n"
"    -v /dev/shm:/dev/shm \\\n"
"    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n"
"    -p 30000:30000 \\\n"
"    -e \"HF_TOKEN=<secret>\" \\\n"
"    sglang-cpu:latest /bin/bash\n"
msgstr ""

#: ../../../platforms/cpu_server.md:55
msgid "Install From Source"
msgstr ""

#: ../../../platforms/cpu_server.md:57
msgid ""
"If you prefer to install SGLang in a bare metal environment, the setup "
"process is as follows:"
msgstr ""

#: ../../../platforms/cpu_server.md:60
msgid ""
"Please install the required packages and libraries beforehand if they are "
"not already present on your system. You can refer to the Ubuntu-based "
"installation commands in [the Dockerfile](https://github.com/sgl-project/"
"sglang/blob/main/docker/xeon.Dockerfile#L11) for guidance."
msgstr ""

#: ../../../platforms/cpu_server.md:66
msgid ""
"Install `uv` package manager, then create and activate a virtual environment:"
msgstr ""

#: ../../../platforms/cpu_server.md:68
msgid ""
"# Taking '/opt' as the example uv env folder, feel free to change it as "
"needed\n"
"cd /opt\n"
"curl -LsSf https://astral.sh/uv/install.sh | sh\n"
"source $HOME/.local/bin/env\n"
"uv venv --python 3.12\n"
"source .venv/bin/activate\n"
msgstr ""

#: ../../../platforms/cpu_server.md:77
msgid ""
"Create a config file to direct the installation channel  (a.k.a. index-url) "
"of `torch` related packages:"
msgstr ""

#: ../../../platforms/cpu_server.md:80
msgid "vim .venv/uv.toml\n"
msgstr ""

#: ../../../platforms/cpu_server.md:84
msgid ""
"Press 'a' to enter insert mode of `vim`, paste the following content into "
"the created file"
msgstr ""

#: ../../../platforms/cpu_server.md:86
msgid ""
"[[index]]\n"
"name = \"torch\"\n"
"url = \"https://download.pytorch.org/whl/cpu\"\n"
"\n"
"[[index]]\n"
"name = \"torchvision\"\n"
"url = \"https://download.pytorch.org/whl/cpu\"\n"
"\n"
"[[index]]\n"
"name = \"triton\"\n"
"url = \"https://download.pytorch.org/whl/cpu\"\n"
"\n"
msgstr ""

#: ../../../platforms/cpu_server.md:101
msgid ""
"Save the file (in `vim`, press 'esc' to exit insert mode, then ':x+Enter'), "
"and set it as the default `uv` config."
msgstr ""

#: ../../../platforms/cpu_server.md:104
msgid "export UV_CONFIG_FILE=/opt/.venv/uv.toml\n"
msgstr ""

#: ../../../platforms/cpu_server.md:108
msgid "Clone the `sglang` source code and build the packages"
msgstr ""

#: ../../../platforms/cpu_server.md:110
msgid ""
"# Clone the SGLang code\n"
"git clone https://github.com/sgl-project/sglang.git\n"
"cd sglang\n"
"git checkout <YOUR-DESIRED-VERSION>\n"
"\n"
"# Use dedicated toml file\n"
"cd python\n"
"cp pyproject_cpu.toml pyproject.toml\n"
"# Install SGLang dependent libs, and build SGLang main package\n"
"uv pip install --upgrade pip setuptools\n"
"uv pip install .\n"
"uv pip install torch==2.9.0 torchvision==0.24.0 triton==3.5.0 --force-"
"reinstall\n"
"\n"
"# Build the CPU backend kernels\n"
"cd ../sgl-kernel\n"
"cp pyproject_cpu.toml pyproject.toml\n"
"uv pip install .\n"
msgstr ""

#: ../../../platforms/cpu_server.md:130
msgid "Set the required environment variables"
msgstr ""

#: ../../../platforms/cpu_server.md:132
msgid ""
"export SGLANG_USE_CPU_ENGINE=1\n"
"\n"
"# Set 'LD_LIBRARY_PATH' and 'LD_PRELOAD' to ensure the libs can be loaded by "
"sglang processes\n"
"export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu\n"
"export LD_PRELOAD=${LD_PRELOAD}:/opt/.venv/lib/libiomp5.so:"
"${LD_LIBRARY_PATH}/libtcmalloc.so.4:${LD_LIBRARY_PATH}/libtbbmalloc.so.2\n"
msgstr ""

#: ../../../platforms/cpu_server.md:140 ../../../platforms/cpu_server.md:179
msgid "Notes:"
msgstr ""

#: ../../../platforms/cpu_server.md:142
msgid ""
"Note that the environment variable `SGLANG_USE_CPU_ENGINE=1`   is required "
"to enable the SGLang service with the CPU engine."
msgstr ""

#: ../../../platforms/cpu_server.md:145
msgid ""
"If you encounter code compilation issues during the `sgl-kernel` building "
"process,   please check your `gcc` and `g++` versions and upgrade them if "
"they are outdated.   It is recommended to use `gcc-13` and `g++-13` as they "
"have been verified   in the official Docker container."
msgstr ""

#: ../../../platforms/cpu_server.md:150
msgid ""
"The system library path is typically located in one of the following "
"directories:   `~/.local/lib/`, `/usr/local/lib/`, `/usr/local/lib64/`, `/"
"usr/lib/`, `/usr/lib64/`   and `/usr/lib/x86_64-linux-gnu/`. In the above "
"example commands, `/usr/lib/x86_64-linux-gnu`   is used. Please adjust the "
"path according to your server configuration."
msgstr ""

#: ../../../platforms/cpu_server.md:155
msgid ""
"It is recommended to add the following to your `~/.bashrc` file to   avoid "
"setting these variables every time you open a new terminal:"
msgstr ""

#: ../../../platforms/cpu_server.md:158
msgid ""
"source .venv/bin/activate\n"
"export SGLANG_USE_CPU_ENGINE=1\n"
"export LD_LIBRARY_PATH=<YOUR-SYSTEM-LIBRARY-FOLDER>\n"
"export LD_PRELOAD=<YOUR-LIBS-PATHS>\n"
msgstr ""

#: ../../../platforms/cpu_server.md:165
msgid "Launch of the Serving Engine"
msgstr ""

#: ../../../platforms/cpu_server.md:167
msgid "Example command to launch SGLang serving:"
msgstr ""

#: ../../../platforms/cpu_server.md:169
msgid ""
"python -m sglang.launch_server   \\\n"
"    --model <MODEL_ID_OR_PATH>   \\\n"
"    --trust-remote-code          \\\n"
"    --disable-overlap-schedule   \\\n"
"    --device cpu                 \\\n"
"    --host 0.0.0.0               \\\n"
"    --tp 6\n"
msgstr ""

#: ../../../platforms/cpu_server.md:181
msgid ""
"For running W8A8 quantized models, please add the flag `--quantization "
"w8a8_int8`."
msgstr ""

#: ../../../platforms/cpu_server.md:183
msgid ""
"The flag `--tp 6` specifies that tensor parallelism will be applied using 6 "
"ranks (TP6).  The number of TP specified is how many TP ranks will be used "
"during the execution.  On a CPU platform, a TP rank means a sub-NUMA cluster "
"(SNC).  Usually we can get the SNC information (How many available) from the "
"Operating System.  Users can specify TP to be no more than the total "
"available SNCs in current system."
msgstr ""

#: ../../../platforms/cpu_server.md:189
msgid ""
"If the specified TP rank number differs from the total SNC count,  the "
"system will automatically utilize the first `n` SNCs.  Note that `n` cannot "
"exceed the total SNC number, doing so will result in an error."
msgstr ""

#: ../../../platforms/cpu_server.md:193
msgid ""
"To specify the cores to be used, we need to explicitly set the environment "
"variable `SGLANG_CPU_OMP_THREADS_BIND`.  For example, if we want to run the "
"SGLang service using the first 40 cores of each SNC on a Xeon® 6980P "
"server,  which has 43-43-42 cores on the 3 SNCs of a socket, we should set:"
msgstr ""

#: ../../../platforms/cpu_server.md:197
msgid ""
"export SGLANG_CPU_OMP_THREADS_BIND=\"0-39|43-82|86-125|128-167|171-210|"
"214-253\"\n"
msgstr ""

#: ../../../platforms/cpu_server.md:201
msgid ""
"Please beware that with SGLANG_CPU_OMP_THREADS_BIND set,  the available "
"memory amounts of the ranks may not be determined in prior.  You may need to "
"set proper `--max-total-tokens` to avoid the out-of-memory error."
msgstr ""

#: ../../../platforms/cpu_server.md:205
msgid ""
"For optimizing decoding with torch.compile, please add the flag `--enable-"
"torch-compile`.  To specify the maximum batch size when using `torch."
"compile`, set the flag `--torch-compile-max-bs`.  For example, `--enable-"
"torch-compile --torch-compile-max-bs 4` means using `torch.compile`  and "
"setting the maximum batch size to 4. Currently the maximum applicable batch "
"size  for optimizing with `torch.compile` is 16."
msgstr ""

#: ../../../platforms/cpu_server.md:211
msgid ""
"A warmup step is automatically triggered when the service is started.  The "
"server is ready when you see the log `The server is fired up and ready to "
"roll!`."
msgstr ""

#: ../../../platforms/cpu_server.md:214
msgid "Benchmarking with Requests"
msgstr ""

#: ../../../platforms/cpu_server.md:216
msgid ""
"You can benchmark the performance via the `bench_serving` script. Run the "
"command in another terminal. An example command would be:"
msgstr ""

#: ../../../platforms/cpu_server.md:219
msgid ""
"python -m sglang.bench_serving   \\\n"
"    --dataset-name random        \\\n"
"    --random-input-len 1024      \\\n"
"    --random-output-len 1024     \\\n"
"    --num-prompts 1              \\\n"
"    --request-rate inf           \\\n"
"    --random-range-ratio 1.0\n"
msgstr ""

#: ../../../platforms/cpu_server.md:229
msgid "Detailed parameter descriptions are available via the command:"
msgstr ""

#: ../../../platforms/cpu_server.md:231
msgid "python -m sglang.bench_serving -h\n"
msgstr ""

#: ../../../platforms/cpu_server.md:235
msgid ""
"Additionally, requests can be formatted using [the OpenAI Completions API]"
"(https://docs.sglang.io/basic_usage/openai_api_completions.html) and sent "
"via the command line (e.g., using `curl`) or through your own scripts."
msgstr ""

#: ../../../platforms/cpu_server.md:239
msgid "Example Usage Commands"
msgstr ""

#: ../../../platforms/cpu_server.md:241
msgid ""
"Large Language Models can range from fewer than 1 billion to several hundred "
"billion parameters. Dense models larger than 20B are expected to run on "
"flagship 6th Gen Intel® Xeon® processors with dual sockets and a total of 6 "
"sub-NUMA clusters. Dense models of approximately 10B parameters or fewer, or "
"MoE (Mixture of Experts) models with fewer than 10B activated parameters, "
"can run on more common 4th generation or newer Intel® Xeon® processors, or "
"utilize a single socket of the flagship 6th Gen Intel® Xeon® processors."
msgstr ""

#: ../../../platforms/cpu_server.md:247
msgid "Example: Running DeepSeek-V3.1-Terminus"
msgstr ""

#: ../../../platforms/cpu_server.md:249
msgid ""
"An example command to launch service of W8A8_INT8 DeepSeek-V3.1-Terminus on "
"a Xeon® 6980P server:"
msgstr ""

#: ../../../platforms/cpu_server.md:251
msgid ""
"python -m sglang.launch_server                                 \\\n"
"    --model IntervitensInc/DeepSeek-V3.1-Terminus-Channel-int8 \\\n"
"    --trust-remote-code                                        \\\n"
"    --disable-overlap-schedule                                 \\\n"
"    --device cpu                                               \\\n"
"    --quantization w8a8_int8                                   \\\n"
"    --host 0.0.0.0                                             \\\n"
"    --enable-torch-compile                                     \\\n"
"    --torch-compile-max-bs 4                                   \\\n"
"    --tp 6\n"
msgstr ""

#: ../../../platforms/cpu_server.md:264
msgid ""
"Similarly, an example command to launch service of FP8 DeepSeek-V3.1-"
"Terminus would be:"
msgstr ""

#: ../../../platforms/cpu_server.md:266
msgid ""
"python -m sglang.launch_server                     \\\n"
"    --model deepseek-ai/DeepSeek-V3.1-Terminus     \\\n"
"    --trust-remote-code                            \\\n"
"    --disable-overlap-schedule                     \\\n"
"    --device cpu                                   \\\n"
"    --host 0.0.0.0                                 \\\n"
"    --enable-torch-compile                         \\\n"
"    --torch-compile-max-bs 4                       \\\n"
"    --tp 6\n"
msgstr ""

#: ../../../platforms/cpu_server.md:278
msgid ""
"Note: Please set `--torch-compile-max-bs` to the maximum desired batch size "
"for your deployment, which can be up to 16. The value `4` in the examples is "
"illustrative."
msgstr ""

#: ../../../platforms/cpu_server.md:281
msgid "Example: Running Llama-3.2-3B"
msgstr ""

#: ../../../platforms/cpu_server.md:283
msgid ""
"An example command to launch service of Llama-3.2-3B with BF16 precision:"
msgstr ""

#: ../../../platforms/cpu_server.md:285
msgid ""
"python -m sglang.launch_server                     \\\n"
"    --model meta-llama/Llama-3.2-3B-Instruct       \\\n"
"    --trust-remote-code                            \\\n"
"    --disable-overlap-schedule                     \\\n"
"    --device cpu                                   \\\n"
"    --host 0.0.0.0                                 \\\n"
"    --enable-torch-compile                         \\\n"
"    --torch-compile-max-bs 16                      \\\n"
"    --tp 2\n"
msgstr ""

#: ../../../platforms/cpu_server.md:297
msgid ""
"The example command to launch service of W8A8_INT8 version of Llama-3.2-3B:"
msgstr ""

#: ../../../platforms/cpu_server.md:299
msgid ""
"python -m sglang.launch_server                     \\\n"
"    --model RedHatAI/Llama-3.2-3B-quantized.w8a8   \\\n"
"    --trust-remote-code                            \\\n"
"    --disable-overlap-schedule                     \\\n"
"    --device cpu                                   \\\n"
"    --quantization w8a8_int8                       \\\n"
"    --host 0.0.0.0                                 \\\n"
"    --enable-torch-compile                         \\\n"
"    --torch-compile-max-bs 16                      \\\n"
"    --tp 2\n"
msgstr ""

#: ../../../platforms/cpu_server.md:312
msgid ""
"Note: The `--torch-compile-max-bs` and `--tp` settings are examples that "
"should be adjusted for your setup. For instance, use `--tp 3` to utilize 1 "
"socket with 3 sub-NUMA clusters on an Intel® Xeon® 6980P server."
msgstr ""

#: ../../../platforms/cpu_server.md:315
msgid ""
"Once the server have been launched, you can test it using the "
"`bench_serving` command or create your own commands or scripts following "
"[the benchmarking example](#benchmarking-with-requests)."
msgstr ""
