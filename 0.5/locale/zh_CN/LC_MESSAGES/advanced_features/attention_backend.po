# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-02-10 09:10+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../advanced_features/attention_backend.md:1
msgid "Attention Backend"
msgstr ""

#: ../../../advanced_features/attention_backend.md:3
msgid ""
"SGLang supports a large variety of attention backends. Each of them has "
"different pros and cons. You can test them according to your needs."
msgstr ""

#: ../../../advanced_features/attention_backend.md:7
msgid ""
"Selecting an optimal attention backend is crucial for maximizing your "
"performance. Different backends excel in various scenarios, so choose based "
"on your model, hardware, and use case. Not all backends are supported on all "
"platforms and model architectures."
msgstr ""

#: ../../../advanced_features/attention_backend.md:9
msgid ""
"If you don't specify `--attention-backend`, SGLang makes a best effort to "
"automatically select the most performant backend based on your hardware and "
"model architecture."
msgstr ""

#: ../../../advanced_features/attention_backend.md:12
msgid "Support Matrix"
msgstr ""

#: ../../../advanced_features/attention_backend.md:14
msgid ""
"The support matrix is split into two parts: MHA (standard attention) and MLA "
"(multi-head latent attention). For an explanation of the key differences "
"between MHA and MLA, please see the [SGLang documentation on DeepSeek MLA]"
"(../basic_usage/deepseek_v3.md#multi-head-latent-attention-mla-throughput-"
"optimizations) and the original [DeepSeek MLA paper](https://arxiv.org/"
"pdf/2405.04434)."
msgstr ""

#: ../../../advanced_features/attention_backend.md:16
msgid "MHA Backends"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Backend**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Page Size > 1 (native)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FP8 KV Cache**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FP4 KV Cache**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Spec topk=1**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Spec topk>1**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Sliding Window**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**MultiModal**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FlashInfer**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "✅"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "❌"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FA3 (FlashAttention 3)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FA4 (FlashAttention 4)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "128"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Triton**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Torch Native (SDPA)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FlexAttention (PyTorch)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**TRTLLM MHA**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "16, 32 or 64"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Dual Chunk FlashAttention**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**AITER (ROCm)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Wave (ROCm)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Ascend (NPU)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Intel XPU**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Intel AMX (CPU)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:34
msgid "MLA Backends"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Native Page Sizes**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Chunked Prefix Cache**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FlashInfer MLA**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "1"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FlashMLA**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "64"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Cutlass MLA**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**TRTLLM MLA (Blackwell)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "32 or 64"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "n/a"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "⚠️ (page_size=1 only)"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FA4**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Ascend MLA (NPU)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:48
msgid ""
"Multimodal attention is selected by `--mm-attention-backend`. The "
"\"MultiModal\" column indicates whether a corresponding multimodal "
"implementation exists for that backend family."
msgstr ""

#: ../../../advanced_features/attention_backend.md:52
msgid "FlashAttention 4 is prefill-only for now."
msgstr ""

#: ../../../advanced_features/attention_backend.md:53
msgid ""
"NSA is specifically designed for [DeepSeek V3.2 DSA](https://lmsys.org/"
"blog/2025-09-29-deepseek-V32/)."
msgstr ""

#: ../../../advanced_features/attention_backend.md:57
msgid ""
"For the KV4 FA4 scenario, FA4 requires using a different --decode-attention-"
"backend to run. Except for trtllm_mha being incompatible with FA4, all other "
"decode backends behave as shown in the table."
msgstr ""

#: ../../../advanced_features/attention_backend.md:61
msgid ""
"Speculative decoding topk: `topk` is the number of draft tokens sampled per "
"step from the draft model. `topk = 1` follows classic EAGLE; `topk > 1` "
"explores multiple branches and requires backend support in both draft and "
"verification paths."
msgstr ""

#: ../../../advanced_features/attention_backend.md:65
msgid ""
"Page size controls how many tokens are grouped into a KV cache block. For "
"the prefix cache to take effect, the number of tokens must fill at least one "
"complete page. For example, if your prompt is only 32 tokens and `page_size "
"= 64`, it won't fill a complete page and cannot be matched in the prefix "
"cache (pages cannot be padded). With 65 tokens and `page_size = 64`, only "
"the first page of 64 tokens will be cached and matched; the remaining 1 "
"token is discarded. Use `page_size = 1` for maximum prefix reuse (token-"
"level matching)."
msgstr ""

#: ../../../advanced_features/attention_backend.md:68
msgid ""
"Many backends that do not natively operate on pages can emulate `page_size > "
"1` at the wrapper layer by expanding page tables to per-token indices. The "
"\"Page Size > 1 (native)\" column indicates true in-kernel paging. Some "
"backends require fixed native page sizes and cannot be reduced/emulated "
"differently: TRTLLM MHA (16/32/64), TRTLLM MLA (32/64), FlashMLA (64), "
"Cutlass MLA (128), Ascend (128)."
msgstr ""

#: ../../../advanced_features/attention_backend.md:70
msgid "MLA page-size constraints:"
msgstr ""

#: ../../../advanced_features/attention_backend.md:71
msgid "FlashInfer MLA: page_size = 1."
msgstr ""

#: ../../../advanced_features/attention_backend.md:72
msgid "FlashMLA: page_size = 64."
msgstr ""

#: ../../../advanced_features/attention_backend.md:73
msgid "Cutlass MLA: page_size = 128."
msgstr ""

#: ../../../advanced_features/attention_backend.md:74
msgid "TRTLLM MLA: page_size ∈ {32, 64}."
msgstr ""

#: ../../../advanced_features/attention_backend.md:76
msgid ""
"Hybrid attention (different backends for prefill vs decode) (Experimental)"
msgstr ""

#: ../../../advanced_features/attention_backend.md:79
msgid "Hybrid attention is an experimental feature."
msgstr ""

#: ../../../advanced_features/attention_backend.md:82
msgid ""
"You can mix-and-match attention backends for prefill and decode. This is "
"useful when one backend excels at prefill and another excels at decode. For "
"the implementation details, please see `python/sglang/srt/layers/attention/"
"hybrid_attn_backend.py`."
msgstr ""

#: ../../../advanced_features/attention_backend.md:84
msgid ""
"# Example: Prefill with FA4, Decode with TRTLLM MLA (Blackwell)\n"
"python3 -m sglang.launch_server \\\n"
"  --model-path nvidia/DeepSeek-R1-FP4 \\\n"
"  --tp 8 \\\n"
"  --attention-backend trtllm_mla \\\n"
"  --moe-runner-backend flashinfer_trtllm \\\n"
"  --quantization modelopt_fp4 \\\n"
"  --prefill-attention-backend fa4\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:95
msgid "Speculative decoding with hybrid attention"
msgstr ""

#: ../../../advanced_features/attention_backend.md:97
msgid ""
"Hybrid attention also works with speculative decoding. The backend used for "
"draft decoding and target verification depends on `--speculative-attention-"
"mode`:"
msgstr ""

#: ../../../advanced_features/attention_backend.md:99
msgid ""
"`--speculative-attention-mode decode` (recommended): draft/verify use the "
"decode backend."
msgstr ""

#: ../../../advanced_features/attention_backend.md:100
msgid ""
"`--speculative-attention-mode prefill` (default): draft/verify use the "
"prefill backend."
msgstr ""

#: ../../../advanced_features/attention_backend.md:102
msgid "Constraints when combining hybrid attention with speculative decoding:"
msgstr ""

#: ../../../advanced_features/attention_backend.md:104
msgid ""
"If any attention backend is `trtllm_mha`, speculative decoding supports only "
"`--speculative-eagle-topk 1`."
msgstr ""

#: ../../../advanced_features/attention_backend.md:105
msgid ""
"For paged MHA backends with `--page-size > 1` and `--speculative-eagle-topk "
"> 1`, only `flashinfer` is supported."
msgstr ""

#: ../../../advanced_features/attention_backend.md:106
msgid ""
"CUDA Graph: the decode backend is always captured; the prefill backend is "
"captured only when `--speculative-attention-mode prefill`."
msgstr ""

#: ../../../advanced_features/attention_backend.md:110
msgid ""
"If you set only one of `--prefill-attention-backend` or `--decode-attention-"
"backend`, the unspecified phase inherits `--attention-backend`. If both are "
"specified and differ, SGLang automatically enables a hybrid wrapper to "
"dispatch to the chosen backend per phase."
msgstr ""

#: ../../../advanced_features/attention_backend.md:114
msgid "Attention Backend Selection Guide (CUDA)"
msgstr ""

#: ../../../advanced_features/attention_backend.md:116
msgid ""
"If the `--attention-backend` argument is not specified, SGLang automatically "
"selects the best backend based on the hardware (CUDA) and model architecture."
msgstr ""

#: ../../../advanced_features/attention_backend.md:118
msgid "Automatic Selection Logic"
msgstr ""

#: ../../../advanced_features/attention_backend.md:120
msgid "**1. MHA Models (e.g., Llama, Qwen)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:121
msgid ""
"**Hopper (e.g., H100, H200)**: Defaults to `fa3` if using CUDA 12.3+ and the "
"model configuration is supported."
msgstr ""

#: ../../../advanced_features/attention_backend.md:122
msgid ""
"**Blackwell (e.g., B200)**: Defaults to `trtllm_mha`, unless using "
"speculative decoding with `topk > 1`."
msgstr ""

#: ../../../advanced_features/attention_backend.md:123
msgid ""
"**Other Architectures (Ampere, Ada, etc.)**: Defaults to `flashinfer` if "
"available; otherwise falls back to `triton`."
msgstr ""

#: ../../../advanced_features/attention_backend.md:125
msgid "**2. MLA Models (e.g., DeepSeek V3)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:126
msgid "**Hopper**: Defaults to `fa3` (requires CUDA 12.3+)."
msgstr ""

#: ../../../advanced_features/attention_backend.md:127
msgid "**Blackwell**: Defaults to `trtllm_mla`."
msgstr ""

#: ../../../advanced_features/attention_backend.md:128
msgid "**Other Architectures**: Defaults to `triton`."
msgstr ""

#: ../../../advanced_features/attention_backend.md:131
msgid "User Guide"
msgstr ""

#: ../../../advanced_features/attention_backend.md:133
msgid "Launch Command for Different Attention Backends"
msgstr ""

#: ../../../advanced_features/attention_backend.md:135
msgid "FlashInfer (Default for Non-Hopper Machines, e.g., A100, A40)"
msgstr ""

#: ../../../advanced_features/attention_backend.md:136
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend flashinfer\n"
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-V3 \\\n"
"  --attention-backend flashinfer \\\n"
"  --trust-remote-code\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:147
msgid "FlashAttention 3 (Default for Hopper Machines, e.g., H100, H200, H20)"
msgstr ""

#: ../../../advanced_features/attention_backend.md:148
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend fa3\n"
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-V3 \\\n"
"  --trust-remote-code \\\n"
"  --attention-backend fa3\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:159
msgid "Triton"
msgstr ""

#: ../../../advanced_features/attention_backend.md:160
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend triton\n"
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-V3 \\\n"
"  --attention-backend triton \\\n"
"  --trust-remote-code\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:171
msgid "FlashMLA"
msgstr ""

#: ../../../advanced_features/attention_backend.md:172
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-R1 \\\n"
"  --attention-backend flashmla \\\n"
"  --trust-remote-code\n"
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-R1 \\\n"
"  --attention-backend flashmla \\\n"
"  --kv-cache-dtype fp8_e4m3 \\\n"
"  --trust-remote-code\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:186
msgid "TRTLLM MLA (Optimized for Blackwell Architecture, e.g., B200)"
msgstr ""

#: ../../../advanced_features/attention_backend.md:187
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-R1 \\\n"
"  --attention-backend trtllm_mla \\\n"
"  --trust-remote-code\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:195
msgid ""
"TRTLLM MLA with FP8 KV Cache (Higher concurrency, lower memory footprint)"
msgstr ""

#: ../../../advanced_features/attention_backend.md:196
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-R1 \\\n"
"  --attention-backend trtllm_mla \\\n"
"  --kv-cache-dtype fp8_e4m3 \\\n"
"  --trust-remote-code\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:205
msgid "FlashAttention 4 (MHA & MLA)"
msgstr ""

#: ../../../advanced_features/attention_backend.md:206
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-R1 \\\n"
"  --prefill-attention-backend fa4 \\\n"
"  --trust-remote-code\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:214
msgid "Cutlass MLA"
msgstr ""

#: ../../../advanced_features/attention_backend.md:215
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-R1 \\\n"
"  --attention-backend cutlass_mla \\\n"
"  --trust-remote-code\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:223
msgid "Ascend"
msgstr ""

#: ../../../advanced_features/attention_backend.md:224
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend ascend\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:230
msgid "Intel XPU"
msgstr ""

#: ../../../advanced_features/attention_backend.md:231
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend intel_xpu\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:237
msgid "Wave"
msgstr ""

#: ../../../advanced_features/attention_backend.md:238
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend wave\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:244
msgid "FlexAttention"
msgstr ""

#: ../../../advanced_features/attention_backend.md:245
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend flex_attention\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:251
msgid "Dual Chunk FlashAttention"
msgstr ""

#: ../../../advanced_features/attention_backend.md:252
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model Qwen/Qwen2.5-14B-Instruct-1M \\\n"
"  --attention-backend dual_chunk_flash_attn\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:258
msgid "Torch Native"
msgstr ""

#: ../../../advanced_features/attention_backend.md:259
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend torch_native\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:265
msgid "Steps to add a new attention backend"
msgstr ""

#: ../../../advanced_features/attention_backend.md:266
msgid ""
"To add a new attention backend, you can learn from the existing backends "
"(`python/sglang/srt/layers/attention/triton_backend.py`, `python/sglang/srt/"
"layers/attention/flashattention_backend.py`) and follow the steps below."
msgstr ""

#: ../../../advanced_features/attention_backend.md:270
msgid "Run without cuda graph. Support the two forward functions"
msgstr ""

#: ../../../advanced_features/attention_backend.md:271
msgid "forward_extend"
msgstr ""

#: ../../../advanced_features/attention_backend.md:272
msgid ""
"Will be used for prefill, prefill with KV cache, and target verification"
msgstr ""

#: ../../../advanced_features/attention_backend.md:273
#: ../../../advanced_features/attention_backend.md:276
msgid "It will be called once per layer"
msgstr ""

#: ../../../advanced_features/attention_backend.md:274
msgid "forward_decode"
msgstr ""

#: ../../../advanced_features/attention_backend.md:275
msgid "Will be used for normal decode, and draft decode"
msgstr ""

#: ../../../advanced_features/attention_backend.md:277
msgid "init_forward_metadata"
msgstr ""

#: ../../../advanced_features/attention_backend.md:278
msgid "Initialize the class and common metadata shared by all layers"
msgstr ""

#: ../../../advanced_features/attention_backend.md:279
msgid "Call the plan function for optimizations like split_kv"
msgstr ""

#: ../../../advanced_features/attention_backend.md:280
msgid "It will be called once per forward"
msgstr ""

#: ../../../advanced_features/attention_backend.md:281
msgid ""
"Run with cuda graph. It has two phases (capture and replay) and you need to "
"implement three functions"
msgstr ""

#: ../../../advanced_features/attention_backend.md:282
msgid "init_cuda_graph_state"
msgstr ""

#: ../../../advanced_features/attention_backend.md:283
msgid "It will be called once during life time"
msgstr ""

#: ../../../advanced_features/attention_backend.md:284
msgid "Create all common shared buffers"
msgstr ""

#: ../../../advanced_features/attention_backend.md:285
msgid "init_forward_metadata_capture_cuda_graph"
msgstr ""

#: ../../../advanced_features/attention_backend.md:286
msgid "It will be called before capturing a cuda graph"
msgstr ""

#: ../../../advanced_features/attention_backend.md:287
msgid ""
"It is similar to init_forward_metadata but write the medatada to some pre-"
"defined buffers"
msgstr ""

#: ../../../advanced_features/attention_backend.md:288
msgid "init_forward_metadata_replay_cuda_graph"
msgstr ""

#: ../../../advanced_features/attention_backend.md:289
msgid "It will be called before replaying a cuda graph"
msgstr ""

#: ../../../advanced_features/attention_backend.md:290
msgid "This function is in the critical path and needs to be fast"
msgstr ""
