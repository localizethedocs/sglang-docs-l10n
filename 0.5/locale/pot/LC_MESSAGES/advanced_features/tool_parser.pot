# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-02-10 09:10+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../advanced_features/tool_parser.ipynb:2
msgid ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:9
msgid "Tool Parser"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:11
msgid ""
"This guide demonstrates how to use SGLang’s `Function calling <https://"
"platform.openai.com/docs/guides/function-calling>`__ functionality."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:23
msgid "Currently supported parsers:"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:26
msgid "Parser"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:26
msgid "Supported Models"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:26
msgid "Notes"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:28
msgid "``deepseekv3``"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:28
msgid "DeepSeek-v3 (e.g., ``deepseek-ai/DeepSeek-V3-0324``)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:28
msgid ""
"Recommend adding ``--chat-template ./examples/chat_template/"
"tool_chat_template_deepseekv3.jinja`` to launch command."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:30
msgid "``deepseekv31``"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:30
msgid ""
"DeepSeek-V3.1 and DeepSeek-V3.2-Exp (e.g. ``deepseek-ai/DeepSeek-V3.1``, "
"``deepseek-ai/DeepSeek-V3.2-Exp``)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:30
msgid ""
"Recommend adding ``--chat-template ./examples/chat_template/"
"tool_chat_template_deepseekv31.jinja`` (Or ..deepseekv32.jinja for DeepSeek-"
"V3.2) to launch command."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:32
msgid "``deepseekv32``"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:32
msgid "DeepSeek-V3.2 (``deepseek-ai/DeepSeek-V3.2``)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:34
msgid "``glm``"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:34
msgid "GLM series (e.g. ``zai-org/GLM-4.6``)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:36
msgid "``gpt-oss``"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:36
msgid ""
"GPT-OSS (e.g., ``openai/gpt-oss-120b``, ``openai/gpt-oss-20b``, ``lmsys/gpt-"
"oss-120b-bf16``, ``lmsys/gpt-oss-20b-bf16``)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:36
msgid ""
"The gpt-oss tool parser filters out analysis channel events and only "
"preserves normal text. This can cause the content to be empty when "
"explanations are in the analysis channel. To work around this, complete the "
"tool round by returning tool results as ``role=\"tool\"`` messages, which "
"enables the model to generate the final content."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:40
msgid "``kimi_k2``"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:40
msgid "``moonshotai/Kimi-K2-Instruct``"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:42
msgid "``llama3``"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:42
msgid ""
"Llama 3.1 / 3.2 / 3.3 (e.g. ``meta-llama/Llama-3.1-8B-Instruct``, ``meta-"
"llama/Llama-3.2-1B-Instruct``, ``meta-llama/Llama-3.3-70B-Instruct``)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:44
msgid "``llama4``"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:44
msgid "Llama 4 (e.g. ``meta-llama/Llama-4-Scout-17B-16E-Instruct``)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:46
msgid "``mistral``"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:46
msgid ""
"Mistral (e.g. ``mistralai/Mistral-7B-Instruct-v0.3``, ``mistralai/Mistral-"
"Nemo-Instruct-2407``, ``mistralai/Mistral-7B-v0.3``)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:48
msgid "``pythonic``"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:48
msgid "Llama-3.2 / Llama-3.3 / Llama-4"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:48
msgid ""
"Model outputs function calls as Python code. Requires ``--tool-call-parser "
"pythonic`` and is recommended to use with a specific chat template."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:50
msgid "``qwen``"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:50
msgid ""
"Qwen series (e.g. ``Qwen/Qwen3-Next-80B-A3B-Instruct``, ``Qwen/Qwen3-VL-30B-"
"A3B-Thinking``) except Qwen3-Coder"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:52
msgid "``qwen3_coder``"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:52
msgid "Qwen3-Coder (e.g. ``Qwen/Qwen3-Coder-30B-A3B-Instruct``)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:54
msgid "``step3``"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:54
msgid "Step-3"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:67
msgid "OpenAI Compatible API"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:79
msgid "Launching the Server"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid "[ ]:"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"import json\n"
"from sglang.test.doc_patch import launch_server_cmd\n"
"from sglang.utils import wait_for_server, print_highlight, "
"terminate_process\n"
"from openai import OpenAI\n"
"\n"
"server_process, port = launch_server_cmd(\n"
"    \"python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-7B-Instruct "
"--tool-call-parser qwen25 --host 0.0.0.0 --log-level warning\"  # qwen25\n"
")\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:107
msgid ""
"Note that ``--tool-call-parser`` defines the parser used to interpret "
"responses."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:119
msgid "Define Tools for Function Call"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:121
msgid ""
"Below is a Python snippet that shows how to define a tool as a dictionary. "
"The dictionary includes a tool name, a description, and property defined "
"Parameters."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"# Define tools\n"
"tools = [\n"
"    {\n"
"        \"type\": \"function\",\n"
"        \"function\": {\n"
"            \"name\": \"get_current_weather\",\n"
"            \"description\": \"Get the current weather in a given "
"location\",\n"
"            \"parameters\": {\n"
"                \"type\": \"object\",\n"
"                \"properties\": {\n"
"                    \"city\": {\n"
"                        \"type\": \"string\",\n"
"                        \"description\": \"The city to find the weather for, "
"e.g. 'San Francisco'\",\n"
"                    },\n"
"                    \"state\": {\n"
"                        \"type\": \"string\",\n"
"                        \"description\": \"the two-letter abbreviation for "
"the state that the city is\"\n"
"                        \" in, e.g. 'CA' which would mean 'California'\",\n"
"                    },\n"
"                    \"unit\": {\n"
"                        \"type\": \"string\",\n"
"                        \"description\": \"The unit to fetch the temperature "
"in\",\n"
"                        \"enum\": [\"celsius\", \"fahrenheit\"],\n"
"                    },\n"
"                },\n"
"                \"required\": [\"city\", \"state\", \"unit\"],\n"
"            },\n"
"        },\n"
"    }\n"
"]"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:171
msgid "Define Messages"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"def get_messages():\n"
"    return [\n"
"        {\n"
"            \"role\": \"user\",\n"
"            \"content\": \"What's the weather like in Boston today? Output a "
"reasoning before act, then use the tools to help you.\",\n"
"        }\n"
"    ]\n"
"\n"
"\n"
"messages = get_messages()"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:201
msgid "Initialize the Client"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"# Initialize OpenAI-like client\n"
"client = OpenAI(api_key=\"None\", base_url=f\"http://0.0.0.0:{port}/v1\")\n"
"model_name = client.models.list().data[0].id"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:224
msgid "Non-Streaming Request"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"# Non-streaming mode test\n"
"response_non_stream = client.chat.completions.create(\n"
"    model=model_name,\n"
"    messages=messages,\n"
"    temperature=0,\n"
"    top_p=0.95,\n"
"    max_tokens=1024,\n"
"    stream=False,  # Non-streaming\n"
"    tools=tools,\n"
")\n"
"print_highlight(\"Non-stream response:\")\n"
"print_highlight(response_non_stream)\n"
"print_highlight(\"==== content ====\")\n"
"print_highlight(response_non_stream.choices[0].message.content)\n"
"print_highlight(\"==== tool_calls ====\")\n"
"print_highlight(response_non_stream.choices[0].message.tool_calls)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:260
#: ../../../advanced_features/tool_parser.ipynb:336
msgid "Handle Tools"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:262
#: ../../../advanced_features/tool_parser.ipynb:338
msgid ""
"When the engine determines it should call a particular tool, it will return "
"arguments or partial arguments through the response. You can parse these "
"arguments and later invoke the tool accordingly."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"name_non_stream = response_non_stream.choices[0].message.tool_calls[0]."
"function.name\n"
"arguments_non_stream = (\n"
"    response_non_stream.choices[0].message.tool_calls[0].function.arguments\n"
")\n"
"\n"
"print_highlight(f\"Final streamed function call name: {name_non_stream}\")\n"
"print_highlight(f\"Final streamed function call arguments: "
"{arguments_non_stream}\")"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:289
msgid "Streaming Request"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"# Streaming mode test\n"
"print_highlight(\"Streaming response:\")\n"
"response_stream = client.chat.completions.create(\n"
"    model=model_name,\n"
"    messages=messages,\n"
"    temperature=0,\n"
"    top_p=0.95,\n"
"    max_tokens=1024,\n"
"    stream=True,  # Enable streaming\n"
"    tools=tools,\n"
")\n"
"\n"
"texts = \"\"\n"
"tool_calls = []\n"
"name = \"\"\n"
"arguments = \"\"\n"
"for chunk in response_stream:\n"
"    if chunk.choices[0].delta.content:\n"
"        texts += chunk.choices[0].delta.content\n"
"    if chunk.choices[0].delta.tool_calls:\n"
"        tool_calls.append(chunk.choices[0].delta.tool_calls[0])\n"
"print_highlight(\"==== Text ====\")\n"
"print_highlight(texts)\n"
"\n"
"print_highlight(\"==== Tool Call ====\")\n"
"for tool_call in tool_calls:\n"
"    print_highlight(tool_call)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"# Parse and combine function call arguments\n"
"arguments = []\n"
"for tool_call in tool_calls:\n"
"    if tool_call.function.name:\n"
"        print_highlight(f\"Streamed function call name: {tool_call.function."
"name}\")\n"
"\n"
"    if tool_call.function.arguments:\n"
"        arguments.append(tool_call.function.arguments)\n"
"\n"
"# Combine all fragments into a single JSON string\n"
"full_arguments = \"\".join(arguments)\n"
"print_highlight(f\"streamed function call arguments: {full_arguments}\")"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:370
msgid "Define a Tool Function"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"# This is a demonstration, define real function according to your usage.\n"
"def get_current_weather(city: str, state: str, unit: \"str\"):\n"
"    return (\n"
"        f\"The weather in {city}, {state} is 85 degrees {unit}. It is \"\n"
"        \"partly cloudly, with highs in the 90's.\"\n"
"    )\n"
"\n"
"\n"
"available_tools = {\"get_current_weather\": get_current_weather}"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:399
msgid "Execute the Tool"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"messages.append(response_non_stream.choices[0].message)\n"
"\n"
"# Call the corresponding tool function\n"
"tool_call = messages[-1].tool_calls[0]\n"
"tool_name = tool_call.function.name\n"
"tool_to_call = available_tools[tool_name]\n"
"result = tool_to_call(**(json.loads(tool_call.function.arguments)))\n"
"print_highlight(f\"Function call result: {result}\")\n"
"# messages.append({\"role\": \"tool\", \"content\": result, \"name\": "
"tool_name})\n"
"messages.append(\n"
"    {\n"
"        \"role\": \"tool\",\n"
"        \"tool_call_id\": tool_call.id,\n"
"        \"content\": str(result),\n"
"        \"name\": tool_name,\n"
"    }\n"
")\n"
"\n"
"print_highlight(f\"Updated message history: {messages}\")"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:438
msgid "Send Results Back to Model"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"final_response = client.chat.completions.create(\n"
"    model=model_name,\n"
"    messages=messages,\n"
"    temperature=0,\n"
"    top_p=0.95,\n"
"    stream=False,\n"
"    tools=tools,\n"
")\n"
"print_highlight(\"Non-stream response:\")\n"
"print_highlight(final_response)\n"
"\n"
"print_highlight(\"==== Text ====\")\n"
"print_highlight(final_response.choices[0].message.content)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:471
msgid "Native API and SGLang Runtime (SRT)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"from transformers import AutoTokenizer\n"
"import requests\n"
"\n"
"# generate an answer\n"
"tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n"
"\n"
"messages = get_messages()\n"
"\n"
"input = tokenizer.apply_chat_template(\n"
"    messages, tokenize=False, add_generation_prompt=True, tools=tools, "
"return_dict=False\n"
")\n"
"\n"
"gen_url = f\"http://localhost:{port}/generate\"\n"
"gen_data = {\n"
"    \"text\": input,\n"
"    \"sampling_params\": {\n"
"        \"skip_special_tokens\": False,\n"
"        \"max_new_tokens\": 1024,\n"
"        \"temperature\": 0,\n"
"        \"top_p\": 0.95,\n"
"    },\n"
"}\n"
"gen_response = requests.post(gen_url, json=gen_data).json()[\"text\"]\n"
"print_highlight(\"==== Response ====\")\n"
"print_highlight(gen_response)\n"
"\n"
"# parse the response\n"
"parse_url = f\"http://localhost:{port}/parse_function_call\"\n"
"\n"
"function_call_input = {\n"
"    \"text\": gen_response,\n"
"    \"tool_call_parser\": \"qwen25\",\n"
"    \"tools\": tools,\n"
"}\n"
"\n"
"function_call_response = requests.post(parse_url, json=function_call_input)\n"
"function_call_response_json = function_call_response.json()\n"
"\n"
"print_highlight(\"==== Text ====\")\n"
"print(function_call_response_json[\"normal_text\"])\n"
"print_highlight(\"==== Calls ====\")\n"
"print(\"function name: \", function_call_response_json[\"calls\"][0]"
"[\"name\"])\n"
"print(\"function arguments: \", function_call_response_json[\"calls\"][0]"
"[\"parameters\"])"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid "terminate_process(server_process)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:543
msgid "Offline Engine API"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"import sglang as sgl\n"
"from sglang.srt.function_call.function_call_parser import "
"FunctionCallParser\n"
"from sglang.srt.managers.io_struct import Tool, Function\n"
"\n"
"llm = sgl.Engine(model_path=\"Qwen/Qwen2.5-7B-Instruct\")\n"
"tokenizer = llm.tokenizer_manager.tokenizer\n"
"input_ids = tokenizer.apply_chat_template(\n"
"    messages, tokenize=True, add_generation_prompt=True, tools=tools, "
"return_dict=False\n"
")\n"
"\n"
"# Note that for gpt-oss tool parser, adding \"no_stop_trim\": True\n"
"# to make sure the tool call token <call> is not trimmed.\n"
"\n"
"sampling_params = {\n"
"    \"max_new_tokens\": 1024,\n"
"    \"temperature\": 0,\n"
"    \"top_p\": 0.95,\n"
"    \"skip_special_tokens\": False,\n"
"}\n"
"\n"
"# 1) Offline generation\n"
"result = llm.generate(input_ids=input_ids, sampling_params=sampling_params)\n"
"generated_text = result[\"text\"]  # Assume there is only one prompt\n"
"\n"
"print_highlight(\"=== Offline Engine Output Text ===\")\n"
"print_highlight(generated_text)\n"
"\n"
"\n"
"# 2) Parse using FunctionCallParser\n"
"def convert_dict_to_tool(tool_dict: dict) -> Tool:\n"
"    function_dict = tool_dict.get(\"function\", {})\n"
"    return Tool(\n"
"        type=tool_dict.get(\"type\", \"function\"),\n"
"        function=Function(\n"
"            name=function_dict.get(\"name\"),\n"
"            description=function_dict.get(\"description\"),\n"
"            parameters=function_dict.get(\"parameters\"),\n"
"        ),\n"
"    )\n"
"\n"
"\n"
"tools = [convert_dict_to_tool(raw_tool) for raw_tool in tools]\n"
"\n"
"parser = FunctionCallParser(tools=tools, tool_call_parser=\"qwen25\")\n"
"normal_text, calls = parser.parse_non_stream(generated_text)\n"
"\n"
"print_highlight(\"=== Parsing Result ===\")\n"
"print(\"Normal text portion:\", normal_text)\n"
"print_highlight(\"Function call portion:\")\n"
"for call in calls:\n"
"    # call: ToolCallItem\n"
"    print_highlight(f\"  - tool name: {call.name}\")\n"
"    print_highlight(f\"    parameters: {call.parameters}\")\n"
"\n"
"# 3) If needed, perform additional logic on the parsed functions, such as "
"automatically calling the corresponding function to obtain a return value, "
"etc."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid "llm.shutdown()"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:627
msgid "Tool Choice Mode"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:629
msgid ""
"SGLang supports OpenAI's ``tool_choice`` parameter to control when and which "
"tools the model should call. This feature is implemented using EBNF "
"(Extended Backus-Naur Form) grammar to ensure reliable tool calling behavior."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:632
msgid "Supported Tool Choice Options"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:634
msgid ""
"**``tool_choice=\"required\"``**: Forces the model to call at least one tool"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:635
msgid ""
"**``tool_choice={\"type\": \"function\", \"function\": {\"name\": "
"\"specific_function\"}}``**: Forces the model to call a specific function"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:638
msgid "Backend Compatibility"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:640
msgid ""
"Tool choice is fully supported with the **Xgrammar backend**, which is the "
"default grammar backend (``--grammar-backend xgrammar``). However, it may "
"not be fully supported with other backends such as ``outlines``."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:643
msgid "Example: Required Tool Choice"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"from openai import OpenAI\n"
"from sglang.utils import wait_for_server, print_highlight, "
"terminate_process\n"
"from sglang.test.doc_patch import launch_server_cmd\n"
"\n"
"# Start a new server session for tool choice examples\n"
"server_process_tool_choice, port_tool_choice = launch_server_cmd(\n"
"    \"python3 -m sglang.launch_server --model-path Qwen/Qwen2.5-7B-Instruct "
"--tool-call-parser qwen25 --host 0.0.0.0  --log-level warning\"\n"
")\n"
"wait_for_server(f\"http://localhost:{port_tool_choice}\")\n"
"\n"
"# Initialize client for tool choice examples\n"
"client_tool_choice = OpenAI(\n"
"    api_key=\"None\", base_url=f\"http://0.0.0.0:{port_tool_choice}/v1\"\n"
")\n"
"model_name_tool_choice = client_tool_choice.models.list().data[0].id\n"
"\n"
"# Example with tool_choice=\"required\" - forces the model to call a tool\n"
"messages_required = [\n"
"    {\"role\": \"user\", \"content\": \"Hello, what is the capital of France?"
"\"}\n"
"]\n"
"\n"
"# Define tools\n"
"tools = [\n"
"    {\n"
"        \"type\": \"function\",\n"
"        \"function\": {\n"
"            \"name\": \"get_current_weather\",\n"
"            \"description\": \"Get the current weather in a given "
"location\",\n"
"            \"parameters\": {\n"
"                \"type\": \"object\",\n"
"                \"properties\": {\n"
"                    \"city\": {\n"
"                        \"type\": \"string\",\n"
"                        \"description\": \"The city to find the weather for, "
"e.g. 'San Francisco'\",\n"
"                    },\n"
"                    \"unit\": {\n"
"                        \"type\": \"string\",\n"
"                        \"description\": \"The unit to fetch the temperature "
"in\",\n"
"                        \"enum\": [\"celsius\", \"fahrenheit\"],\n"
"                    },\n"
"                },\n"
"                \"required\": [\"city\", \"unit\"],\n"
"            },\n"
"        },\n"
"    }\n"
"]\n"
"\n"
"response_required = client_tool_choice.chat.completions.create(\n"
"    model=model_name_tool_choice,\n"
"    messages=messages_required,\n"
"    temperature=0,\n"
"    max_tokens=1024,\n"
"    tools=tools,\n"
"    tool_choice=\"required\",  # Force the model to call a tool\n"
")\n"
"\n"
"print_highlight(\"Response with tool_choice='required':\")\n"
"print(\"Content:\", response_required.choices[0].message.content)\n"
"print(\"Tool calls:\", response_required.choices[0].message.tool_calls)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:722
msgid "Example: Specific Function Choice"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"# Example with specific function choice - forces the model to call a "
"specific function\n"
"messages_specific = [\n"
"    {\"role\": \"user\", \"content\": \"What are the most attactive places "
"in France?\"}\n"
"]\n"
"\n"
"response_specific = client_tool_choice.chat.completions.create(\n"
"    model=model_name_tool_choice,\n"
"    messages=messages_specific,\n"
"    temperature=0,\n"
"    max_tokens=1024,\n"
"    tools=tools,\n"
"    tool_choice={\n"
"        \"type\": \"function\",\n"
"        \"function\": {\"name\": \"get_current_weather\"},\n"
"    },  # Force the model to call the specific get_current_weather function\n"
")\n"
"\n"
"print_highlight(\"Response with specific function choice:\")\n"
"print(\"Content:\", response_specific.choices[0].message.content)\n"
"print(\"Tool calls:\", response_specific.choices[0].message.tool_calls)\n"
"\n"
"if response_specific.choices[0].message.tool_calls:\n"
"    tool_call = response_specific.choices[0].message.tool_calls[0]\n"
"    print_highlight(f\"Called function: {tool_call.function.name}\")\n"
"    print_highlight(f\"Arguments: {tool_call.function.arguments}\")"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid "terminate_process(server_process_tool_choice)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:776
msgid "Pythonic Tool Call Format (Llama-3.2 / Llama-3.3 / Llama-4)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:778
msgid ""
"Some Llama models (such as Llama-3.2-1B, Llama-3.2-3B, Llama-3.3-70B, and "
"Llama-4) support a \"pythonic\" tool call format, where the model outputs "
"function calls as Python code, e.g.:"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:780
msgid ""
"[get_current_weather(city=\"San Francisco\", state=\"CA\", unit=\"celsius\")]"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:784
msgid ""
"The output is a Python list of function calls, with arguments as Python "
"literals (not JSON)."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:785
msgid "Multiple tool calls can be returned in the same list:"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:787
msgid ""
"[get_current_weather(city=\"San Francisco\", state=\"CA\", "
"unit=\"celsius\"),\n"
" get_current_weather(city=\"New York\", state=\"NY\", unit=\"fahrenheit\")]"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:792
msgid ""
"For more information, refer to Meta’s documentation on `Zero shot function "
"calling <https://github.com/meta-llama/llama-models/blob/main/models/llama4/"
"prompt_format.md#zero-shot-function-calling---system-message>`__."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:794
msgid "Note that this feature is still under development on Blackwell."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:797
msgid "How to enable"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:799
msgid "Launch the server with ``--tool-call-parser pythonic``"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:800
msgid ""
"You may also specify --chat-template with the improved template for the "
"model (e.g., ``--chat-template=examples/chat_template/"
"tool_chat_template_llama4_pythonic.jinja``). This is recommended because the "
"model expects a special prompt format to reliably produce valid pythonic "
"tool call outputs. The template ensures that the prompt structure (e.g., "
"special tokens, message boundaries like ``<|eom|>``, and function call "
"delimiters) matches what the model was trained or fine-tuned on. If you do "
"not use the correct chat template, tool calling may fail or produce "
"inconsistent results."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:804
msgid "Forcing Pythonic Tool Call Output Without a Chat Template"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:806
msgid ""
"If you don't want to specify a chat template, you must give the model "
"extremely explicit instructions in your messages to enforce pythonic output. "
"For example, for ``Llama-3.2-1B-Instruct``, you need:"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:-1
msgid ""
"import openai\n"
"\n"
"server_process, port = launch_server_cmd(\n"
"    \" python3 -m sglang.launch_server --model-path meta-llama/Llama-3.2-1B-"
"Instruct --tool-call-parser pythonic --tp 1  --log-level warning\"  # "
"llama-3.2-1b-instruct\n"
")\n"
"wait_for_server(f\"http://localhost:{port}\")\n"
"\n"
"tools = [\n"
"    {\n"
"        \"type\": \"function\",\n"
"        \"function\": {\n"
"            \"name\": \"get_weather\",\n"
"            \"description\": \"Get the current weather for a given location."
"\",\n"
"            \"parameters\": {\n"
"                \"type\": \"object\",\n"
"                \"properties\": {\n"
"                    \"location\": {\n"
"                        \"type\": \"string\",\n"
"                        \"description\": \"The name of the city or location."
"\",\n"
"                    }\n"
"                },\n"
"                \"required\": [\"location\"],\n"
"            },\n"
"        },\n"
"    },\n"
"    {\n"
"        \"type\": \"function\",\n"
"        \"function\": {\n"
"            \"name\": \"get_tourist_attractions\",\n"
"            \"description\": \"Get a list of top tourist attractions for a "
"given city.\",\n"
"            \"parameters\": {\n"
"                \"type\": \"object\",\n"
"                \"properties\": {\n"
"                    \"city\": {\n"
"                        \"type\": \"string\",\n"
"                        \"description\": \"The name of the city to find "
"attractions for.\",\n"
"                    }\n"
"                },\n"
"                \"required\": [\"city\"],\n"
"            },\n"
"        },\n"
"    },\n"
"]\n"
"\n"
"\n"
"def get_messages():\n"
"    return [\n"
"        {\n"
"            \"role\": \"system\",\n"
"            \"content\": (\n"
"                \"You are a travel assistant. \"\n"
"                \"When asked to call functions, ALWAYS respond ONLY with a "
"python list of function calls, \"\n"
"                \"using this format: [func_name1(param1=value1, "
"param2=value2), func_name2(param=value)]. \"\n"
"                \"Do NOT use JSON, do NOT use variables, do NOT use any "
"other format. \"\n"
"                \"Here is an example:\\n\"\n"
"                '[get_weather(location=\"Paris\"), "
"get_tourist_attractions(city=\"Paris\")]'\n"
"            ),\n"
"        },\n"
"        {\n"
"            \"role\": \"user\",\n"
"            \"content\": (\n"
"                \"I'm planning a trip to Tokyo next week. What's the weather "
"like and what are some top tourist attractions? \"\n"
"                \"Propose parallel tool calls at once, using the python list "
"of function calls format as shown above.\"\n"
"            ),\n"
"        },\n"
"    ]\n"
"\n"
"\n"
"messages = get_messages()\n"
"\n"
"client = openai.Client(base_url=f\"http://localhost:{port}/v1\", "
"api_key=\"xxxxxx\")\n"
"model_name = client.models.list().data[0].id\n"
"\n"
"\n"
"response_non_stream = client.chat.completions.create(\n"
"    model=model_name,\n"
"    messages=messages,\n"
"    temperature=0,\n"
"    top_p=0.9,\n"
"    stream=False,  # Non-streaming\n"
"    tools=tools,\n"
")\n"
"print_highlight(\"Non-stream response:\")\n"
"print_highlight(response_non_stream)\n"
"\n"
"response_stream = client.chat.completions.create(\n"
"    model=model_name,\n"
"    messages=messages,\n"
"    temperature=0,\n"
"    top_p=0.9,\n"
"    stream=True,\n"
"    tools=tools,\n"
")\n"
"texts = \"\"\n"
"tool_calls = []\n"
"name = \"\"\n"
"arguments = \"\"\n"
"\n"
"for chunk in response_stream:\n"
"    if chunk.choices[0].delta.content:\n"
"        texts += chunk.choices[0].delta.content\n"
"    if chunk.choices[0].delta.tool_calls:\n"
"        tool_calls.append(chunk.choices[0].delta.tool_calls[0])\n"
"\n"
"print_highlight(\"Streaming Response:\")\n"
"print_highlight(\"==== Text ====\")\n"
"print_highlight(texts)\n"
"\n"
"print_highlight(\"==== Tool Call ====\")\n"
"for tool_call in tool_calls:\n"
"    print_highlight(tool_call)\n"
"\n"
"terminate_process(server_process)"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:938
msgid "**Note:**"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:939
msgid ""
"The model may still default to JSON if it was heavily finetuned on that "
"format. Prompt engineering (including examples) is the only way to increase "
"the chance of pythonic output if you are not using a chat template."
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:951
msgid "How to support a new model?"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:953
msgid ""
"Update the TOOLS_TAG_LIST in sglang/srt/function_call_parser.py with the "
"model’s tool tags. Currently supported tags include:"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:957
msgid ""
"TOOLS_TAG_LIST = [\n"
"    “<|plugin|>“,\n"
"    “<function=“,\n"
"    “<tool_call>“,\n"
"    “<|python_tag|>“,\n"
"    “[TOOL_CALLS]”\n"
"]"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:965
msgid ""
"Create a new detector class in sglang/srt/function_call_parser.py that "
"inherits from BaseFormatDetector. The detector should handle the model’s "
"specific function call format. For example:"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:969
msgid "class NewModelDetector(BaseFormatDetector):"
msgstr ""

#: ../../../advanced_features/tool_parser.ipynb:971
msgid ""
"Add the new detector to the MultiFormatParser class that manages all the "
"format detectors."
msgstr ""
