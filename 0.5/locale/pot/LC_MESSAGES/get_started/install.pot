# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-02-10 09:10+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../get_started/install.md:1
msgid "Install SGLang"
msgstr ""

#: ../../../get_started/install.md:3
msgid "You can install SGLang using one of the methods below."
msgstr ""

#: ../../../get_started/install.md:5
msgid ""
"This page primarily applies to common NVIDIA GPU platforms. For other or "
"newer platforms, please refer to the dedicated pages for [AMD GPUs](../"
"platforms/amd_gpu.md), [Intel Xeon CPUs](../platforms/cpu_server.md), [TPU]"
"(../platforms/tpu.md), [NVIDIA DGX Spark](https://lmsys.org/blog/2025-11-03-"
"gpt-oss-on-nvidia-dgx-spark/), [NVIDIA Jetson](../platforms/nvidia_jetson."
"md), [Ascend NPUs](../platforms/ascend_npu.md), and [Intel XPU](../platforms/"
"xpu.md)."
msgstr ""

#: ../../../get_started/install.md:8
msgid "Method 1: With pip or uv"
msgstr ""

#: ../../../get_started/install.md:10
msgid "It is recommended to use uv for faster installation:"
msgstr ""

#: ../../../get_started/install.md:12
msgid ""
"pip install --upgrade pip\n"
"pip install uv\n"
"uv pip install \"sglang\"\n"
msgstr ""

#: ../../../get_started/install.md:18 ../../../get_started/install.md:47
msgid "**Quick fixes to common problems**"
msgstr ""

#: ../../../get_started/install.md:19
msgid ""
"In some cases (e.g., GB200), the above command might install a wrong torch "
"version (e.g., the CPU version) due to dependency resolution. To fix this, "
"you can first run the above command and then force-reinstall the correct "
"[PyTorch](https://pytorch.org/get-started/locally/) with the following:"
msgstr ""

#: ../../../get_started/install.md:20
msgid ""
"uv pip install \"torch\" --extra-index-url https://download.pytorch.org/whl/"
"cu129 --force-reinstall\n"
msgstr ""

#: ../../../get_started/install.md:23
msgid ""
"For CUDA 13, Docker is recommended (see the Method 3 note on B300/GB300/CUDA "
"13). If you do not have Docker access, installing the matching `sgl_kernel` "
"wheel from [the sgl-project whl releases](https://github.com/sgl-project/whl/"
"releases) after installing SGLang also works. Replace `X.Y.Z` with the "
"`sgl_kernel` version required by your SGLang (you can find this by running "
"`uv pip show sgl_kernel`). Examples:"
msgstr ""

#: ../../../get_started/install.md:24
msgid ""
"# x86_64\n"
"uv pip install \"https://github.com/sgl-project/whl/releases/download/vX.Y.Z/"
"sgl_kernel-X.Y.Z+cu130-cp310-abi3-manylinux2014_x86_64.whl\"\n"
"\n"
"# aarch64\n"
"uv pip install \"https://github.com/sgl-project/whl/releases/download/vX.Y.Z/"
"sgl_kernel-X.Y.Z+cu130-cp310-abi3-manylinux2014_aarch64.whl\"\n"
msgstr ""

#: ../../../get_started/install.md:31
msgid ""
"If you encounter `OSError: CUDA_HOME environment variable is not set`, set "
"it to your CUDA install root with either of the following solutions:"
msgstr ""

#: ../../../get_started/install.md:32
msgid ""
"Use `export CUDA_HOME=/usr/local/cuda-<your-cuda-version>` to set the "
"`CUDA_HOME` environment variable."
msgstr ""

#: ../../../get_started/install.md:33
msgid ""
"Install FlashInfer first following [FlashInfer installation doc](https://"
"docs.flashinfer.ai/installation.html), then install SGLang as described "
"above."
msgstr ""

#: ../../../get_started/install.md:35
msgid "Method 2: From source"
msgstr ""

#: ../../../get_started/install.md:37
msgid ""
"# Use the last release branch\n"
"git clone -b v0.5.6.post2 https://github.com/sgl-project/sglang.git\n"
"cd sglang\n"
"\n"
"# Install the python packages\n"
"pip install --upgrade pip\n"
"pip install -e \"python\"\n"
msgstr ""

#: ../../../get_started/install.md:49
msgid ""
"If you want to develop SGLang, you can try the dev docker image. Please "
"refer to [setup docker container](../developer_guide/"
"development_guide_using_docker.md#setup-docker-container). The docker image "
"is `lmsysorg/sglang:dev`."
msgstr ""

#: ../../../get_started/install.md:51
msgid "Method 3: Using docker"
msgstr ""

#: ../../../get_started/install.md:53
msgid ""
"The docker images are available on Docker Hub at [lmsysorg/sglang](https://"
"hub.docker.com/r/lmsysorg/sglang/tags), built from [Dockerfile](https://"
"github.com/sgl-project/sglang/tree/main/docker). Replace `<secret>` below "
"with your huggingface hub [token](https://huggingface.co/docs/hub/en/"
"security-tokens)."
msgstr ""

#: ../../../get_started/install.md:56
msgid ""
"docker run --gpus all \\\n"
"    --shm-size 32g \\\n"
"    -p 30000:30000 \\\n"
"    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n"
"    --env \"HF_TOKEN=<secret>\" \\\n"
"    --ipc=host \\\n"
"    lmsysorg/sglang:latest \\\n"
"    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-"
"Instruct --host 0.0.0.0 --port 30000\n"
msgstr ""

#: ../../../get_started/install.md:67
msgid ""
"For production deployments, use the `runtime` variant which is significantly "
"smaller (~40% reduction) by excluding build tools and development "
"dependencies:"
msgstr ""

#: ../../../get_started/install.md:69
msgid ""
"docker run --gpus all \\\n"
"    --shm-size 32g \\\n"
"    -p 30000:30000 \\\n"
"    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n"
"    --env \"HF_TOKEN=<secret>\" \\\n"
"    --ipc=host \\\n"
"    lmsysorg/sglang:latest-runtime \\\n"
"    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-"
"Instruct --host 0.0.0.0 --port 30000\n"
msgstr ""

#: ../../../get_started/install.md:80
msgid ""
"You can also find the nightly docker images [here](https://hub.docker.com/r/"
"lmsysorg/sglang/tags?name=nightly)."
msgstr ""

#: ../../../get_started/install.md:82
msgid "Notes:"
msgstr ""

#: ../../../get_started/install.md:83
msgid ""
"On B300/GB300 (SM103) or CUDA 13 environment, we recommend using the nightly "
"image at `lmsysorg/sglang:dev-cu13` or stable image at `lmsysorg/sglang:"
"latest-cu130-runtime`. Please, do not re-install the project as editable "
"inside the docker image, since it will override the version of libraries "
"specified by the cu13 docker image."
msgstr ""

#: ../../../get_started/install.md:85
msgid "Method 4: Using Kubernetes"
msgstr ""

#: ../../../get_started/install.md:87
msgid ""
"Please check out [OME](https://github.com/sgl-project/ome), a Kubernetes "
"operator for enterprise-grade management and serving of large language "
"models (LLMs)."
msgstr ""

#: ../../../get_started/install.md:89 ../../../get_started/install.md:104
#: ../../../get_started/install.md:116 ../../../get_started/install.md:160
msgid ""
"<details>\n"
"<summary>More</summary>\n"
msgstr ""

#: ../../../get_started/install.md:92
msgid ""
"Option 1: For single node serving (typically when the model size fits into "
"GPUs on one node)"
msgstr ""

#: ../../../get_started/install.md:94
msgid ""
"Execute command `kubectl apply -f docker/k8s-sglang-service.yaml`, to create "
"k8s deployment and service, with llama-31-8b as example."
msgstr ""

#: ../../../get_started/install.md:96
msgid ""
"Option 2: For multi-node serving (usually when a large model requires more "
"than one GPU node, such as `DeepSeek-R1`)"
msgstr ""

#: ../../../get_started/install.md:98
msgid ""
"Modify the LLM model path and arguments as necessary, then execute command "
"`kubectl apply -f docker/k8s-sglang-distributed-sts.yaml`, to create two "
"nodes k8s statefulset and serving service."
msgstr ""

#: ../../../get_started/install.md:100 ../../../get_started/install.md:112
#: ../../../get_started/install.md:144 ../../../get_started/install.md:156
#: ../../../get_started/install.md:201 ../../../get_started/install.md:209
msgid "</details>\n"
msgstr ""

#: ../../../get_started/install.md:102
msgid "Method 5: Using docker compose"
msgstr ""

#: ../../../get_started/install.md:107
msgid ""
"This method is recommended if you plan to serve it as a service. A better "
"approach is to use the [k8s-sglang-service.yaml](https://github.com/sgl-"
"project/sglang/blob/main/docker/k8s-sglang-service.yaml)."
msgstr ""

#: ../../../get_started/install.md:110
msgid ""
"Copy the [compose.yml](https://github.com/sgl-project/sglang/blob/main/"
"docker/compose.yaml) to your local machine"
msgstr ""

#: ../../../get_started/install.md:111
msgid "Execute the command `docker compose up -d` in your terminal."
msgstr ""

#: ../../../get_started/install.md:114
msgid "Method 6: Run on Kubernetes or Clouds with SkyPilot"
msgstr ""

#: ../../../get_started/install.md:119
msgid ""
"To deploy on Kubernetes or 12+ clouds, you can use [SkyPilot](https://github."
"com/skypilot-org/skypilot)."
msgstr ""

#: ../../../get_started/install.md:121
msgid ""
"Install SkyPilot and set up Kubernetes cluster or cloud access: see "
"[SkyPilot's documentation](https://skypilot.readthedocs.io/en/latest/getting-"
"started/installation.html)."
msgstr ""

#: ../../../get_started/install.md:122
msgid ""
"Deploy on your own infra with a single command and get the HTTP API endpoint:"
msgstr ""

#: ../../../get_started/install.md:123
msgid ""
"<details>\n"
"<summary>SkyPilot YAML: <code>sglang.yaml</code></summary>\n"
msgstr ""

#: ../../../get_started/install.md:126
msgid ""
"# sglang.yaml\n"
"envs:\n"
"  HF_TOKEN: null\n"
"\n"
"resources:\n"
"  image_id: docker:lmsysorg/sglang:latest\n"
"  accelerators: A100\n"
"  ports: 30000\n"
"\n"
"run: |\n"
"  conda deactivate\n"
"  python3 -m sglang.launch_server \\\n"
"    --model-path meta-llama/Llama-3.1-8B-Instruct \\\n"
"    --host 0.0.0.0 \\\n"
"    --port 30000\n"
msgstr ""

#: ../../../get_started/install.md:146
msgid ""
"# Deploy on any cloud or Kubernetes cluster. Use --cloud <cloud> to select a "
"specific cloud provider.\n"
"HF_TOKEN=<secret> sky launch -c sglang --env HF_TOKEN sglang.yaml\n"
"\n"
"# Get the HTTP API endpoint\n"
"sky status --endpoint 30000 sglang\n"
msgstr ""

#: ../../../get_started/install.md:154
msgid ""
"To further scale up your deployment with autoscaling and failure recovery, "
"check out the [SkyServe + SGLang guide](https://github.com/skypilot-org/"
"skypilot/tree/master/llm/sglang#serving-llama-2-with-sglang-for-more-traffic-"
"using-skyserve)."
msgstr ""

#: ../../../get_started/install.md:158
msgid "Method 7: Run on AWS SageMaker"
msgstr ""

#: ../../../get_started/install.md:163
msgid ""
"To deploy on SGLang on AWS SageMaker, check out [AWS SageMaker Inference]"
"(https://aws.amazon.com/sagemaker/ai/deploy)"
msgstr ""

#: ../../../get_started/install.md:165
msgid ""
"Amazon Web Services provide supports for SGLang containers along with "
"routine security patching. For available SGLang containers, check out [AWS "
"SGLang DLCs](https://github.com/aws/deep-learning-containers/blob/master/"
"available_images.md#sglang-containers)"
msgstr ""

#: ../../../get_started/install.md:167
msgid "To host a model with your own container, follow the following steps:"
msgstr ""

#: ../../../get_started/install.md:169
msgid ""
"Build a docker container with [sagemaker.Dockerfile](https://github.com/sgl-"
"project/sglang/blob/main/docker/sagemaker.Dockerfile) alongside the [serve]"
"(https://github.com/sgl-project/sglang/blob/main/docker/serve) script."
msgstr ""

#: ../../../get_started/install.md:170
msgid "Push your container onto AWS ECR."
msgstr ""

#: ../../../get_started/install.md:172
msgid ""
"<details>\n"
"<summary>Dockerfile Build Script: <code>build-and-push.sh</code></summary>\n"
msgstr ""

#: ../../../get_started/install.md:175
msgid ""
"#!/bin/bash\n"
"AWS_ACCOUNT=\"<YOUR_AWS_ACCOUNT>\"\n"
"AWS_REGION=\"<YOUR_AWS_REGION>\"\n"
"REPOSITORY_NAME=\"<YOUR_REPOSITORY_NAME>\"\n"
"IMAGE_TAG=\"<YOUR_IMAGE_TAG>\"\n"
"\n"
"ECR_REGISTRY=\"${AWS_ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com\"\n"
"IMAGE_URI=\"${ECR_REGISTRY}/${REPOSITORY_NAME}:${IMAGE_TAG}\"\n"
"\n"
"echo \"Starting build and push process...\"\n"
"\n"
"# Login to ECR\n"
"echo \"Logging into ECR...\"\n"
"aws ecr get-login-password --region ${AWS_REGION} | docker login --username "
"AWS --password-stdin ${ECR_REGISTRY}\n"
"\n"
"# Build the image\n"
"echo \"Building Docker image...\"\n"
"docker build -t ${IMAGE_URI} -f sagemaker.Dockerfile .\n"
"\n"
"echo \"Pushing ${IMAGE_URI}\"\n"
"docker push ${IMAGE_URI}\n"
"\n"
"echo \"Build and push completed successfully!\"\n"
msgstr ""

#: ../../../get_started/install.md:203
msgid ""
"Deploy a model for serving on AWS Sagemaker, refer to "
"[deploy_and_serve_endpoint.py](https://github.com/sgl-project/sglang/blob/"
"main/examples/sagemaker/deploy_and_serve_endpoint.py). For more information, "
"check out [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-"
"sdk)."
msgstr ""

#: ../../../get_started/install.md:204
msgid ""
"By default, the model server on SageMaker will run with the following "
"command: `python3 -m sglang.launch_server --model-path opt/ml/model --host "
"0.0.0.0 --port 8080`. This is optimal for hosting your own model with "
"SageMaker."
msgstr ""

#: ../../../get_started/install.md:205
msgid ""
"To modify your model serving parameters, the [serve](https://github.com/sgl-"
"project/sglang/blob/main/docker/serve) script allows for all available "
"options within `python3 -m sglang.launch_server --help` cli by specifying "
"environment variables with prefix `SM_SGLANG_`."
msgstr ""

#: ../../../get_started/install.md:206
msgid ""
"The serve script will automatically convert all environment variables with "
"prefix `SM_SGLANG_` from `SM_SGLANG_INPUT_ARGUMENT` into `--input-argument` "
"to be parsed into `python3 -m sglang.launch_server` cli."
msgstr ""

#: ../../../get_started/install.md:207
msgid ""
"For example, to run [Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/"
"Qwen3-0.6B) with reasoning parser, simply add additional environment "
"variables `SM_SGLANG_MODEL_PATH=Qwen/Qwen3-0.6B` and "
"`SM_SGLANG_REASONING_PARSER=qwen3`."
msgstr ""

#: ../../../get_started/install.md:211
msgid "Common Notes"
msgstr ""

#: ../../../get_started/install.md:213
msgid ""
"[FlashInfer](https://github.com/flashinfer-ai/flashinfer) is the default "
"attention kernel backend. It only supports sm75 and above. If you encounter "
"any FlashInfer-related issues on sm75+ devices (e.g., T4, A10, A100, L4, "
"L40S, H100), please switch to other kernels by adding `--attention-backend "
"triton --sampling-backend pytorch` and open an issue on GitHub."
msgstr ""

#: ../../../get_started/install.md:214
msgid ""
"To reinstall flashinfer locally, use the following command: `pip3 install --"
"upgrade flashinfer-python --force-reinstall --no-deps` and then delete the "
"cache with `rm -rf ~/.cache/flashinfer`."
msgstr ""

#: ../../../get_started/install.md:215
msgid ""
"When encountering `ptxas fatal   : Value 'sm_103a' is not defined for option "
"'gpu-name'` on B300/GB300, fix it with `export TRITON_PTXAS_PATH=/usr/local/"
"cuda/bin/ptxas`."
msgstr ""
