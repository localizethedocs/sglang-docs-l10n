# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../references/torch_compile_cache.md:1
msgid "Enabling cache for torch.compile"
msgstr ""

#: ../../../references/torch_compile_cache.md:3
msgid ""
"SGLang uses `max-autotune-no-cudagraphs` mode of torch.compile. The auto-"
"tuning can be slow. If you want to deploy a model on many different "
"machines, you can ship the torch.compile cache to these machines and skip "
"the compilation steps."
msgstr ""

#: ../../../references/torch_compile_cache.md:6
msgid ""
"This is based on https://pytorch.org/tutorials/recipes/"
"torch_compile_caching_tutorial.html"
msgstr ""

#: ../../../references/torch_compile_cache.md:9
msgid ""
"Generate the cache by setting TORCHINDUCTOR_CACHE_DIR and running the model "
"once."
msgstr ""

#: ../../../references/torch_compile_cache.md:10
msgid ""
"TORCHINDUCTOR_CACHE_DIR=/root/inductor_root_cache python3 -m sglang."
"launch_server --model meta-llama/Llama-3.1-8B-Instruct --enable-torch-"
"compile\n"
msgstr ""

#: ../../../references/torch_compile_cache.md:13
msgid ""
"Copy the cache folder to other machines and launch the server with "
"`TORCHINDUCTOR_CACHE_DIR`."
msgstr ""
