# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2024, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.3\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../backend/native_api.ipynb:2
msgid ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"
msgstr ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"

#: ../../../backend/native_api.ipynb:9
msgid "Native APIs"
msgstr ""

#: ../../../backend/native_api.ipynb:11
msgid ""
"Apart from the OpenAI compatible APIs, the SGLang Runtime also provides its "
"native server APIs. We introduce these following APIs:"
msgstr ""

#: ../../../backend/native_api.ipynb:13
msgid "``/generate`` (text generation model)"
msgstr ""

#: ../../../backend/native_api.ipynb:14
msgid "``/get_server_args``"
msgstr ""

#: ../../../backend/native_api.ipynb:15
msgid "``/get_model_info``"
msgstr ""

#: ../../../backend/native_api.ipynb:16
msgid "``/health``"
msgstr ""

#: ../../../backend/native_api.ipynb:17
msgid "``/health_generate``"
msgstr ""

#: ../../../backend/native_api.ipynb:18
msgid "``/flush_cache``"
msgstr ""

#: ../../../backend/native_api.ipynb:19
msgid "``/get_memory_pool_size``"
msgstr ""

#: ../../../backend/native_api.ipynb:20
msgid "``/update_weights``"
msgstr ""

#: ../../../backend/native_api.ipynb:21
msgid "``/encode``\\ (embedding model)"
msgstr ""

#: ../../../backend/native_api.ipynb:22
msgid "``/classify``\\ (reward model)"
msgstr ""

#: ../../../backend/native_api.ipynb:24
msgid ""
"We mainly use ``requests`` to test these APIs in the following examples. You "
"can also use ``curl``."
msgstr ""

#: ../../../backend/native_api.ipynb:36
msgid "Launch A Server"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid "[ ]:"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"from sglang.utils import (\n"
"    execute_shell_command,\n"
"    wait_for_server,\n"
"    terminate_process,\n"
"    print_highlight,\n"
")\n"
"\n"
"import requests\n"
"\n"
"server_process = execute_shell_command(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model-path meta-llama/Llama-3.2-1B-"
"Instruct --port=30010\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(\"http://localhost:30010\")"
msgstr ""

#: ../../../backend/native_api.ipynb:72
msgid "Generate (text generation model)"
msgstr ""

#: ../../../backend/native_api.ipynb:74
msgid ""
"Generate completions. This is similar to the ``/v1/completions`` in OpenAI "
"API. Detailed parameters can be found in the `sampling parameters <../"
"references/sampling_params.md>`__."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"url = \"http://localhost:30010/generate\"\n"
"data = {\"text\": \"What is the capital of France?\"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"print_highlight(response.json())"
msgstr ""

#: ../../../backend/native_api.ipynb:99
msgid "Get Server Args"
msgstr ""

#: ../../../backend/native_api.ipynb:101
msgid "Get the arguments of a server."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"url = \"http://localhost:30010/get_server_args\"\n"
"\n"
"response = requests.get(url)\n"
"print_highlight(response.json())"
msgstr ""

#: ../../../backend/native_api.ipynb:125
msgid "Get Model Info"
msgstr ""

#: ../../../backend/native_api.ipynb:127
msgid "Get the information of the model."
msgstr ""

#: ../../../backend/native_api.ipynb:129
msgid "``model_path``: The path/name of the model."
msgstr ""

#: ../../../backend/native_api.ipynb:130
msgid ""
"``is_generation``: Whether the model is used as generation model or "
"embedding model."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"url = \"http://localhost:30010/get_model_info\"\n"
"\n"
"response = requests.get(url)\n"
"response_json = response.json()\n"
"print_highlight(response_json)\n"
"assert response_json[\"model_path\"] == \"meta-llama/Llama-3.2-1B-"
"Instruct\"\n"
"assert response_json[\"is_generation\"] is True\n"
"assert response_json.keys() == {\"model_path\", \"is_generation\"}"
msgstr ""

#: ../../../backend/native_api.ipynb:158
msgid "Health Check"
msgstr ""

#: ../../../backend/native_api.ipynb:160
msgid "``/health``: Check the health of the server."
msgstr ""

#: ../../../backend/native_api.ipynb:161
msgid ""
"``/health_generate``: Check the health of the server by generating one token."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"url = \"http://localhost:30010/health_generate\"\n"
"\n"
"response = requests.get(url)\n"
"print_highlight(response.text)"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"url = \"http://localhost:30010/health\"\n"
"\n"
"response = requests.get(url)\n"
"print_highlight(response.text)"
msgstr ""

#: ../../../backend/native_api.ipynb:197
msgid "Flush Cache"
msgstr ""

#: ../../../backend/native_api.ipynb:199
msgid ""
"Flush the radix cache. It will be automatically triggered when the model "
"weights are updated by the ``/update_weights`` API."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"# flush cache\n"
"\n"
"url = \"http://localhost:30010/flush_cache\"\n"
"\n"
"response = requests.post(url)\n"
"print_highlight(response.text)"
msgstr ""

#: ../../../backend/native_api.ipynb:225
msgid "Get Memory Pool Size"
msgstr ""

#: ../../../backend/native_api.ipynb:227
msgid "Get the memory pool size in number of tokens."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"# get_memory_pool_size\n"
"\n"
"url = \"http://localhost:30010/get_memory_pool_size\"\n"
"\n"
"response = requests.get(url)\n"
"print_highlight(response.text)"
msgstr ""

#: ../../../backend/native_api.ipynb:253
msgid "Update Weights"
msgstr ""

#: ../../../backend/native_api.ipynb:255
msgid ""
"Update model weights without restarting the server. Use for continuous "
"evaluation during training. Only applicable for models with the same "
"architecture and parameter size."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"# successful update with same architecture and size\n"
"\n"
"url = \"http://localhost:30010/update_weights\"\n"
"data = {\"model_path\": \"meta-llama/Llama-3.2-1B\"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"print_highlight(response.text)\n"
"assert response.json()[\"success\"] is True\n"
"assert response.json()[\"message\"] == \"Succeeded to update model weights."
"\"\n"
"assert response.json().keys() == {\"success\", \"message\"}"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"# failed update with different parameter size\n"
"\n"
"url = \"http://localhost:30010/update_weights\"\n"
"data = {\"model_path\": \"meta-llama/Llama-3.2-3B\"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"response_json = response.json()\n"
"print_highlight(response_json)\n"
"assert response_json[\"success\"] is False\n"
"assert response_json[\"message\"] == (\n"
"    \"Failed to update weights: The size of tensor a (2048) must match \"\n"
"    \"the size of tensor b (3072) at non-singleton dimension 1.\\n\"\n"
"    \"Rolling back to original weights.\"\n"
")"
msgstr ""

#: ../../../backend/native_api.ipynb:307
msgid "Encode (embedding model)"
msgstr ""

#: ../../../backend/native_api.ipynb:309
msgid ""
"Encode text into embeddings. Note that this API is only available for "
"`embedding models <openai_api_embeddings.html#openai-apis-embedding>`__ and "
"will raise an error for generation models. Therefore, we launch a new server "
"to server an embedding model."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"terminate_process(server_process)\n"
"\n"
"embedding_process = execute_shell_command(\n"
"    \"\"\"\n"
"python -m sglang.launch_server --model-path Alibaba-NLP/gte-Qwen2-7B-"
"instruct \\\n"
"    --port 30020 --host 0.0.0.0 --is-embedding\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(\"http://localhost:30020\")"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"# successful encode for embedding model\n"
"\n"
"url = \"http://localhost:30020/encode\"\n"
"data = {\"model\": \"Alibaba-NLP/gte-Qwen2-7B-instruct\", \"text\": \"Once "
"upon a time\"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"response_json = response.json()\n"
"print_highlight(f\"Text embedding (first 10): {response_json['embedding']"
"[:10]}\")"
msgstr ""

#: ../../../backend/native_api.ipynb:355
msgid "Classify (reward model)"
msgstr ""

#: ../../../backend/native_api.ipynb:357
msgid ""
"SGLang Runtime also supports reward models. Here we use a reward model to "
"classify the quality of pairwise generations."
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"terminate_process(embedding_process)\n"
"\n"
"# Note that SGLang now treats embedding models and reward models as the same "
"type of models.\n"
"# This will be updated in the future.\n"
"\n"
"reward_process = execute_shell_command(\n"
"    \"\"\"\n"
"python -m sglang.launch_server --model-path Skywork/Skywork-Reward-"
"Llama-3.1-8B-v0.2 --port 30030 --host 0.0.0.0 --is-embedding\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(\"http://localhost:30030\")"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid ""
"from transformers import AutoTokenizer\n"
"\n"
"PROMPT = (\n"
"    \"What is the range of the numeric output of a sigmoid node in a neural "
"network?\"\n"
")\n"
"\n"
"RESPONSE1 = \"The output of a sigmoid node is bounded between -1 and 1.\"\n"
"RESPONSE2 = \"The output of a sigmoid node is bounded between 0 and 1.\"\n"
"\n"
"CONVS = [\n"
"    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", "
"\"content\": RESPONSE1}],\n"
"    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", "
"\"content\": RESPONSE2}],\n"
"]\n"
"\n"
"tokenizer = AutoTokenizer.from_pretrained(\"Skywork/Skywork-Reward-"
"Llama-3.1-8B-v0.2\")\n"
"prompts = tokenizer.apply_chat_template(CONVS, tokenize=False)\n"
"\n"
"url = \"http://localhost:30030/classify\"\n"
"data = {\"model\": \"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\", \"text\": "
"prompts}\n"
"\n"
"responses = requests.post(url, json=data).json()\n"
"for response in responses:\n"
"    print_highlight(f\"reward: {response['embedding'][0]}\")"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid "[15]:"
msgstr ""

#: ../../../backend/native_api.ipynb:-1
msgid "terminate_process(reward_process)"
msgstr ""
