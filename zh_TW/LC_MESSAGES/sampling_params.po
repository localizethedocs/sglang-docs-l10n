# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2024, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.2\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:47+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../sampling_params.md:1
msgid "Sampling Parameters in SGLang Runtime"
msgstr ""

#: ../../../sampling_params.md:2
msgid ""
"This doc describes the sampling parameters of the SGLang Runtime. It is the "
"low-level endpoint of the runtime. If you want a high-level endpoint that "
"can automatically handle chat templates, consider using the [OpenAI "
"Compatible API ](https://github.com/sgl-project/sglang?tab=readme-ov-"
"file#openai-compatible-api)."
msgstr ""

#: ../../../sampling_params.md:7
msgid ""
"The `/generate` endpoint accepts the following arguments in the JSON format."
msgstr ""

#: ../../../sampling_params.md:9
msgid ""
"@dataclass\n"
"class GenerateReqInput:\n"
"    # The input prompt. It can be a single prompt or a batch of prompts.\n"
"    text: Optional[Union[List[str], str]] = None\n"
"    # The token ids for text; one can either specify text or input_ids.\n"
"    input_ids: Optional[Union[List[List[int]], List[int]]] = None\n"
"    # The image input. It can be a file name, a url, or base64 encoded "
"string.\n"
"    # See also python/sglang/srt/utils.py:load_image.\n"
"    image_data: Optional[Union[List[str], str]] = None\n"
"    # The sampling_params. See descriptions below.\n"
"    sampling_params: Union[List[Dict], Dict] = None\n"
"    # The request id.\n"
"    rid: Optional[Union[List[str], str]] = None\n"
"    # Whether to return logprobs.\n"
"    return_logprob: Optional[Union[List[bool], bool]] = None\n"
"    # The start location of the prompt for return_logprob.\n"
"    logprob_start_len: Optional[Union[List[int], int]] = None\n"
"    # The number of top logprobs to return.\n"
"    top_logprobs_num: Optional[Union[List[int], int]] = None\n"
"    # Whether to detokenize tokens in text in the returned logprobs.\n"
"    return_text_in_logprobs: bool = False\n"
"    # Whether to stream output.\n"
"    stream: bool = False\n"
msgstr ""

#: ../../../sampling_params.md:35
msgid "The `sampling_params` follows this format"
msgstr ""

#: ../../../sampling_params.md:37
msgid ""
"# The maximum number of output tokens\n"
"max_new_tokens: int = 128,\n"
"# Stop when hitting any of the strings in this list.\n"
"stop: Optional[Union[str, List[str]]] = None,\n"
"# Stop when hitting any of the token_ids in this list. Could be useful when "
"mixed with\n"
"# `min_new_tokens`.\n"
"stop_token_ids: Optional[List[int]] = [],\n"
"# Sampling temperature\n"
"temperature: float = 1.0,\n"
"# Top-p sampling\n"
"top_p: float = 1.0,\n"
"# Top-k sampling\n"
"top_k: int = -1,\n"
"# Min-p sampling\n"
"min_p: float = 0.0,\n"
"# Whether to ignore EOS token.\n"
"ignore_eos: bool = False,\n"
"# Whether to skip the special tokens during detokenization.\n"
"skip_special_tokens: bool = True,\n"
"# Whether to add spaces between special tokens during detokenization.\n"
"spaces_between_special_tokens: bool = True,\n"
"# Constrains the output to follow a given regular expression.\n"
"regex: Optional[str] = None,\n"
"# Do parallel sampling and return `n` outputs.\n"
"n: int = 1,\n"
"# Constrains the output to follow a given JSON schema.\n"
"# `regex` and `json_schema` cannot be set at the same time.\n"
"json_schema: Optional[str] = None,\n"
"\n"
"## Penalties. See [Performance Implications on Penalties] section below for "
"more informations.\n"
"\n"
"# Float that penalizes new tokens based on their frequency in the generated "
"text so far.\n"
"# Values > 0 encourage the model to use new tokens, while values < 0 "
"encourage the model to\n"
"# repeat tokens. Must be -2 <= value <= 2. Setting to 0 (default) will "
"disable this penalty.\n"
"frequency_penalty: float = 0.0,\n"
"# Float that penalizes new tokens based on whether they appear in the "
"generated text so far.\n"
"# Values > 0 encourage the model to use new tokens, while values < 0 "
"encourage the model to repeat\n"
"# tokens. Must be -2 <= value <= 2. Setting to 0 (default) will disable this "
"penalty.\n"
"presence_penalty: float = 0.0,\n"
"# Float that penalizes new tokens based on whether they appear in the prompt "
"and the generated text\n"
"# so far. Values > 1 encourage the model to use new tokens, while values < 1 "
"encourage the model to\n"
"# repeat tokens. Must be 0 <= value <= 2. Setting to 1 (default) will "
"disable this penalty.\n"
"repetition_penalty: float = 1.0,\n"
"# Guides inference to generate at least this number of tokens by penalizing "
"logits of tokenizer's\n"
"# EOS token and `stop_token_ids` to -inf, until the output token reaches "
"given length.\n"
"# Note that any of the `stop` string can be generated before reaching "
"`min_new_tokens`, as it is\n"
"# difficult to infer the correct token ID by given `stop` strings.\n"
"# Must be 0 <= value < max_new_tokens. Setting to 0 (default) will disable "
"this penalty.\n"
"min_new_tokens: int = 0,\n"
msgstr ""

#: ../../../sampling_params.md:89
msgid "Examples"
msgstr "範例"

#: ../../../sampling_params.md:91
msgid "Normal"
msgstr ""

#: ../../../sampling_params.md:92 ../../../sampling_params.md:147
msgid "Launch a server"
msgstr ""

#: ../../../sampling_params.md:93
msgid ""
"python -m sglang.launch_server --model-path meta-llama/Meta-Llama-3-8B-"
"Instruct --port 30000\n"
msgstr ""

#: ../../../sampling_params.md:97 ../../../sampling_params.md:157
msgid "Send a request"
msgstr ""

#: ../../../sampling_params.md:98
msgid ""
"import requests\n"
"\n"
"response = requests.post(\n"
"    \"http://localhost:30000/generate\",\n"
"    json={\n"
"        \"text\": \"The capital of France is\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 32,\n"
"        },\n"
"    },\n"
")\n"
"print(response.json())\n"
msgstr ""

#: ../../../sampling_params.md:114
msgid "Streaming"
msgstr ""

#: ../../../sampling_params.md:115
msgid "Send a request and stream the output"
msgstr ""

#: ../../../sampling_params.md:116
msgid ""
"import requests, json\n"
"\n"
"response = requests.post(\n"
"    \"http://localhost:30000/generate\",\n"
"    json={\n"
"        \"text\": \"The capital of France is\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 32,\n"
"        },\n"
"        \"stream\": True,\n"
"    },\n"
"    stream=True,\n"
")\n"
"\n"
"prev = 0\n"
"for chunk in response.iter_lines(decode_unicode=False):\n"
"    chunk = chunk.decode(\"utf-8\")\n"
"    if chunk and chunk.startswith(\"data:\"):\n"
"        if chunk == \"data: [DONE]\":\n"
"            break\n"
"        data = json.loads(chunk[5:].strip(\"\\n\"))\n"
"        output = data[\"text\"].strip()\n"
"        print(output[prev:], end=\"\", flush=True)\n"
"        prev = len(output)\n"
"print(\"\")\n"
msgstr ""

#: ../../../sampling_params.md:145
msgid "Multi modal"
msgstr ""

#: ../../../sampling_params.md:148
msgid ""
"python3 -m sglang.launch_server --model-path lmms-lab/llava-onevision-"
"qwen2-7b-ov --chat-template chatml-llava\n"
msgstr ""

#: ../../../sampling_params.md:152
msgid "Download an image"
msgstr ""

#: ../../../sampling_params.md:153
msgid ""
"curl -o example_image.png -L https://github.com/sgl-project/sglang/blob/main/"
"test/lang/example_image.png?raw=true\n"
msgstr ""

#: ../../../sampling_params.md:158
msgid ""
"import requests\n"
"\n"
"response = requests.post(\n"
"    \"http://localhost:30000/generate\",\n"
"    json={\n"
"        \"text\": \"<|im_start|>system\\nYou are a helpful assistant.<|"
"im_end|>\\n\"\n"
"                \"<|im_start|>user\\n<image>\\nDescribe this image in a very "
"short sentence.<|im_end|>\\n\"\n"
"                \"<|im_start|>assistant\\n\",\n"
"        \"image_data\": \"example_image.png\",\n"
"        \"sampling_params\": {\n"
"            \"temperature\": 0,\n"
"            \"max_new_tokens\": 32,\n"
"        },\n"
"    },\n"
")\n"
"print(response.json())\n"
msgstr ""

#: ../../../sampling_params.md:177
msgid ""
"The `image_data` can be a file name, a URL, or a base64 encoded string. See "
"also `python/sglang/srt/utils.py:load_image`. Streaming is supported in a "
"similar manner as [above](#streaming)."
msgstr ""

#: ../../../sampling_params.md:180
msgid "Performance Implications on Penalties"
msgstr ""

#: ../../../sampling_params.md:182
msgid ""
"While you can apply penalties by supplying relevant `sampling_params`, this "
"comes with some drawbacks."
msgstr ""

#: ../../../sampling_params.md:184
msgid ""
"These drawbacks will be applied to every single requests in the same batch, "
"as penalizers also applies in batch."
msgstr ""

#: ../../../sampling_params.md:186
msgid "Latency"
msgstr ""

#: ../../../sampling_params.md:188
msgid ""
"While we try to compute penalty algorithms through CUDA, it is still "
"additional computation on top of the basic sampling logic. For detailed "
"overhead, we recommend you to run your own benchmarks, but you can find "
"samples below to get a glimpse."
msgstr ""

#: ../../../sampling_params.md:190
msgid "Memory"
msgstr ""

#: ../../../sampling_params.md:192
msgid ""
"Since we compute penalty algorithms through CUDA, the logic stores relevant "
"parameters on GPU. This is usually in a scale of `vocab_size` multiplied by "
"`running_requests`."
msgstr ""

#: ../../../sampling_params.md:194
msgid ""
"You can run your own benchmark with desired parameters on your own hardware "
"to make sure it's not OOMing before using."
msgstr ""

#: ../../../sampling_params.md:196
msgid ""
"Tuning `--mem-fraction-static` and/or `--max-running-requests` will help. "
"See [here](hyperparameter_tuning.md#minor-tune---max-prefill-tokens---mem-"
"fraction-static---max-running-requests) for more information."
msgstr ""

#: ../../../sampling_params.md:198
msgid "Benchmarks"
msgstr ""

#: ../../../sampling_params.md:200
msgid "All the benchmarks below were ran on NVIDIA H100 SXM5."
msgstr ""

#: ../../../sampling_params.md:202
msgid "<details>\n"
msgstr ""

#: ../../../sampling_params.md:204
msgid "Baseline"
msgstr ""

#: ../../../sampling_params.md:206
msgid ""
"Measured at [dc9d06d886151707f97d0b78095df9de262fd3c9](https://github.com/"
"sgl-project/sglang/commit/dc9d06d886151707f97d0b78095df9de262fd3c9)."
msgstr ""

#: ../../../sampling_params.md:208
msgid ""
"$ python3 -m sglang.bench_serving --backend sglang --port 8413 --dataset-"
"name random --num-prompts 3000 --random-input 256 --random-output 512\n"
"\n"
"============ Serving Benchmark Result ============\n"
"Backend:                                 sglang\n"
"Traffic request rate:                    inf\n"
"Successful requests:                     3000\n"
"Benchmark duration (s):                  66.11\n"
"Total input tokens:                      378633\n"
"Total generated tokens:                  775651\n"
"Total generated tokens (retokenized):    775118\n"
"Request throughput (req/s):              45.38\n"
"Input token throughput (tok/s):          5727.04\n"
"Output token throughput (tok/s):         11732.16\n"
"----------------End-to-End Latency----------------\n"
"Mean E2E Latency (ms):                   40881.94\n"
"Median E2E Latency (ms):                 43967.10\n"
"---------------Time to First Token----------------\n"
"Mean TTFT (ms):                          19884.75\n"
"Median TTFT (ms):                        14226.56\n"
"P99 TTFT (ms):                           47738.97\n"
"-----Time per Output Token (excl. 1st token)------\n"
"Mean TPOT (ms):                          91.96\n"
"Median TPOT (ms):                        90.11\n"
"P99 TPOT (ms):                           308.54\n"
"---------------Inter-token Latency----------------\n"
"Mean ITL (ms):                           174.54\n"
"Median ITL (ms):                         58.56\n"
"P99 ITL (ms):                            440.18\n"
"==================================================\n"
msgstr ""

#: ../../../sampling_params.md:240
msgid "All Together"
msgstr ""

#: ../../../sampling_params.md:242
msgid ""
"$ python3 -m sglang.bench_serving --backend sglang --port 8413 --dataset-"
"name random --num-prompts 3000 --random-input 256 --random-output 512 --"
"extra-request-body '{\n"
"  \"frequency_penalty\": 1.1,\n"
"  \"presence_penalty\": 1.1,\n"
"  \"repetition_penalty\": 0.1,\n"
"  \"min_new_tokens\": 5\n"
"}'\n"
"\n"
"============ Serving Benchmark Result ============\n"
"Backend:                                 sglang\n"
"Traffic request rate:                    inf\n"
"Successful requests:                     3000\n"
"Benchmark duration (s):                  78.35\n"
"Total input tokens:                      378633\n"
"Total generated tokens:                  775651\n"
"Total generated tokens (retokenized):    774756\n"
"Request throughput (req/s):              38.29\n"
"Input token throughput (tok/s):          4832.86\n"
"Output token throughput (tok/s):         9900.39\n"
"----------------End-to-End Latency----------------\n"
"Mean E2E Latency (ms):                   49017.68\n"
"Median E2E Latency (ms):                 52825.70\n"
"---------------Time to First Token----------------\n"
"Mean TTFT (ms):                          23892.60\n"
"Median TTFT (ms):                        18895.47\n"
"P99 TTFT (ms):                           57426.01\n"
"-----Time per Output Token (excl. 1st token)------\n"
"Mean TPOT (ms):                          114.54\n"
"Median TPOT (ms):                        107.27\n"
"P99 TPOT (ms):                           293.31\n"
"---------------Inter-token Latency----------------\n"
"Mean ITL (ms):                           205.68\n"
"Median ITL (ms):                         73.97\n"
"P99 ITL (ms):                            453.86\n"
"==================================================\n"
msgstr ""

#: ../../../sampling_params.md:279
msgid "Frequency Penalty"
msgstr ""

#: ../../../sampling_params.md:281
msgid ""
"$ python3 -m sglang.bench_serving --backend sglang --port 8413 --dataset-"
"name random --num-prompts 3000 --random-input 256 --random-output 512 --"
"extra-request-body '{\n"
"    \"frequency_penalty\": 1.1\n"
"}'\n"
"\n"
"============ Serving Benchmark Result ============\n"
"Backend:                                 sglang\n"
"Traffic request rate:                    inf\n"
"Successful requests:                     3000\n"
"Benchmark duration (s):                  72.72\n"
"Total input tokens:                      378633\n"
"Total generated tokens:                  775651\n"
"Total generated tokens (retokenized):    774955\n"
"Request throughput (req/s):              41.26\n"
"Input token throughput (tok/s):          5206.84\n"
"Output token throughput (tok/s):         10666.51\n"
"----------------End-to-End Latency----------------\n"
"Mean E2E Latency (ms):                   45445.56\n"
"Median E2E Latency (ms):                 48960.39\n"
"---------------Time to First Token----------------\n"
"Mean TTFT (ms):                          22363.16\n"
"Median TTFT (ms):                        17125.02\n"
"P99 TTFT (ms):                           52920.95\n"
"-----Time per Output Token (excl. 1st token)------\n"
"Mean TPOT (ms):                          104.71\n"
"Median TPOT (ms):                        98.30\n"
"P99 TPOT (ms):                           268.06\n"
"---------------Inter-token Latency----------------\n"
"Mean ITL (ms):                           191.60\n"
"Median ITL (ms):                         67.83\n"
"P99 ITL (ms):                            455.46\n"
"==================================================\n"
msgstr ""

#: ../../../sampling_params.md:315
msgid "Presence Penalty"
msgstr ""

#: ../../../sampling_params.md:317
msgid ""
"$ python3 -m sglang.bench_serving --backend sglang --port 8413 --dataset-"
"name random --num-prompts 3000 --random-input 256 --random-output 512 --"
"extra-request-body '{\n"
"    \"presence_penalty\": 1.1\n"
"}'\n"
"\n"
"============ Serving Benchmark Result ============\n"
"Backend:                                 sglang\n"
"Traffic request rate:                    inf\n"
"Successful requests:                     3000\n"
"Benchmark duration (s):                  72.04\n"
"Total input tokens:                      378633\n"
"Total generated tokens:                  775651\n"
"Total generated tokens (retokenized):    775210\n"
"Request throughput (req/s):              41.64\n"
"Input token throughput (tok/s):          5255.98\n"
"Output token throughput (tok/s):         10767.18\n"
"----------------End-to-End Latency----------------\n"
"Mean E2E Latency (ms):                   44926.61\n"
"Median E2E Latency (ms):                 48302.88\n"
"---------------Time to First Token----------------\n"
"Mean TTFT (ms):                          22095.39\n"
"Median TTFT (ms):                        16740.93\n"
"P99 TTFT (ms):                           52554.03\n"
"-----Time per Output Token (excl. 1st token)------\n"
"Mean TPOT (ms):                          103.54\n"
"Median TPOT (ms):                        97.37\n"
"P99 TPOT (ms):                           271.86\n"
"---------------Inter-token Latency----------------\n"
"Mean ITL (ms):                           189.86\n"
"Median ITL (ms):                         68.45\n"
"P99 ITL (ms):                            447.11\n"
"==================================================\n"
msgstr ""

#: ../../../sampling_params.md:351
msgid "Repetition Penalty"
msgstr ""

#: ../../../sampling_params.md:353
msgid ""
"$ python3 -m sglang.bench_serving --backend sglang --port 8413 --dataset-"
"name random --num-prompts 3000 --random-input 256 --random-output 512 --"
"extra-request-body '{\n"
"    \"repetition_penalty\": 0.1\n"
"}'\n"
"\n"
"============ Serving Benchmark Result ============\n"
"Backend: sglang\n"
"Traffic request rate: inf\n"
"Successful requests: 3000\n"
"Benchmark duration (s): 74.54\n"
"Total input tokens: 378633\n"
"Total generated tokens: 775651\n"
"Total generated tokens (retokenized): 766008\n"
"Request throughput (req/s): 40.24\n"
"Input token throughput (tok/s): 5079.36\n"
"Output token throughput (tok/s): 10405.35\n"
"----------------End-to-End Latency----------------\n"
"Mean E2E Latency (ms): 46530.38\n"
"Median E2E Latency (ms): 50302.65\n"
"---------------Time to First Token----------------\n"
"Mean TTFT (ms): 22603.47\n"
"Median TTFT (ms): 17167.08\n"
"P99 TTFT (ms): 54497.85\n"
"-----Time per Output Token (excl. 1st token)------\n"
"Mean TPOT (ms): 117.59\n"
"Median TPOT (ms): 101.79\n"
"P99 TPOT (ms): 320.04\n"
"---------------Inter-token Latency----------------\n"
"Mean ITL (ms): 195.26\n"
"Median ITL (ms): 69.51\n"
"P99 ITL (ms): 433.86\n"
"==================================================\n"
msgstr ""

#: ../../../sampling_params.md:387
msgid "Min New Tokens"
msgstr ""

#: ../../../sampling_params.md:389
msgid ""
"The min new tokens penalizer computes until generation process reaches given "
"`min_new_tokens`."
msgstr ""

#: ../../../sampling_params.md:391
msgid ""
"Dislike other penalizers, setting this to higher value will have more "
"latency implications."
msgstr ""

#: ../../../sampling_params.md:393
msgid ""
"$ python3 -m sglang.bench_serving --backend sglang --port 8413 --dataset-"
"name random --num-prompts 3000 --random-input 256 --random-output 512 --"
"extra-request-body '{\n"
"    \"min_new_tokens\": 5\n"
"}'\n"
"\n"
"============ Serving Benchmark Result ============\n"
"Backend: sglang\n"
"Traffic request rate: inf\n"
"Successful requests: 3000\n"
"Benchmark duration (s): 66.94\n"
"Total input tokens: 378633\n"
"Total generated tokens: 775651\n"
"Total generated tokens (retokenized): 775220\n"
"Request throughput (req/s): 44.81\n"
"Input token throughput (tok/s): 5656.13\n"
"Output token throughput (tok/s): 11586.90\n"
"----------------End-to-End Latency----------------\n"
"Mean E2E Latency (ms): 41888.55\n"
"Median E2E Latency (ms): 45354.16\n"
"---------------Time to First Token----------------\n"
"Mean TTFT (ms): 20866.91\n"
"Median TTFT (ms): 16219.79\n"
"P99 TTFT (ms): 49263.91\n"
"-----Time per Output Token (excl. 1st token)------\n"
"Mean TPOT (ms): 97.05\n"
"Median TPOT (ms): 89.76\n"
"P99 TPOT (ms): 233.50\n"
"---------------Inter-token Latency----------------\n"
"Mean ITL (ms): 179.17\n"
"Median ITL (ms): 55.08\n"
"P99 ITL (ms): 409.12\n"
"==================================================\n"
msgstr ""

#: ../../../sampling_params.md:427
msgid "</details>\n"
msgstr ""
