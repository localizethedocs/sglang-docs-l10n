# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang stable\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../platforms/cpu_server.md:1
msgid "CPU Servers"
msgstr ""

#: ../../../platforms/cpu_server.md:3
msgid ""
"The document addresses how to set up the [SGLang](https://github.com/sgl-"
"project/sglang) environment and run LLM inference on CPU servers. SGLang is "
"enabled and optimized on the CPUs equipped with Intel® AMX® Instructions, "
"which are 4th generation or newer Intel® Xeon® Scalable Processors."
msgstr ""

#: ../../../platforms/cpu_server.md:7
msgid "Optimized Model List"
msgstr ""

#: ../../../platforms/cpu_server.md:9
msgid ""
"A list of popular LLMs are optimized and run efficiently on CPU, including "
"the most notable open-source models like Llama series, Qwen series, and "
"DeepSeek series like DeepSeek-R1 and DeepSeek-V3.1-Terminus."
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "Model Name"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "BF16"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "W8A8_INT8"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "FP8"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "DeepSeek-R1"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[meituan/DeepSeek-R1-Channel-INT8](https://huggingface.co/meituan/DeepSeek-"
"R1-Channel-INT8)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[deepseek-ai/DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "DeepSeek-V3.1-Terminus"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[IntervitensInc/DeepSeek-V3.1-Terminus-Channel-int8](https://huggingface.co/"
"IntervitensInc/DeepSeek-V3.1-Terminus-Channel-int8)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[deepseek-ai/DeepSeek-V3.1-Terminus](https://huggingface.co/deepseek-ai/"
"DeepSeek-V3.1-Terminus)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "Llama-3.2-3B"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[meta-llama/Llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/"
"Llama-3.2-3B-Instruct)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[RedHatAI/Llama-3.2-3B-quantized.w8a8](https://huggingface.co/RedHatAI/"
"Llama-3.2-3B-Instruct-quantized.w8a8)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "Llama-3.1-8B"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/"
"Llama-3.1-8B-Instruct)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[RedHatAI/Meta-Llama-3.1-8B-quantized.w8a8](https://huggingface.co/RedHatAI/"
"Meta-Llama-3.1-8B-quantized.w8a8)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "QwQ-32B"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[RedHatAI/QwQ-32B-quantized.w8a8](https://huggingface.co/RedHatAI/QwQ-32B-"
"quantized.w8a8)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "DeepSeek-Distilled-Llama"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[RedHatAI/DeepSeek-R1-Distill-Llama-70B-quantized.w8a8](https://huggingface."
"co/RedHatAI/DeepSeek-R1-Distill-Llama-70B-quantized.w8a8)"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid "Qwen3-235B"
msgstr ""

#: ../../../platforms/cpu_server.md:0
msgid ""
"[Qwen/Qwen3-235B-A22B-FP8](https://huggingface.co/Qwen/Qwen3-235B-A22B-FP8)"
msgstr ""

#: ../../../platforms/cpu_server.md:23
msgid ""
"**Note:** The model identifiers listed in the table above have been verified "
"on 6th Gen Intel® Xeon® P-core platforms."
msgstr ""

#: ../../../platforms/cpu_server.md:26
msgid "Installation"
msgstr ""

#: ../../../platforms/cpu_server.md:28
msgid "Install Using Docker"
msgstr ""

#: ../../../platforms/cpu_server.md:30
msgid ""
"It is recommended to use Docker for setting up the SGLang environment. A "
"[Dockerfile](https://github.com/sgl-project/sglang/blob/main/docker/xeon."
"Dockerfile) is provided to facilitate the installation. Replace `<secret>` "
"below with your [HuggingFace access token](https://huggingface.co/docs/hub/"
"en/security-tokens)."
msgstr ""

#: ../../../platforms/cpu_server.md:34
msgid ""
"# Clone the SGLang repository\n"
"git clone https://github.com/sgl-project/sglang.git\n"
"cd sglang/docker\n"
"\n"
"# Build the docker image\n"
"docker build -t sglang-cpu:latest -f xeon.Dockerfile .\n"
"\n"
"# Initiate a docker container\n"
"docker run \\\n"
"    -it \\\n"
"    --privileged \\\n"
"    --ipc=host \\\n"
"    --network=host \\\n"
"    -v /dev/shm:/dev/shm \\\n"
"    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n"
"    -p 30000:30000 \\\n"
"    -e \"HF_TOKEN=<secret>\" \\\n"
"    sglang-cpu:latest /bin/bash\n"
msgstr ""

#: ../../../platforms/cpu_server.md:55
msgid "Install From Source"
msgstr ""

#: ../../../platforms/cpu_server.md:57
msgid ""
"If you'd prefer to install SGLang in a bare metal environment, the command "
"list is as below. It is worth noting that the environment variable "
"`SGLANG_USE_CPU_ENGINE=1` is required to enable SGLang service with CPU "
"engine."
msgstr ""

#: ../../../platforms/cpu_server.md:62
msgid ""
"# Create and activate a conda environment\n"
"conda create -n sgl-cpu python=3.12 -y\n"
"conda activate sgl-cpu\n"
"\n"
"# Set PyTorch CPU as primary pip install channel to avoid installing the "
"larger CUDA-enabled version and prevent potential runtime issues.\n"
"pip config set global.index-url https://download.pytorch.org/whl/cpu\n"
"pip config set global.extra-index-url https://pypi.org/simple\n"
"\n"
"# Check if some conda related environment variables have been set\n"
"env | grep -i conda\n"
"# The following environment variable settings are required\n"
"# if they have not been set properly\n"
"export CONDA_EXE=$(which conda)\n"
"export CONDA_ROOT=${CONDA_EXE}/../..\n"
"export CONDA_PREFIX=${CONDA_ROOT}/envs/sgl-cpu\n"
"export PATH=${PATH}:${CONDA_ROOT}/bin:${CONDA_ROOT}/condabin\n"
"\n"
"# Clone the SGLang code\n"
"git clone https://github.com/sgl-project/sglang.git\n"
"cd sglang\n"
"git checkout <YOUR-DESIRED-VERSION>\n"
"\n"
"# Use dedicated toml file\n"
"cd python\n"
"cp pyproject_cpu.toml pyproject.toml\n"
"# Install SGLang dependent libs, and build SGLang main package\n"
"pip install --upgrade pip setuptools\n"
"conda install -y libsqlite==3.48.0 gperftools tbb libnuma numactl\n"
"pip install .\n"
"pip install torch==2.9.0 torchvision==0.24.0 triton==3.5.0 --force-"
"reinstall\n"
"\n"
"# Build the CPU backend kernels\n"
"cd ../sgl-kernel\n"
"cp pyproject_cpu.toml pyproject.toml\n"
"pip install .\n"
"\n"
"# Other required environment variables\n"
"# Recommend to set these in ~/.bashrc in order not to set every time in a "
"new terminal\n"
"export SGLANG_USE_CPU_ENGINE=1\n"
"export LD_PRELOAD=${LD_PRELOAD}:${CONDA_PREFIX}/lib/libiomp5.so:"
"${CONDA_PREFIX}/lib/libtcmalloc.so:${CONDA_PREFIX}/lib/libtbbmalloc.so.2\n"
msgstr ""

#: ../../../platforms/cpu_server.md:105
msgid "Launch of the Serving Engine"
msgstr ""

#: ../../../platforms/cpu_server.md:107
msgid "Example command to launch SGLang serving:"
msgstr ""

#: ../../../platforms/cpu_server.md:109
msgid ""
"python -m sglang.launch_server   \\\n"
"    --model <MODEL_ID_OR_PATH>   \\\n"
"    --trust-remote-code          \\\n"
"    --disable-overlap-schedule   \\\n"
"    --device cpu                 \\\n"
"    --host 0.0.0.0               \\\n"
"    --tp 6\n"
msgstr ""

#: ../../../platforms/cpu_server.md:119
msgid "Notes:"
msgstr ""

#: ../../../platforms/cpu_server.md:121
msgid ""
"For running W8A8 quantized models, please add the flag `--quantization "
"w8a8_int8`."
msgstr ""

#: ../../../platforms/cpu_server.md:123
msgid ""
"The flag `--tp 6` specifies that tensor parallelism will be applied using 6 "
"ranks (TP6).  The number of TP specified is how many TP ranks will be used "
"during the execution.  On a CPU platform, a TP rank means a sub-NUMA cluster "
"(SNC).  Usually we can get the SNC information (How many available) from the "
"Operating System.  Users can specify TP to be no more than the total "
"available SNCs in current system."
msgstr ""

#: ../../../platforms/cpu_server.md:129
msgid ""
"If the specified TP rank number differs from the total SNC count,  the "
"system will automatically utilize the first `n` SNCs.  Note that `n` cannot "
"exceed the total SNC number, doing so will result in an error."
msgstr ""

#: ../../../platforms/cpu_server.md:133
msgid ""
"To specify the cores to be used, we need to explicitly set the environment "
"variable `SGLANG_CPU_OMP_THREADS_BIND`.  For example, if we want to run the "
"SGLang service using the first 40 cores of each SNC on a Xeon® 6980P "
"server,  which has 43-43-42 cores on the 3 SNCs of a socket, we should set:"
msgstr ""

#: ../../../platforms/cpu_server.md:137
msgid ""
"export SGLANG_CPU_OMP_THREADS_BIND=\"0-39|43-82|86-125|128-167|171-210|"
"214-253\"\n"
msgstr ""

#: ../../../platforms/cpu_server.md:141
msgid ""
"Please beware that with SGLANG_CPU_OMP_THREADS_BIND set,  the available "
"memory amounts of the ranks may not be determined in prior.  You may need to "
"set proper `--max-total-tokens` to avoid the out-of-memory error."
msgstr ""

#: ../../../platforms/cpu_server.md:145
msgid ""
"For optimizing decoding with torch.compile, please add the flag `--enable-"
"torch-compile`.  To specify the maximum batch size when using `torch."
"compile`, set the flag `--torch-compile-max-bs`.  For example, `--enable-"
"torch-compile --torch-compile-max-bs 4` means using `torch.compile`  and "
"setting the maximum batch size to 4. Currently the maximum applicable batch "
"size  for optimizing with `torch.compile` is 16."
msgstr ""

#: ../../../platforms/cpu_server.md:151
msgid ""
"A warmup step is automatically triggered when the service is started.  The "
"server is ready when you see the log `The server is fired up and ready to "
"roll!`."
msgstr ""

#: ../../../platforms/cpu_server.md:154
msgid "Benchmarking with Requests"
msgstr ""

#: ../../../platforms/cpu_server.md:156
msgid ""
"You can benchmark the performance via the `bench_serving` script. Run the "
"command in another terminal."
msgstr ""

#: ../../../platforms/cpu_server.md:159
msgid ""
"python -m sglang.bench_serving   \\\n"
"    --dataset-name random        \\\n"
"    --random-input-len 1024      \\\n"
"    --random-output-len 1024     \\\n"
"    --num-prompts 1              \\\n"
"    --request-rate inf           \\\n"
"    --random-range-ratio 1.0\n"
msgstr ""

#: ../../../platforms/cpu_server.md:169
msgid ""
"The detail explanations of the parameters can be looked up by the command:"
msgstr ""

#: ../../../platforms/cpu_server.md:171
msgid "python -m sglang.bench_serving -h\n"
msgstr ""

#: ../../../platforms/cpu_server.md:175
msgid ""
"Additionally, the requests can be formed with [OpenAI Completions API]"
"(https://docs.sglang.ai/basic_usage/openai_api_completions.html) and sent "
"via the command line (e.g. using `curl`) or via your own script."
msgstr ""

#: ../../../platforms/cpu_server.md:179
msgid "Example: Running DeepSeek-V3.1-Terminus"
msgstr ""

#: ../../../platforms/cpu_server.md:181
msgid ""
"An example command to launch service for W8A8_INT8 DeepSeek-V3.1-Terminus on "
"a Xeon® 6980P server:"
msgstr ""

#: ../../../platforms/cpu_server.md:183
msgid ""
"python -m sglang.launch_server                                   \\\n"
"    --model IntervitensInc/DeepSeek-V3.1-Terminus-Channel-int8   \\\n"
"    --trust-remote-code                                          \\\n"
"    --disable-overlap-schedule                                   \\\n"
"    --device cpu                                                 \\\n"
"    --quantization w8a8_int8                                     \\\n"
"    --host 0.0.0.0                                               \\\n"
"    --mem-fraction-static 0.8                                    \\\n"
"    --enable-torch-compile                                       \\\n"
"    --torch-compile-max-bs 4                                     \\\n"
"    --tp 6\n"
msgstr ""

#: ../../../platforms/cpu_server.md:197
msgid ""
"Similarly, an example command to launch service for FP8 DeepSeek-V3.1-"
"Terminus would be:"
msgstr ""

#: ../../../platforms/cpu_server.md:199
msgid ""
"python -m sglang.launch_server                 \\\n"
"    --model deepseek-ai/DeepSeek-V3.1-Terminus \\\n"
"    --trust-remote-code                        \\\n"
"    --disable-overlap-schedule                 \\\n"
"    --device cpu                               \\\n"
"    --host 0.0.0.0                             \\\n"
"    --mem-fraction-static 0.8                  \\\n"
"    --enable-torch-compile                     \\\n"
"    --torch-compile-max-bs 4                   \\\n"
"    --tp 6\n"
msgstr ""

#: ../../../platforms/cpu_server.md:212
msgid ""
"Note: Please set `--torch-compile-max-bs` to the maximum desired batch size "
"for your deployment, which can be up to 16. The value `4` in the examples is "
"illustrative."
msgstr ""

#: ../../../platforms/cpu_server.md:215
msgid ""
"Then you can test with `bench_serving` command or construct your own command "
"or script following [the benchmarking example](#benchmarking-with-requests)."
msgstr ""
