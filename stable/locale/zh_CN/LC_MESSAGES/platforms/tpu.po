# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang stable\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-06 08:37+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../platforms/tpu.md:1
msgid "TPU"
msgstr ""

#: ../../../platforms/tpu.md:3
msgid ""
"SGLang supports high-performance TPU inference through the SGLang-JAX "
"backend, which is specifically optimized for Google Cloud TPUs. The JAX-"
"based implementation delivers exceptional throughput and low latency for "
"Large Language Model (LLM) serving workloads on TPU hardware."
msgstr ""

#: ../../../platforms/tpu.md:5
msgid ""
"For TPU-specific issues or feature requests, please visit the [sglang-jax "
"GitHub issues page](https://github.com/sgl-project/sglang-jax/issues)."
msgstr ""

#: ../../../platforms/tpu.md:7
msgid ""
"**NOTE:** SGLang TPU support is implemented via the SGLang-JAX backend, a "
"dedicated JAX-based inference engine maintained as a separate repository at "
"[https://github.com/sgl-project/sglang-jax](https://github.com/sgl-project/"
"sglang-jax)."
msgstr ""

#: ../../../platforms/tpu.md:9
msgid "System Requirements"
msgstr ""

#: ../../../platforms/tpu.md:11
msgid "Supported TPU Hardware"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "TPU Type"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "HBM Memory"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Availability"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "TPU v6e"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "32 GB"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Google Cloud"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "TPU v7"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "96 GB per core"
msgstr ""

#: ../../../platforms/tpu.md:18
msgid "Software Requirements"
msgstr ""

#: ../../../platforms/tpu.md:20
msgid "**Python:** 3.12 or higher"
msgstr ""

#: ../../../platforms/tpu.md:21
msgid "**JAX:** Latest version with TPU support"
msgstr ""

#: ../../../platforms/tpu.md:22
msgid "**Environment:** Google Cloud TPU VM or compatible TPU runtime"
msgstr ""

#: ../../../platforms/tpu.md:23
msgid "**Optional:** SkyPilot for simplified cloud deployment"
msgstr ""

#: ../../../platforms/tpu.md:25
msgid "Feature Support Matrix"
msgstr ""

#: ../../../platforms/tpu.md:27
msgid ""
"SGLang-JAX provides comprehensive TPU-optimized features for production LLM "
"serving:"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Feature"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Support Status"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Description"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "High-Throughput Continuous Batching"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "‚úÖ"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Dynamic request batching for maximum TPU utilization"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Radix Tree KV Cache"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Memory-efficient prefix sharing between requests"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "FlashAttention Backend"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "TPU-optimized attention kernel for long sequences"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Tensor Parallelism"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Distribute models across multiple TPU cores"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Paged Attention"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Flexible KV cache management with paging"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Speculative Decoding (EAGLE/EAGLE3)"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "20-40% throughput improvement for compatible models"
msgstr ""

#: ../../../platforms/tpu.md:0 ../../../platforms/tpu.md:402
msgid "Chunked Prefill"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Mixed prefill-decode batching"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "OpenAI-Compatible API"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Drop-in replacement for OpenAI API"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Data Parallel Attention"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "üöß"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "In development - Attention computation with data parallelism"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Quantization"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "In development - Model quantization for reduced memory usage"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Multi-LoRA"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "In development - Serve multiple LoRA adapters simultaneously"
msgstr ""

#: ../../../platforms/tpu.md:43
msgid "Attention Backend Comparison"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Backend"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Spec Decoding"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "MLA"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Sliding Window"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "FlashAttention (fa)"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "‚ùå"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Native"
msgstr ""

#: ../../../platforms/tpu.md:50
msgid ""
"**NOTE:** FlashAttention backend is recommended for production workloads due "
"to superior memory efficiency and performance."
msgstr ""

#: ../../../platforms/tpu.md:52
msgid "Optimized Model List"
msgstr ""

#: ../../../platforms/tpu.md:54
msgid "The following models have been tested and optimized for TPU deployment:"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Model Family"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Performance Status"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "[Qwen 3](https://huggingface.co/Qwen)"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "‚≠ê Recommended for production"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "[Qwen 3 MoE](https://huggingface.co/Qwen)"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "‚≠ê Best performance"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "[Qwen 2](https://huggingface.co/Qwen)"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Needs improvement"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "[Qwen 2 MoE](https://huggingface.co/Qwen)"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "[Qwen 1.5](https://huggingface.co/Qwen)"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "[Llama/LLaMA](https://huggingface.co/meta-llama)"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "[Grok-2](https://huggingface.co/xai-org)"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "[Gemma 2](https://huggingface.co/google)"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Verified on TPU"
msgstr ""

#: ../../../platforms/tpu.md:0
msgid "Bailing MoE"
msgstr ""

#: ../../../platforms/tpu.md:68
msgid "Installation"
msgstr ""

#: ../../../platforms/tpu.md:70
msgid "Method 1: Using PyPI (Recommended)"
msgstr ""

#: ../../../platforms/tpu.md:72
msgid "pip install sglang-jax\n"
msgstr ""

#: ../../../platforms/tpu.md:76
msgid "Method 2: From Source"
msgstr ""

#: ../../../platforms/tpu.md:78
msgid ""
"git clone https://github.com/sgl-project/sglang-jax\n"
"cd sglang-jax\n"
"uv venv --python 3.12 && source .venv/bin/activate\n"
"uv pip install -e \"python[all]\"\n"
msgstr ""

#: ../../../platforms/tpu.md:85
msgid "Method 3: Using Docker"
msgstr ""

#: ../../../platforms/tpu.md:87
msgid ""
"**NOTE:** Docker support for TPU is currently under development. Please use "
"PyPI or source installation methods."
msgstr ""

#: ../../../platforms/tpu.md:89
msgid "Method 4: Cloud TPU with SkyPilot"
msgstr ""

#: ../../../platforms/tpu.md:91
msgid ""
"[SkyPilot](https://github.com/skypilot-org/skypilot) provides simplified "
"deployment on Google Cloud TPU:"
msgstr ""

#: ../../../platforms/tpu.md:93
msgid ""
"Install SkyPilot and configure GCP access (see [SkyPilot documentation]"
"(https://skypilot.readthedocs.io/))"
msgstr ""

#: ../../../platforms/tpu.md:95
msgid "Create a SkyPilot configuration file:"
msgstr ""

#: ../../../platforms/tpu.md:97
msgid ""
"<details>\n"
"<summary>SkyPilot YAML: <code>sglang-jax.sky.yaml</code></summary>\n"
msgstr ""

#: ../../../platforms/tpu.md:100
msgid ""
"# sglang-jax.sky.yaml\n"
"resources:\n"
"   accelerators: tpu-v6e-4\n"
"   accelerator_args:\n"
"      tpu_vm: True\n"
"      runtime_version: v2-alpha-tpuv6e\n"
"\n"
"run: |\n"
"  git clone https://github.com/sgl-project/sglang-jax.git\n"
"  cd sglang-jax\n"
"  uv venv --python 3.12\n"
"  source .venv/bin/activate\n"
"  uv pip install -e \"python[all]\"\n"
msgstr ""

#: ../../../platforms/tpu.md:116
msgid "</details>\n"
msgstr ""

#: ../../../platforms/tpu.md:118
msgid "Launch your TPU cluster:"
msgstr ""

#: ../../../platforms/tpu.md:120
msgid ""
"# Standard deployment\n"
"sky launch -c sglang-jax sglang-jax.sky.yaml --infra=gcp\n"
"\n"
"# With spot instances for cost savings\n"
"sky launch -c sglang-jax sglang-jax.sky.yaml --infra=gcp --use-spot\n"
msgstr ""

#: ../../../platforms/tpu.md:128
msgid "Launch of the Serving Engine"
msgstr ""

#: ../../../platforms/tpu.md:130
msgid "Basic Example: Qwen-7B"
msgstr ""

#: ../../../platforms/tpu.md:132
msgid ""
"JAX_COMPILATION_CACHE_DIR=/tmp/jit_cache python3 -u -m sgl_jax.launch_server "
"\\\n"
"    --model-path Qwen/Qwen-7B-Chat \\\n"
"    --trust-remote-code \\\n"
"    --dist-init-addr=0.0.0.0:10011 \\\n"
"    --nnodes=1 \\\n"
"    --tp-size=4 \\\n"
"    --device=tpu \\\n"
"    --random-seed=3 \\\n"
"    --node-rank=0 \\\n"
"    --mem-fraction-static=0.8 \\\n"
"    --max-prefill-tokens=8192 \\\n"
"    --download-dir=/tmp \\\n"
"    --dtype=bfloat16 \\\n"
"    --skip-server-warmup \\\n"
"    --host 0.0.0.0 \\\n"
"    --port 30000\n"
msgstr ""

#: ../../../platforms/tpu.md:151
msgid "**Key Parameters Explained:**"
msgstr ""

#: ../../../platforms/tpu.md:153
msgid ""
"`JAX_COMPILATION_CACHE_DIR=/tmp/jit_cache` - Enables JIT compilation caching "
"to accelerate server startup on subsequent runs"
msgstr ""

#: ../../../platforms/tpu.md:154
msgid ""
"`--tp-size=4` - Tensor parallelism size; match this to your TPU core count "
"(typically 1, 4, or 8)"
msgstr ""

#: ../../../platforms/tpu.md:155
msgid ""
"`--device=tpu` - Specifies TPU device (this is the default for sglang-jax)"
msgstr ""

#: ../../../platforms/tpu.md:156
msgid ""
"`--dtype=bfloat16` - Uses bfloat16 precision, which TPUs are optimized for"
msgstr ""

#: ../../../platforms/tpu.md:157
msgid ""
"`--mem-fraction-static=0.8` - Allocates 80% of TPU HBM for static memory "
"(adjustable from 0.2 to 0.9)"
msgstr ""

#: ../../../platforms/tpu.md:158
msgid ""
"`--max-prefill-tokens=8192` - Maximum number of tokens processed in the "
"prefill phase"
msgstr ""

#: ../../../platforms/tpu.md:160
msgid "High-Performance Configuration: Qwen3-8B"
msgstr ""

#: ../../../platforms/tpu.md:162
msgid "For production workloads with optimal throughput:"
msgstr ""

#: ../../../platforms/tpu.md:164
msgid ""
"python3 -u -m sgl_jax.launch_server \\\n"
"    --model-path Qwen/Qwen3-8B \\\n"
"    --trust-remote-code \\\n"
"    --tp-size=4 \\\n"
"    --device=tpu \\\n"
"    --mem-fraction-static=0.8 \\\n"
"    --chunked-prefill-size=2048 \\\n"
"    --dtype=bfloat16 \\\n"
"    --max-running-requests=256 \\\n"
"    --page-size=128 \\\n"
"    --attention-backend=fa\n"
msgstr ""

#: ../../../platforms/tpu.md:178
msgid "Advanced: Speculative Decoding (EAGLE3)"
msgstr ""

#: ../../../platforms/tpu.md:180
msgid ""
"Speculative decoding can improve throughput by 20-40% for compatible models:"
msgstr ""

#: ../../../platforms/tpu.md:182
msgid ""
"python3 -u -m sgl_jax.launch_server \\\n"
"    --model-path Qwen/Qwen3-32B \\\n"
"    --trust-remote-code \\\n"
"    --device=tpu \\\n"
"    --tp-size=4 \\\n"
"    --mem-fraction-static=0.8 \\\n"
"    --max-prefill-tokens=4096 \\\n"
"    --attention-backend=fa \\\n"
"    --dtype=bfloat16 \\\n"
"    --port=30000 \\\n"
"    --host=0.0.0.0 \\\n"
"    --disable-overlap-schedule \\\n"
"    --speculative-algorithm=EAGLE3 \\\n"
"    --speculative-draft-model-path=AngelSlim/Qwen3-32B_eagle3 \\\n"
"    --page-size=64 \\\n"
"    --speculative-eagle-topk=1 \\\n"
"    --speculative-num-steps=3 \\\n"
"    --speculative-num-draft-tokens=4\n"
msgstr ""

#: ../../../platforms/tpu.md:203
msgid ""
"**NOTE:** Speculative decoding is currently supported for Qwen3 and LLaMA "
"model families. See the [Speculative Decoding documentation](https://github."
"com/sgl-project/sglang-jax/blob/main/docs/features/speculative_decoding.md) "
"for detailed configuration guidance."
msgstr ""

#: ../../../platforms/tpu.md:206
msgid "Multi-Node Distributed Serving"
msgstr ""

#: ../../../platforms/tpu.md:208
msgid "For large models requiring multiple TPU VMs:"
msgstr ""

#: ../../../platforms/tpu.md:210
msgid ""
"# Node 0 (coordinator)\n"
"python3 -m sgl_jax.launch_server \\\n"
"    --model-path MODEL_PATH \\\n"
"    --dist-init-addr=NODE0_IP:10011 \\\n"
"    --nnodes=2 \\\n"
"    --node-rank=0 \\\n"
"    --tp-size=8 \\\n"
"    [other parameters...]\n"
"\n"
"# Node 1 (worker)\n"
"python3 -m sgl_jax.launch_server \\\n"
"    --model-path MODEL_PATH \\\n"
"    --dist-init-addr=NODE0_IP:10011 \\\n"
"    --nnodes=2 \\\n"
"    --node-rank=1 \\\n"
"    --tp-size=8 \\\n"
"    [other parameters...]\n"
msgstr ""

#: ../../../platforms/tpu.md:230
msgid "Benchmarking with Requests"
msgstr ""

#: ../../../platforms/tpu.md:232
msgid "Throughput Testing"
msgstr ""

#: ../../../platforms/tpu.md:234
msgid "Basic throughput benchmark:"
msgstr ""

#: ../../../platforms/tpu.md:236
msgid ""
"python3 -m sgl_jax.bench_serving \\\n"
"    --backend sgl-jax \\\n"
"    --dataset-name random \\\n"
"    --num-prompts=100 \\\n"
"    --random-input=512 \\\n"
"    --random-output=128 \\\n"
"    --max-concurrency=8 \\\n"
"    --random-range-ratio=1 \\\n"
"    --warmup-requests=0\n"
msgstr ""

#: ../../../platforms/tpu.md:248
msgid "Latency Testing"
msgstr ""

#: ../../../platforms/tpu.md:250
msgid "Measure single-batch latency:"
msgstr ""

#: ../../../platforms/tpu.md:252
msgid ""
"python3 -m sgl_jax.bench_one_batch_server \\\n"
"    --base-url http://127.0.0.1:30000 \\\n"
"    --model-path Qwen/Qwen-7B-Chat \\\n"
"    --batch-size=32 \\\n"
"    --input-len=256 \\\n"
"    --output-len=32\n"
msgstr ""

#: ../../../platforms/tpu.md:261
msgid "Comprehensive Benchmark Script"
msgstr ""

#: ../../../platforms/tpu.md:263
msgid "For systematic performance evaluation across different configurations:"
msgstr ""

#: ../../../platforms/tpu.md:265
msgid ""
"#!/bin/bash\n"
"set -e\n"
"\n"
"backend=${1:-sgl-jax}\n"
"num_prompts_per_concurrency=3\n"
"input_seq_lens=(1024 4096 8192)\n"
"output_seq_lens=(1 1024)\n"
"max_concurrencies=(8 16 32 64 128 256)\n"
"\n"
"for input_seq_len in \"${input_seq_lens[@]}\"; do\n"
"    for output_seq_len in \"${output_seq_lens[@]}\"; do\n"
"        echo \"=======================================\"\n"
"        echo \"Testing ISL/OSL: $input_seq_len/$output_seq_len\"\n"
"        echo \"=======================================\"\n"
"        for max_concurrency in \"${max_concurrencies[@]}\"; do\n"
"            num_prompts=$((num_prompts_per_concurrency * max_concurrency))\n"
"            python3 -m sgl_jax.bench_serving \\\n"
"                --backend ${backend} \\\n"
"                --dataset-name random \\\n"
"                --num-prompts ${num_prompts} \\\n"
"                --random-input ${input_seq_len} \\\n"
"                --random-output ${output_seq_len} \\\n"
"                --max-concurrency ${max_concurrency} \\\n"
"                --random-range-ratio 1 \\\n"
"                --disable-ignore-eos \\\n"
"                --warmup-requests 0\n"
"        done\n"
"    done\n"
"done\n"
msgstr ""

#: ../../../platforms/tpu.md:297
msgid "For detailed help on all benchmark parameters:"
msgstr ""

#: ../../../platforms/tpu.md:299
msgid "python3 -m sgl_jax.bench_serving --help\n"
msgstr ""

#: ../../../platforms/tpu.md:303
msgid ""
"See the [Benchmark and Profiling Guide](https://github.com/sgl-project/"
"sglang-jax/blob/main/docs/developer_guide/benchmark_and_profiling.md) for "
"advanced benchmarking techniques and profiling with JAX Profiler."
msgstr ""

#: ../../../platforms/tpu.md:305
msgid "Performance Optimization"
msgstr ""

#: ../../../platforms/tpu.md:307
msgid "Memory Optimization"
msgstr ""

#: ../../../platforms/tpu.md:309
msgid "**Reduce memory usage:**"
msgstr ""

#: ../../../platforms/tpu.md:310
msgid "Lower `--mem-fraction-static` (from 0.8 ‚Üí 0.5 ‚Üí 0.3)"
msgstr ""

#: ../../../platforms/tpu.md:311
msgid "Decrease `--max-prefill-tokens` (from 16384 ‚Üí 8192 ‚Üí 4096)"
msgstr ""

#: ../../../platforms/tpu.md:312
msgid "Reduce `--max-running-requests`"
msgstr ""

#: ../../../platforms/tpu.md:314
msgid "**Handle OOM errors:**"
msgstr ""

#: ../../../platforms/tpu.md:315
msgid "Start with conservative memory settings (`--mem-fraction-static=0.5`)"
msgstr ""

#: ../../../platforms/tpu.md:316
msgid "Gradually increase until you find the optimal balance"
msgstr ""

#: ../../../platforms/tpu.md:317
msgid "Increase `--page-size` for better memory locality (1 ‚Üí 16 ‚Üí 64 ‚Üí 128)"
msgstr ""

#: ../../../platforms/tpu.md:319
msgid "Throughput Optimization"
msgstr ""

#: ../../../platforms/tpu.md:321
msgid "To maximize tokens per second:"
msgstr ""

#: ../../../platforms/tpu.md:323
msgid "Use FlashAttention backend: `--attention-backend=fa`"
msgstr ""

#: ../../../platforms/tpu.md:324
msgid ""
"Enable speculative decoding (EAGLE3) for Qwen3 models (20-40% improvement)"
msgstr ""

#: ../../../platforms/tpu.md:325
msgid "Increase `--max-running-requests` to 256+"
msgstr ""

#: ../../../platforms/tpu.md:326
msgid "Set `--mem-fraction-static` to 0.8+ (if memory allows)"
msgstr ""

#: ../../../platforms/tpu.md:327
msgid "Use larger page sizes (64-128)"
msgstr ""

#: ../../../platforms/tpu.md:328
msgid "Enable chunked prefill: `--chunked-prefill-size=2048`"
msgstr ""

#: ../../../platforms/tpu.md:330
msgid "Latency Optimization"
msgstr ""

#: ../../../platforms/tpu.md:332
msgid "To minimize time-to-first-token (TTFT) and inter-token latency:"
msgstr ""

#: ../../../platforms/tpu.md:334
msgid "Reduce `--page-size` to 1-4"
msgstr ""

#: ../../../platforms/tpu.md:335
msgid "Lower `--max-running-requests` (16-32) for smaller batches"
msgstr ""

#: ../../../platforms/tpu.md:336
msgid "Reduce `--chunked-prefill-size`"
msgstr ""

#: ../../../platforms/tpu.md:337
msgid "Use conservative memory settings to avoid GC pauses"
msgstr ""

#: ../../../platforms/tpu.md:339
msgid "TPU-Specific Optimizations"
msgstr ""

#: ../../../platforms/tpu.md:341
msgid "**JIT Compilation Cache:**"
msgstr ""

#: ../../../platforms/tpu.md:342
msgid "export JAX_COMPILATION_CACHE_DIR=/tmp/jit_cache\n"
msgstr ""

#: ../../../platforms/tpu.md:345
msgid ""
"Always set this environment variable to cache compiled kernels and "
"accelerate server startup."
msgstr ""

#: ../../../platforms/tpu.md:347
msgid ""
"**Data Type Optimization:** Use `--dtype=bfloat16` for TPU native "
"optimization. TPUs are specifically designed for bfloat16 computations."
msgstr ""

#: ../../../platforms/tpu.md:350
msgid ""
"**Tensor Parallelism:** Match `--tp-size` to your TPU core configuration (1, "
"4, or 8) for optimal model distribution."
msgstr ""

#: ../../../platforms/tpu.md:353
msgid ""
"**Attention Backend:** Always use `--attention-backend=fa` (FlashAttention) "
"for production workloads."
msgstr ""

#: ../../../platforms/tpu.md:356
msgid "Troubleshooting"
msgstr ""

#: ../../../platforms/tpu.md:358
msgid "OOM (Out of Memory) Errors"
msgstr ""

#: ../../../platforms/tpu.md:360
msgid "If you encounter out-of-memory errors:"
msgstr ""

#: ../../../platforms/tpu.md:362
msgid "Reduce `--mem-fraction-static` from 0.8 to 0.5 or lower"
msgstr ""

#: ../../../platforms/tpu.md:363
msgid "Decrease `--max-prefill-tokens` from 8192 to 4096 or 2048"
msgstr ""

#: ../../../platforms/tpu.md:364
msgid "Lower `--max-running-requests` to reduce concurrent batch size"
msgstr ""

#: ../../../platforms/tpu.md:365
msgid "Increase `--page-size` for better memory layout efficiency"
msgstr ""

#: ../../../platforms/tpu.md:367
msgid "Compilation Long-Time"
msgstr ""

#: ../../../platforms/tpu.md:369
msgid "If the server takes too long to start:"
msgstr ""

#: ../../../platforms/tpu.md:371
msgid "Ensure `JAX_COMPILATION_CACHE_DIR` is properly set"
msgstr ""

#: ../../../platforms/tpu.md:372
msgid "Understand that the first run requires JIT compilation (this is normal)"
msgstr ""

#: ../../../platforms/tpu.md:373
msgid "Subsequent runs will be significantly faster with cached compilations"
msgstr ""

#: ../../../platforms/tpu.md:374
msgid ""
"Consider using `--skip-server-warmup` to defer compilation until first "
"request"
msgstr ""

#: ../../../platforms/tpu.md:376
msgid "Low Throughput"
msgstr ""

#: ../../../platforms/tpu.md:378
msgid "If you're not achieving expected throughput:"
msgstr ""

#: ../../../platforms/tpu.md:380
msgid "Verify `--tp-size` matches your TPU core configuration"
msgstr ""

#: ../../../platforms/tpu.md:381
msgid "Check that `--attention-backend=fa` is enabled"
msgstr ""

#: ../../../platforms/tpu.md:382
msgid "Increase `--max-running-requests` to enable larger batch formation"
msgstr ""

#: ../../../platforms/tpu.md:383
msgid "Consider enabling speculative decoding for compatible models"
msgstr ""

#: ../../../platforms/tpu.md:384
msgid "Ensure memory settings allow for sufficient batch sizes"
msgstr ""

#: ../../../platforms/tpu.md:386
msgid "Connection Issues"
msgstr ""

#: ../../../platforms/tpu.md:388
msgid "If clients cannot connect to the server:"
msgstr ""

#: ../../../platforms/tpu.md:390
msgid "Ensure `--host=0.0.0.0` for external access (not just `127.0.0.1`)"
msgstr ""

#: ../../../platforms/tpu.md:391
msgid ""
"Verify firewall rules allow traffic on the specified port (default: 30000)"
msgstr ""

#: ../../../platforms/tpu.md:392
msgid ""
"Check that the server process is running: `curl http://localhost:30000/"
"health`"
msgstr ""

#: ../../../platforms/tpu.md:394
msgid "Advanced Features"
msgstr ""

#: ../../../platforms/tpu.md:396
msgid "Speculative Decoding"
msgstr ""

#: ../../../platforms/tpu.md:398
msgid ""
"SGLang-JAX supports EAGLE and EAGLE3 speculative decoding algorithms for "
"Qwen3 and LLaMA model families. Speculative decoding can improve throughput "
"by 20-40% without affecting output quality."
msgstr ""

#: ../../../platforms/tpu.md:400
msgid ""
"See the [Speculative Decoding documentation](https://github.com/sgl-project/"
"sglang-jax/blob/main/docs/features/speculative_decoding.md) for detailed "
"configuration and supported model combinations."
msgstr ""

#: ../../../platforms/tpu.md:404
msgid "Enable mixed prefill-decode batching for better TPU utilization:"
msgstr ""

#: ../../../platforms/tpu.md:406
msgid "--chunked-prefill-size=2048 --enable-mixed-chunk\n"
msgstr ""

#: ../../../platforms/tpu.md:410
msgid ""
"This allows the scheduler to mix prefill operations with decode operations "
"in the same batch, improving overall throughput."
msgstr ""

#: ../../../platforms/tpu.md:412
msgid "Custom Attention Backends"
msgstr ""

#: ../../../platforms/tpu.md:414
msgid ""
"SGLang-JAX supports a plugin-based attention backend system. You can "
"implement custom attention kernels optimized for specific use cases."
msgstr ""

#: ../../../platforms/tpu.md:416
msgid ""
"See the [Attention Backend documentation](https://github.com/sgl-project/"
"sglang-jax/blob/main/docs/features/attention_backend.md) for implementation "
"details."
msgstr ""

#: ../../../platforms/tpu.md:418
msgid "Environment Verification"
msgstr ""

#: ../../../platforms/tpu.md:420
msgid "Verify your TPU setup before deploying:"
msgstr ""

#: ../../../platforms/tpu.md:422
msgid "python -c \"from sgl_jax import check_env; check_env.check_env()\"\n"
msgstr ""

#: ../../../platforms/tpu.md:426
msgid "This command checks:"
msgstr ""

#: ../../../platforms/tpu.md:427
msgid "Installed package versions"
msgstr ""

#: ../../../platforms/tpu.md:428
msgid "TPU device availability and specifications"
msgstr ""

#: ../../../platforms/tpu.md:429
msgid "System resources and configuration"
msgstr ""

#: ../../../platforms/tpu.md:430
msgid "Compatibility of settings"
msgstr ""

#: ../../../platforms/tpu.md:432
msgid "Contributing"
msgstr ""

#: ../../../platforms/tpu.md:434
msgid "We welcome contributions to improve TPU support in SGLang-JAX!"
msgstr ""

#: ../../../platforms/tpu.md:436
msgid "Areas for Contribution"
msgstr ""

#: ../../../platforms/tpu.md:438
msgid ""
"**Check the [Development Roadmap](https://github.com/sgl-project/sglang-jax/"
"issues/190)** to see planned features and find opportunities to contribute "
"new functionality."
msgstr ""

#: ../../../platforms/tpu.md:440
msgid "Current contribution areas include:"
msgstr ""

#: ../../../platforms/tpu.md:442
msgid "Performance optimizations for specific TPU generations"
msgstr ""

#: ../../../platforms/tpu.md:443
msgid "Support for additional model architectures"
msgstr ""

#: ../../../platforms/tpu.md:444
msgid "Documentation improvements and examples"
msgstr ""

#: ../../../platforms/tpu.md:445
msgid "Bug reports and fixes"
msgstr ""

#: ../../../platforms/tpu.md:446
msgid "Benchmark results and performance analysis"
msgstr ""

#: ../../../platforms/tpu.md:448
msgid "How to Contribute"
msgstr ""

#: ../../../platforms/tpu.md:450
msgid ""
"Visit the [sglang-jax repository](https://github.com/sgl-project/sglang-jax)"
msgstr ""

#: ../../../platforms/tpu.md:451
msgid ""
"Read the [Contribution Guide](https://github.com/sgl-project/sglang-jax/blob/"
"main/docs/developer_guide/contribution_guide.md)"
msgstr ""

#: ../../../platforms/tpu.md:452
msgid ""
"Join the [SGL-JAX Slack community](https://sgl-fru7574.slack.com/archives/"
"C09EBE5HT5X) for discussions"
msgstr ""

#: ../../../platforms/tpu.md:453
msgid ""
"Report issues at [sglang-jax/issues](https://github.com/sgl-project/sglang-"
"jax/issues)"
msgstr ""

#: ../../../platforms/tpu.md:455
msgid "Testing on TPU"
msgstr ""

#: ../../../platforms/tpu.md:457
msgid "For contributors who need TPU access for testing:"
msgstr ""

#: ../../../platforms/tpu.md:459
msgid ""
"Refer to the [TPU Resources Guide](https://github.com/sgl-project/sglang-jax/"
"blob/main/docs/developer_guide/tpu_resources_guide.md) for information on "
"accessing TPU hardware"
msgstr ""

#: ../../../platforms/tpu.md:460
msgid "Use SkyPilot with spot instances for cost-effective testing"
msgstr ""

#: ../../../platforms/tpu.md:461
msgid ""
"Follow the [Benchmark and Profiling Guide](https://github.com/sgl-project/"
"sglang-jax/blob/main/docs/developer_guide/benchmark_and_profiling.md) for "
"performance validation"
msgstr ""

#: ../../../platforms/tpu.md:463
msgid "References"
msgstr ""

#: ../../../platforms/tpu.md:465
msgid "Documentation"
msgstr ""

#: ../../../platforms/tpu.md:467
msgid "[SGLang-JAX Repository](https://github.com/sgl-project/sglang-jax)"
msgstr ""

#: ../../../platforms/tpu.md:468
msgid ""
"[SGLang-JAX Installation Guide](https://github.com/sgl-project/sglang-jax/"
"blob/main/docs/get_started/install.md)"
msgstr ""

#: ../../../platforms/tpu.md:469
msgid ""
"[Qwen Models Quick Start](https://github.com/sgl-project/sglang-jax/blob/"
"main/docs/basic_usage/qwen.md)"
msgstr ""

#: ../../../platforms/tpu.md:470
msgid ""
"[Benchmark and Profiling Guide](https://github.com/sgl-project/sglang-jax/"
"blob/main/docs/developer_guide/benchmark_and_profiling.md)"
msgstr ""

#: ../../../platforms/tpu.md:471
msgid ""
"[Speculative Decoding](https://github.com/sgl-project/sglang-jax/blob/main/"
"docs/features/speculative_decoding.md)"
msgstr ""

#: ../../../platforms/tpu.md:473
msgid "External Resources"
msgstr ""

#: ../../../platforms/tpu.md:475
msgid "[JAX Documentation](https://jax.readthedocs.io/)"
msgstr ""

#: ../../../platforms/tpu.md:476
msgid "[Google Cloud TPU Documentation](https://cloud.google.com/tpu/docs)"
msgstr ""

#: ../../../platforms/tpu.md:477
msgid "[SkyPilot Documentation](https://skypilot.readthedocs.io/)"
msgstr ""
