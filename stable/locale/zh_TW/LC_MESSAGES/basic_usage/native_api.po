# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang stable\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-06 08:37+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../basic_usage/native_api.ipynb:2
msgid ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"
msgstr ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"

#: ../../../basic_usage/native_api.ipynb:9
msgid "SGLang Native APIs"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:11
msgid ""
"Apart from the OpenAI compatible APIs, the SGLang Runtime also provides its "
"native server APIs. We introduce the following APIs:"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:13
msgid "``/generate`` (text generation model)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:14
msgid "``/get_model_info``"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:15
msgid "``/get_server_info``"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:16
msgid "``/health``"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:17
msgid "``/health_generate``"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:18
msgid "``/flush_cache``"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:19
msgid "``/update_weights``"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:20
msgid "``/encode``\\ (embedding model)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:21
msgid "``/v1/rerank``\\ (cross encoder rerank model)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:22
msgid "``/classify``\\ (reward model)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:23
msgid "``/start_expert_distribution_record``"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:24
msgid "``/stop_expert_distribution_record``"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:25
msgid "``/dump_expert_distribution_record``"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:26
msgid "``/tokenize``"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:27
msgid "``/detokenize``"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:28
msgid ""
"A full list of these APIs can be found at `http_server.py <https://github."
"com/sgl-project/sglang/blob/main/python/sglang/srt/entrypoints/http_server."
"py>`__"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:30
msgid ""
"We mainly use ``requests`` to test these APIs in the following examples. You "
"can also use ``curl``."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:42
msgid "Launch A Server"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid "[ ]:"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"from sglang.test.doc_patch import launch_server_cmd\n"
"from sglang.utils import wait_for_server, print_highlight, "
"terminate_process\n"
"\n"
"server_process, port = launch_server_cmd(\n"
"    \"python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-"
"instruct --host 0.0.0.0 --log-level warning\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:70
msgid "Generate (text generation model)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:72
msgid ""
"Generate completions. This is similar to the ``/v1/completions`` in OpenAI "
"API. Detailed parameters can be found in the `sampling parameters "
"<sampling_params.md>`__."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"import requests\n"
"\n"
"url = f\"http://localhost:{port}/generate\"\n"
"data = {\"text\": \"What is the capital of France?\"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"print_highlight(response.json())"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:99
msgid "Get Model Info"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:101
msgid "Get the information of the model."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:103
msgid "``model_path``: The path/name of the model."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:104
msgid ""
"``is_generation``: Whether the model is used as generation model or "
"embedding model."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:105
msgid "``tokenizer_path``: The path/name of the tokenizer."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:106
msgid ""
"``preferred_sampling_params``: The default sampling params specified via ``--"
"preferred-sampling-params``. ``None`` is returned in this example as we did "
"not explicitly configure it in server args."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:107
msgid ""
"``weight_version``: This field contains the version of the model weights. "
"This is often used to track changes or updates to the modelâ€™s trained "
"parameters."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:108
msgid ""
"``has_image_understanding``: Whether the model has image-understanding "
"capability."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:109
msgid ""
"``has_audio_understanding``: Whether the model has audio-understanding "
"capability."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:110
msgid ""
"``model_type``: The model type from the HuggingFace config (e.g., \"qwen2\", "
"\"llama\")."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:111
msgid ""
"``architectures``: The model architectures from the HuggingFace config (e."
"g., [\"Qwen2ForCausalLM\"])."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"url = f\"http://localhost:{port}/get_model_info\"\n"
"\n"
"response = requests.get(url)\n"
"response_json = response.json()\n"
"print_highlight(response_json)\n"
"assert response_json[\"model_path\"] == \"qwen/qwen2.5-0.5b-instruct\"\n"
"assert response_json[\"is_generation\"] is True\n"
"assert response_json[\"tokenizer_path\"] == \"qwen/qwen2.5-0.5b-instruct\"\n"
"assert response_json[\"preferred_sampling_params\"] is None\n"
"assert response_json.keys() == {\n"
"    \"model_path\",\n"
"    \"is_generation\",\n"
"    \"tokenizer_path\",\n"
"    \"preferred_sampling_params\",\n"
"    \"weight_version\",\n"
"    \"has_image_understanding\",\n"
"    \"has_audio_understanding\",\n"
"    \"model_type\",\n"
"    \"architectures\",\n"
"}"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:151
msgid "Get Server Info"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:153
msgid ""
"Gets the server information including CLI arguments, token limits, and "
"memory pool sizes."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:155
msgid "Note: ``get_server_info`` merges the following deprecated endpoints:"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:157
msgid "``get_server_args``"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:158
msgid "``get_memory_pool_size``"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:159
msgid "``get_max_total_num_tokens``"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"url = f\"http://localhost:{port}/get_server_info\"\n"
"\n"
"response = requests.get(url)\n"
"print_highlight(response.text)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:183
msgid "Health Check"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:185
msgid "``/health``: Check the health of the server."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:186
msgid ""
"``/health_generate``: Check the health of the server by generating one token."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"url = f\"http://localhost:{port}/health_generate\"\n"
"\n"
"response = requests.get(url)\n"
"print_highlight(response.text)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"url = f\"http://localhost:{port}/health\"\n"
"\n"
"response = requests.get(url)\n"
"print_highlight(response.text)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:222
msgid "Flush Cache"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:224
msgid ""
"Flush the radix cache. It will be automatically triggered when the model "
"weights are updated by the ``/update_weights`` API."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"url = f\"http://localhost:{port}/flush_cache\"\n"
"\n"
"response = requests.post(url)\n"
"print_highlight(response.text)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:248
msgid "Update Weights From Disk"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:250
msgid ""
"Update model weights from disk without restarting the server. Only "
"applicable for models with the same architecture and parameter size."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:252
msgid ""
"SGLang support ``update_weights_from_disk`` API for continuous evaluation "
"during training (save checkpoint to disk and update weights from disk)."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"# successful update with same architecture and size\n"
"\n"
"url = f\"http://localhost:{port}/update_weights_from_disk\"\n"
"data = {\"model_path\": \"qwen/qwen2.5-0.5b-instruct\"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"print_highlight(response.text)\n"
"assert response.json()[\"success\"] is True\n"
"assert response.json()[\"message\"] == \"Succeeded to update model weights.\""
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"# failed update with different parameter size or wrong name\n"
"\n"
"url = f\"http://localhost:{port}/update_weights_from_disk\"\n"
"data = {\"model_path\": \"qwen/qwen2.5-0.5b-instruct-wrong\"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"response_json = response.json()\n"
"print_highlight(response_json)\n"
"assert response_json[\"success\"] is False\n"
"assert response_json[\"message\"] == (\n"
"    \"Failed to get weights iterator: \"\n"
"    \"qwen/qwen2.5-0.5b-instruct-wrong\"\n"
"    \" (repository not found).\"\n"
")"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid "terminate_process(server_process)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:312
msgid "Encode (embedding model)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:314
msgid ""
"Encode text into embeddings. Note that this API is only available for "
"`embedding models <openai_api_embeddings.ipynb>`__ and will raise an error "
"for generation models. Therefore, we launch a new server to server an "
"embedding model."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"embedding_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model-path Alibaba-NLP/gte-Qwen2-1.5B-"
"instruct \\\n"
"    --host 0.0.0.0 --is-embedding --log-level warning\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"# successful encode for embedding model\n"
"\n"
"url = f\"http://localhost:{port}/encode\"\n"
"data = {\"model\": \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\", \"text\": \"Once "
"upon a time\"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"response_json = response.json()\n"
"print_highlight(f\"Text embedding (first 10): {response_json['embedding']"
"[:10]}\")"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid "terminate_process(embedding_process)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:367
msgid "v1/rerank (cross encoder rerank model)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:369
msgid ""
"Rerank a list of documents given a query using a cross-encoder model. Note "
"that this API is only available for cross encoder model like `BAAI/bge-"
"reranker-v2-m3 <https://huggingface.co/BAAI/bge-reranker-v2-m3>`__ with "
"``attention-backend`` ``triton`` and ``torch_native``."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"reranker_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model-path BAAI/bge-reranker-v2-m3 \\\n"
"    --host 0.0.0.0 --disable-radix-cache --chunked-prefill-size -1 --"
"attention-backend triton --is-embedding --log-level warning\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"# compute rerank scores for query and documents\n"
"\n"
"url = f\"http://localhost:{port}/v1/rerank\"\n"
"data = {\n"
"    \"model\": \"BAAI/bge-reranker-v2-m3\",\n"
"    \"query\": \"what is panda?\",\n"
"    \"documents\": [\n"
"        \"hi\",\n"
"        \"The giant panda (Ailuropoda melanoleuca), sometimes called a panda "
"bear or simply panda, is a bear species endemic to China.\",\n"
"    ],\n"
"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"response_json = response.json()\n"
"for item in response_json:\n"
"    print_highlight(f\"Score: {item['score']:.2f} - Document: "
"'{item['document']}'\")"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid "terminate_process(reranker_process)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:430
msgid "Classify (reward model)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:432
msgid ""
"SGLang Runtime also supports reward models. Here we use a reward model to "
"classify the quality of pairwise generations."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"# Note that SGLang now treats embedding models and reward models as the same "
"type of models.\n"
"# This will be updated in the future.\n"
"\n"
"reward_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model-path Skywork/Skywork-Reward-"
"Llama-3.1-8B-v0.2 --host 0.0.0.0 --is-embedding --log-level warning\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"from transformers import AutoTokenizer\n"
"\n"
"PROMPT = (\n"
"    \"What is the range of the numeric output of a sigmoid node in a neural "
"network?\"\n"
")\n"
"\n"
"RESPONSE1 = \"The output of a sigmoid node is bounded between -1 and 1.\"\n"
"RESPONSE2 = \"The output of a sigmoid node is bounded between 0 and 1.\"\n"
"\n"
"CONVS = [\n"
"    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", "
"\"content\": RESPONSE1}],\n"
"    [{\"role\": \"user\", \"content\": PROMPT}, {\"role\": \"assistant\", "
"\"content\": RESPONSE2}],\n"
"]\n"
"\n"
"tokenizer = AutoTokenizer.from_pretrained(\"Skywork/Skywork-Reward-"
"Llama-3.1-8B-v0.2\")\n"
"prompts = tokenizer.apply_chat_template(CONVS, tokenize=False, "
"return_dict=False)\n"
"\n"
"url = f\"http://localhost:{port}/classify\"\n"
"data = {\"model\": \"Skywork/Skywork-Reward-Llama-3.1-8B-v0.2\", \"text\": "
"prompts}\n"
"\n"
"responses = requests.post(url, json=data).json()\n"
"for response in responses:\n"
"    print_highlight(f\"reward: {response['embedding'][0]}\")"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid "terminate_process(reward_process)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:502
msgid "Capture expert selection distribution in MoE models"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:504
msgid ""
"SGLang Runtime supports recording the number of times an expert is selected "
"in a MoE model run for each expert in the model. This is useful when "
"analyzing the throughput of the model and plan for optimization."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:506
msgid ""
"*Note: We only print out the first 10 lines of the csv below for better "
"readability. Please adjust accordingly if you want to analyze the results "
"more deeply.*"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"expert_record_server_process, port = launch_server_cmd(\n"
"    \"python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --"
"host 0.0.0.0 --expert-distribution-recorder-mode stat --log-level warning\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"response = requests.post(f\"http://localhost:{port}/"
"start_expert_distribution_record\")\n"
"print_highlight(response)\n"
"\n"
"url = f\"http://localhost:{port}/generate\"\n"
"data = {\"text\": \"What is the capital of France?\"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"print_highlight(response.json())\n"
"\n"
"response = requests.post(f\"http://localhost:{port}/"
"stop_expert_distribution_record\")\n"
"print_highlight(response)\n"
"\n"
"response = requests.post(f\"http://localhost:{port}/"
"dump_expert_distribution_record\")\n"
"print_highlight(response)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid "terminate_process(expert_record_server_process)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:562
msgid "Tokenize/Detokenize Example (Round Trip)"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:564
msgid ""
"This example demonstrates how to use the /tokenize and /detokenize endpoints "
"together. We first tokenize a string, then detokenize the resulting IDs to "
"reconstruct the original text. This workflow is useful when you need to "
"handle tokenization externally but still leverage the server for "
"detokenization."
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"tokenizer_free_server_process, port = launch_server_cmd(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-instruct\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid ""
"import requests\n"
"from sglang.utils import print_highlight\n"
"\n"
"base_url = f\"http://localhost:{port}\"\n"
"tokenize_url = f\"{base_url}/tokenize\"\n"
"detokenize_url = f\"{base_url}/detokenize\"\n"
"\n"
"model_name = \"qwen/qwen2.5-0.5b-instruct\"\n"
"input_text = \"SGLang provides efficient tokenization endpoints.\"\n"
"print_highlight(f\"Original Input Text:\\n'{input_text}'\")\n"
"\n"
"# --- tokenize the input text ---\n"
"tokenize_payload = {\n"
"    \"model\": model_name,\n"
"    \"prompt\": input_text,\n"
"    \"add_special_tokens\": False,\n"
"}\n"
"try:\n"
"    tokenize_response = requests.post(tokenize_url, json=tokenize_payload)\n"
"    tokenize_response.raise_for_status()\n"
"    tokenization_result = tokenize_response.json()\n"
"    token_ids = tokenization_result.get(\"tokens\")\n"
"\n"
"    if not token_ids:\n"
"        raise ValueError(\"Tokenization returned empty tokens.\")\n"
"\n"
"    print_highlight(f\"\\nTokenized Output (IDs):\\n{token_ids}\")\n"
"    print_highlight(f\"Token Count: {tokenization_result.get('count')}\")\n"
"    print_highlight(f\"Max Model Length: {tokenization_result."
"get('max_model_len')}\")\n"
"\n"
"    # --- detokenize the obtained token IDs ---\n"
"    detokenize_payload = {\n"
"        \"model\": model_name,\n"
"        \"tokens\": token_ids,\n"
"        \"skip_special_tokens\": True,\n"
"    }\n"
"\n"
"    detokenize_response = requests.post(detokenize_url, "
"json=detokenize_payload)\n"
"    detokenize_response.raise_for_status()\n"
"    detokenization_result = detokenize_response.json()\n"
"    reconstructed_text = detokenization_result.get(\"text\")\n"
"\n"
"    print_highlight(f\"\\nDetokenized Output (Text):"
"\\n'{reconstructed_text}'\")\n"
"\n"
"    if input_text == reconstructed_text:\n"
"        print_highlight(\n"
"            \"\\nRound Trip Successful: Original and reconstructed text "
"match.\"\n"
"        )\n"
"    else:\n"
"        print_highlight(\n"
"            \"\\nRound Trip Mismatch: Original and reconstructed text differ."
"\"\n"
"        )\n"
"\n"
"except requests.exceptions.RequestException as e:\n"
"    print_highlight(f\"\\nHTTP Request Error: {e}\")\n"
"except Exception as e:\n"
"    print_highlight(f\"\\nAn error occurred: {e}\")"
msgstr ""

#: ../../../basic_usage/native_api.ipynb:-1
msgid "terminate_process(tokenizer_free_server_process)"
msgstr ""
