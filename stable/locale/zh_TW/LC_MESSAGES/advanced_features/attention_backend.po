# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang stable\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-09 11:06+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../advanced_features/attention_backend.md:1
msgid "Attention Backend"
msgstr ""

#: ../../../advanced_features/attention_backend.md:3
msgid ""
"SGLang supports a large variety of attention backends. Each of them has "
"different pros and cons. You can test them according to your needs."
msgstr ""

#: ../../../advanced_features/attention_backend.md:7
msgid ""
"Selecting an optimal attention backend is crucial for maximizing your "
"performance. Different backends excel in various scenarios, so choose based "
"on your model, hardware, and use case. Not all backends are supported on all "
"platforms and model architectures."
msgstr ""

#: ../../../advanced_features/attention_backend.md:10
msgid "Support Matrix"
msgstr ""

#: ../../../advanced_features/attention_backend.md:12
msgid ""
"The support matrix is split into two parts: MHA (standard attention) and MLA "
"(multi-head latent attention). For an explanation of the key differences "
"between MHA and MLA, please see the [SGLang documentation on DeepSeek MLA]"
"(https://github.com/sgl-project/sglang/blob/main/docs/basic_usage/deepseek."
"md#multi-head-latent-attention-mla) and the original [DeepSeek MLA paper]"
"(https://arxiv.org/pdf/2405.04434)."
msgstr ""

#: ../../../advanced_features/attention_backend.md:14
msgid "MHA Backends"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Backend**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Page Size > 1 (native)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FP8 KV Cache**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Spec topk=1**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Spec topk>1**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Sliding Window**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**MultiModal**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FlashInfer**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "✅"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "❌"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FA3 (FlashAttention 3)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FA4 (FlashAttention 4)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Triton**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Torch Native (SDPA)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FlexAttention (PyTorch)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**TRTLLM MHA**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "16, 32 or 64"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Dual Chunk FlashAttention**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**AITER (ROCm)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Wave (ROCm)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Ascend (NPU)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Intel XPU**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:31
msgid "MLA Backends"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Native Page Sizes**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Chunked Prefix Cache**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FlashInfer MLA**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "1"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FlashMLA**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "64"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Cutlass MLA**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "128"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**TRTLLM MLA (Blackwell)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "32 or 64"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "n/a"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "⚠️ (page_size=1 only)"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**FA4**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:6
msgid "**Ascend MLA (NPU)**"
msgstr ""

#: ../../../advanced_features/attention_backend.md:45
msgid ""
"Multimodal attention is selected by `--mm-attention-backend`. The "
"\"MultiModal\" column indicates whether a corresponding multimodal "
"implementation exists for that backend family."
msgstr ""

#: ../../../advanced_features/attention_backend.md:49
msgid ""
"FlashMLA FP8 KV cache is currently not working. See upstream issue [#8856]"
"(https://github.com/sgl-project/sglang/pull/8856). Use non-FP8 KV or another "
"backend when FP8 KV cache is required."
msgstr ""

#: ../../../advanced_features/attention_backend.md:53
msgid "FlashAttention 4 is prefill-only for now."
msgstr ""

#: ../../../advanced_features/attention_backend.md:54
msgid ""
"NSA is specifically designed for [DeepSeek V3.2 DSA](https://lmsys.org/"
"blog/2025-09-29-deepseek-V32/)."
msgstr ""

#: ../../../advanced_features/attention_backend.md:58
msgid ""
"Speculative decoding topk: `topk` is the number of draft tokens sampled per "
"step from the draft model. `topk = 1` follows classic EAGLE; `topk > 1` "
"explores multiple branches and requires backend support in both draft and "
"verification paths."
msgstr ""

#: ../../../advanced_features/attention_backend.md:61
msgid ""
"Note: Many backends that do not natively operate on pages can emulate "
"`page_size > 1` at the wrapper layer by expanding page tables to per-token "
"indices. The \"Page Size > 1 (native)\" column indicates true in-kernel "
"paging. Some backends require fixed native page sizes and cannot be reduced/"
"emulated differently: TRTLLM MHA (16/32/64), TRTLLM MLA (32/64), FlashMLA "
"(64), Cutlass MLA (128), FA4 (128), Ascend (128)."
msgstr ""

#: ../../../advanced_features/attention_backend.md:63
msgid "MLA page-size constraints:"
msgstr ""

#: ../../../advanced_features/attention_backend.md:64
msgid "FlashInfer MLA: page_size = 1."
msgstr ""

#: ../../../advanced_features/attention_backend.md:65
msgid "FlashMLA: page_size = 64."
msgstr ""

#: ../../../advanced_features/attention_backend.md:66
msgid "Cutlass MLA: page_size = 128."
msgstr ""

#: ../../../advanced_features/attention_backend.md:67
msgid "TRTLLM MLA: page_size ∈ {32, 64}."
msgstr ""

#: ../../../advanced_features/attention_backend.md:68
msgid "FA4: page_size = 128."
msgstr ""

#: ../../../advanced_features/attention_backend.md:70
msgid ""
"Hybrid attention (different backends for prefill vs decode) (Experimental)"
msgstr ""

#: ../../../advanced_features/attention_backend.md:73
msgid "Hybrid attention is an experimental feature."
msgstr ""

#: ../../../advanced_features/attention_backend.md:76
msgid ""
"You can mix-and-match attention backends for prefill and decode. This is "
"useful when one backend excels at prefill and another excels at decode. For "
"the implementation details, please see `python/sglang/srt/layers/attention/"
"hybrid_attn_backend.py`."
msgstr ""

#: ../../../advanced_features/attention_backend.md:78
msgid ""
"# Example: Prefill with FA4, Decode with TRTLLM MLA (Blackwell)\n"
"python3 -m sglang.launch_server \\\n"
"  --model-path nvidia/DeepSeek-R1-FP4 \\\n"
"  --tp 8 \\\n"
"  --attention-backend trtllm_mla \\\n"
"  --moe-runner-backend flashinfer_trtllm \\\n"
"  --quantization modelopt_fp4 \\\n"
"  --prefill-attention-backend fa4\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:89
msgid "Speculative decoding with hybrid attention"
msgstr ""

#: ../../../advanced_features/attention_backend.md:91
msgid ""
"Hybrid attention also works with speculative decoding. The backend used for "
"draft decoding and target verification depends on `--speculative-attention-"
"mode`:"
msgstr ""

#: ../../../advanced_features/attention_backend.md:93
msgid ""
"`--speculative-attention-mode decode` (recommended): draft/verify use the "
"decode backend."
msgstr ""

#: ../../../advanced_features/attention_backend.md:94
msgid ""
"`--speculative-attention-mode prefill` (default): draft/verify use the "
"prefill backend."
msgstr ""

#: ../../../advanced_features/attention_backend.md:96
msgid "Constraints when combining hybrid attention with speculative decoding:"
msgstr ""

#: ../../../advanced_features/attention_backend.md:98
msgid ""
"If any attention backend is `trtllm_mha`, speculative decoding supports only "
"`--speculative-eagle-topk 1`."
msgstr ""

#: ../../../advanced_features/attention_backend.md:99
msgid ""
"For paged MHA backends with `--page-size > 1` and `--speculative-eagle-topk "
"> 1`, only `flashinfer` is supported."
msgstr ""

#: ../../../advanced_features/attention_backend.md:100
msgid "`flex_attention` is not supported with speculative decoding."
msgstr ""

#: ../../../advanced_features/attention_backend.md:101
msgid ""
"For MLA backends, `trtllm_mla` supports `topk > 1`; `flashmla` and "
"`flashinfer_mla` support only `topk = 1`."
msgstr ""

#: ../../../advanced_features/attention_backend.md:102
msgid ""
"CUDA Graph: the decode backend is always captured; the prefill backend is "
"captured only when `--speculative-attention-mode prefill`."
msgstr ""

#: ../../../advanced_features/attention_backend.md:106
msgid ""
"If you set only one of `--prefill-attention-backend` or `--decode-attention-"
"backend`, the unspecified phase inherits `--attention-backend`. If both are "
"specified and differ, SGLang automatically enables a hybrid wrapper to "
"dispatch to the chosen backend per phase."
msgstr ""

#: ../../../advanced_features/attention_backend.md:110
msgid "User Guide"
msgstr "使用者指南"

#: ../../../advanced_features/attention_backend.md:112
msgid "Launch Command for Different Attention Backends"
msgstr ""

#: ../../../advanced_features/attention_backend.md:114
msgid "FlashInfer (Default for Non-Hopper Machines, e.g., A100, A40)"
msgstr ""

#: ../../../advanced_features/attention_backend.md:115
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend flashinfer\n"
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-V3 \\\n"
"  --attention-backend flashinfer \\\n"
"  --trust-remote-code\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:126
msgid "FlashAttention 3 (Default for Hopper Machines, e.g., H100, H200, H20)"
msgstr ""

#: ../../../advanced_features/attention_backend.md:127
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend fa3\n"
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-V3 \\\n"
"  --trust-remote-code \\\n"
"  --attention-backend fa3\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:138
msgid "Triton"
msgstr ""

#: ../../../advanced_features/attention_backend.md:139
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend triton\n"
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-V3 \\\n"
"  --attention-backend triton \\\n"
"  --trust-remote-code\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:150
msgid "Torch Native"
msgstr ""

#: ../../../advanced_features/attention_backend.md:151
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend torch_native\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:157
msgid "FlashMLA"
msgstr ""

#: ../../../advanced_features/attention_backend.md:158
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-R1 \\\n"
"  --attention-backend flashmla \\\n"
"  --trust-remote-code\n"
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-R1 \\\n"
"  --attention-backend flashmla \\\n"
"  --kv-cache-dtype fp8_e4m3 \\\n"
"  --trust-remote-code\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:172
msgid "TRTLLM MLA (Optimized for Blackwell Architecture, e.g., B200)"
msgstr ""

#: ../../../advanced_features/attention_backend.md:173
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-R1 \\\n"
"  --attention-backend trtllm_mla \\\n"
"  --trust-remote-code\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:181
msgid ""
"TRTLLM MLA with FP8 KV Cache (Higher concurrency, lower memory footprint)"
msgstr ""

#: ../../../advanced_features/attention_backend.md:182
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-R1 \\\n"
"  --attention-backend trtllm_mla \\\n"
"  --kv-cache-dtype fp8_e4m3 \\\n"
"  --trust-remote-code\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:191
msgid "Ascend"
msgstr ""

#: ../../../advanced_features/attention_backend.md:192
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend ascend\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:198
msgid "Intel XPU"
msgstr ""

#: ../../../advanced_features/attention_backend.md:199
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend intel_xpu\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:205
msgid "Wave"
msgstr ""

#: ../../../advanced_features/attention_backend.md:206
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend wave\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:212
msgid "FlexAttention"
msgstr ""

#: ../../../advanced_features/attention_backend.md:213
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model meta-llama/Meta-Llama-3.1-8B-Instruct \\\n"
"  --attention-backend flex_attention\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:219
msgid "Dual Chunk FlashAttention"
msgstr ""

#: ../../../advanced_features/attention_backend.md:220
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model Qwen/Qwen2.5-14B-Instruct-1M \\\n"
"  --attention-backend dual_chunk_flash_attn\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:226
msgid "Cutlass MLA"
msgstr ""

#: ../../../advanced_features/attention_backend.md:227
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-R1 \\\n"
"  --attention-backend cutlass_mla \\\n"
"  --trust-remote-code\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:235
msgid "FlashAttention 4 (MHA & MLA)"
msgstr ""

#: ../../../advanced_features/attention_backend.md:236
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --tp 8 \\\n"
"  --model deepseek-ai/DeepSeek-R1 \\\n"
"  --prefill-attention-backend fa4 \\\n"
"  --trust-remote-code\n"
msgstr ""

#: ../../../advanced_features/attention_backend.md:244
msgid "Steps to add a new attention backend"
msgstr ""

#: ../../../advanced_features/attention_backend.md:245
msgid ""
"To add a new attention backend, you can learn from the existing backends "
"(`python/sglang/srt/layers/attention/triton_backend.py`, `python/sglang/srt/"
"layers/attention/flashattention_backend.py`) and follow the steps below."
msgstr ""

#: ../../../advanced_features/attention_backend.md:249
msgid "Run without cuda graph. Support the two forward functions"
msgstr ""

#: ../../../advanced_features/attention_backend.md:250
msgid "forward_extend"
msgstr ""

#: ../../../advanced_features/attention_backend.md:251
msgid ""
"Will be used for prefill, prefill with KV cache, and target verification"
msgstr ""

#: ../../../advanced_features/attention_backend.md:252
#: ../../../advanced_features/attention_backend.md:255
msgid "It will be called once per layer"
msgstr ""

#: ../../../advanced_features/attention_backend.md:253
msgid "forward_decode"
msgstr ""

#: ../../../advanced_features/attention_backend.md:254
msgid "Will be used for normal decode, and draft decode"
msgstr ""

#: ../../../advanced_features/attention_backend.md:256
msgid "init_forward_metadata"
msgstr ""

#: ../../../advanced_features/attention_backend.md:257
msgid "Initialize the class and common metadata shared by all layers"
msgstr ""

#: ../../../advanced_features/attention_backend.md:258
msgid "Call the plan function for optimizations like split_kv"
msgstr ""

#: ../../../advanced_features/attention_backend.md:259
msgid "It will be called once per forward"
msgstr ""

#: ../../../advanced_features/attention_backend.md:260
msgid ""
"Run with cuda graph. It has two phases (capture and replay) and you need to "
"implement three functions"
msgstr ""

#: ../../../advanced_features/attention_backend.md:261
msgid "init_cuda_graph_state"
msgstr ""

#: ../../../advanced_features/attention_backend.md:262
msgid "It will be called once during life time"
msgstr ""

#: ../../../advanced_features/attention_backend.md:263
msgid "Create all common shared buffers"
msgstr ""

#: ../../../advanced_features/attention_backend.md:264
msgid "init_forward_metadata_capture_cuda_graph"
msgstr ""

#: ../../../advanced_features/attention_backend.md:265
msgid "It will be called before capturing a cuda graph"
msgstr ""

#: ../../../advanced_features/attention_backend.md:266
msgid ""
"It is similar to init_forward_metadata but write the medatada to some pre-"
"defined buffers"
msgstr ""

#: ../../../advanced_features/attention_backend.md:267
msgid "init_forward_metadata_replay_cuda_graph"
msgstr ""

#: ../../../advanced_features/attention_backend.md:268
msgid "It will be called before replaying a cuda graph"
msgstr ""

#: ../../../advanced_features/attention_backend.md:269
msgid "This function is in the critical path and needs to be fast"
msgstr ""
