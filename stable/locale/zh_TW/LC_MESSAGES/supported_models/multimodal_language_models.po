# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang stable\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_TW\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../supported_models/multimodal_language_models.md:1
msgid "Multimodal Language Models"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:3
msgid ""
"These models accept multi-modal inputs (e.g., images and text) and generate "
"text output. They augment language models with multimodal encoders."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:5
msgid "Example launch Command"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:7
msgid ""
"python3 -m sglang.launch_server \\\n"
"  --model-path meta-llama/Llama-3.2-11B-Vision-Instruct \\  # example HF/"
"local path\n"
"  --host 0.0.0.0 \\\n"
"  --port 30000 \\\n"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:14
msgid ""
"See the [OpenAI APIs section](https://docs.sglang.ai/basic_usage/"
"openai_api_vision.html) for how to send multimodal requests."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:16
msgid "Supported models"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:18
msgid "Below the supported models are summarized in a table."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:20
msgid ""
"If you are unsure if a specific architecture is implemented, you can search "
"for it via GitHub. For example, to search for "
"`Qwen2_5_VLForConditionalGeneration`, use the expression:"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:22
msgid ""
"repo:sgl-project/sglang path:/^python\\/sglang\\/srt\\/models\\// "
"Qwen2_5_VLForConditionalGeneration\n"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:26
msgid "in the GitHub search bar."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "Model Family (Variants)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "Example HuggingFace Identifier"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "Description"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "Notes"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**Qwen-VL**"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`Qwen/Qwen3-VL-235B-A22B-Instruct`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Alibaba's vision-language extension of Qwen; for example, Qwen2.5-VL (7B and "
"larger variants) can analyze and converse about image content."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**DeepSeek-VL2**"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`deepseek-ai/deepseek-vl2`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Vision-language variant of DeepSeek (with a dedicated image processor), "
"enabling advanced multimodal reasoning on image and text inputs."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**Janus-Pro** (1B, 7B)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`deepseek-ai/Janus-Pro-7B`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"DeepSeek's open-source multimodal model capable of both image understanding "
"and generation. Janus-Pro employs a decoupled architecture for separate "
"visual encoding paths, enhancing performance in both tasks."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**MiniCPM-V / MiniCPM-o**"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`openbmb/MiniCPM-V-2_6`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"MiniCPM-V (2.6, ~8B) supports image inputs, and MiniCPM-o adds audio/video; "
"these multimodal LLMs are optimized for end-side deployment on mobile/edge "
"devices."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**Llama 3.2 Vision** (11B)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`meta-llama/Llama-3.2-11B-Vision-Instruct`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Vision-enabled variant of Llama 3 (11B) that accepts image inputs for visual "
"question answering and other multimodal tasks."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**LLaVA** (v1.5 & v1.6)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "*e.g.* `liuhaotian/llava-v1.5-13b`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Open vision-chat models that add an image encoder to LLaMA/Vicuna (e.g. "
"LLaMA2 13B) for following multimodal instruction prompts."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**LLaVA-NeXT** (8B, 72B)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`lmms-lab/llava-next-72b`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Improved LLaVA models (with an 8B Llama3 version and a 72B version) offering "
"enhanced visual instruction-following and accuracy on multimodal benchmarks."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**LLaVA-OneVision**"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`lmms-lab/llava-onevision-qwen2-7b-ov`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Enhanced LLaVA variant integrating Qwen as the backbone; supports multiple "
"images (and even video frames) as inputs via an OpenAI Vision API-compatible "
"format."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**Gemma 3 (Multimodal)**"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`google/gemma-3-4b-it`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Gemma 3's larger models (4B, 12B, 27B) accept images (each image encoded as "
"256 tokens) alongside text in a combined 128K-token context."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**Kimi-VL** (A3B)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`moonshotai/Kimi-VL-A3B-Instruct`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Kimi-VL is a multimodal model that can understand and generate text from "
"images."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**Mistral-Small-3.1-24B**"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`mistralai/Mistral-Small-3.1-24B-Instruct-2503`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Mistral 3.1 is a multimodal model that can generate text from text or images "
"input. It also supports tool calling and structured output."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**Phi-4-multimodal-instruct**"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`microsoft/Phi-4-multimodal-instruct`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Phi-4-multimodal-instruct is the multimodal variant of the Phi-4-mini model, "
"enhanced with LoRA for improved multimodal capabilities. It supports text, "
"vision and audio modalities in SGLang."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**MiMo-VL** (7B)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`XiaomiMiMo/MiMo-VL-7B-RL`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Xiaomi's compact yet powerful vision-language model featuring a native "
"resolution ViT encoder for fine-grained visual details, an MLP projector for "
"cross-modal alignment, and the MiMo-7B language model optimized for complex "
"reasoning tasks."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**GLM-4.5V** (106B) /  **GLM-4.1V**(9B)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`zai-org/GLM-4.5V`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with "
"Scalable Reinforcement Learning"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "Use `--chat-template glm-4v`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**DotsVLM** (General/OCR)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`rednote-hilab/dots.vlm1.inst`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"RedNote's vision-language model built on a 1.2B vision encoder and DeepSeek "
"V3 LLM, featuring NaViT vision encoder trained from scratch with dynamic "
"resolution support and enhanced OCR capabilities through structured image "
"data training."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**DotsVLM-OCR**"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`rednote-hilab/dots.ocr`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Specialized OCR variant of DotsVLM optimized for optical character "
"recognition tasks with enhanced text extraction and document understanding "
"capabilities."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "Don't use `--trust-remote-code`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**NVILA** (8B, 15B, Lite-2B, Lite-8B, Lite-15B)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`Efficient-Large-Model/NVILA-8B`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`chatml`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"NVILA explores the full stack efficiency of multi-modal design, achieving "
"cheaper training, faster deployment and better performance."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:49
msgid "Video Input Support"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:51
msgid ""
"SGLang supports video input for Vision-Language Models (VLMs), enabling "
"temporal reasoning tasks such as video question answering, captioning, and "
"holistic scene understanding. Video clips are decoded, key frames are "
"sampled, and the resulting tensors are batched together with the text "
"prompt, allowing multimodal inference to integrate visual and linguistic "
"context."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "Model Family"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "Example Identifier"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "Video notes"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**Qwen-VL** (Qwen2-VL, Qwen2.5-VL, Qwen3-VL, Qwen3-Omni)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"The processor gathers `video_data`, runs Qwen's frame sampler, and merges "
"the resulting features with text tokens before inference."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**GLM-4v** (4.5V, 4.1V, MOE)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"Video clips are read with Decord, converted to tensors, and passed to the "
"model alongside metadata for rotary-position handling."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**NVILA** (Full & Lite)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"The runtime samples eight frames per clip and attaches them to the "
"multimodal request when `video_data` is present."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "**LLaVA video variants** (LLaVA-NeXT-Video, LLaVA-OneVision)"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid "`lmms-lab/LLaVA-NeXT-Video-7B`"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:0
msgid ""
"The processor routes video prompts to the LlavaVid video-enabled "
"architecture, and the provided example shows how to query it with `sgl."
"video(...)` clips."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:60
msgid ""
"Use `sgl.video(path, num_frames)` when building prompts to attach clips from "
"your SGLang programs."
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:62
msgid "Example OpenAI-compatible request that sends a video clip:"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:64
msgid ""
"import requests\n"
"\n"
"url = \"http://localhost:30000/v1/chat/completions\"\n"
"\n"
"data = {\n"
"    \"model\": \"Qwen/Qwen3-VL-30B-A3B-Instruct\",\n"
"    \"messages\": [\n"
"        {\n"
"            \"role\": \"user\",\n"
"            \"content\": [\n"
"                {\"type\": \"text\", \"text\": \"Whatâ€™s happening in this "
"video?\"},\n"
"                {\n"
"                    \"type\": \"video_url\",\n"
"                    \"video_url\": {\n"
"                        \"url\": \"https://github.com/sgl-project/sgl-test-"
"files/raw/refs/heads/main/videos/jobs_presenting_ipod.mp4\"\n"
"                    },\n"
"                },\n"
"            ],\n"
"        }\n"
"    ],\n"
"    \"max_tokens\": 300,\n"
"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"print(response.text)\n"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:92
msgid "Usage Notes"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:94
msgid "Performance Optimization"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:96
msgid ""
"For multimodal models, you can use the `--keep-mm-feature-on-device` flag to "
"optimize for latency at the cost of increased GPU memory usage:"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:98
msgid ""
"**Default behavior**: Multimodal feature tensors are moved to CPU after "
"processing to save GPU memory"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:99
msgid ""
"**With `--keep-mm-feature-on-device`**: Feature tensors remain on GPU, "
"reducing device-to-host copy overhead and improving latency, but consuming "
"more GPU memory"
msgstr ""

#: ../../../supported_models/multimodal_language_models.md:101
msgid ""
"Use this flag when you have sufficient GPU memory and want to minimize "
"latency for multimodal inference."
msgstr ""
