# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2025, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang stable\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-12-09 11:06+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../basic_usage/openai_api_completions.ipynb:2
msgid ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:9
msgid "OpenAI APIs - Completions"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:11
msgid ""
"SGLang provides OpenAI-compatible APIs to enable a smooth transition from "
"OpenAI services to self-hosted local models. A complete reference for the "
"API is available in the `OpenAI API Reference <https://platform.openai.com/"
"docs/api-reference>`__."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:13
msgid "This tutorial covers the following popular APIs:"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:15
msgid "``chat/completions``"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:16
msgid "``completions``"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:18
msgid ""
"Check out other tutorials to learn about `vision APIs <openai_api_vision."
"ipynb>`__ for vision-language models and `embedding APIs "
"<openai_api_embeddings.ipynb>`__ for embedding models."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:30
msgid "Launch A Server"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:32
msgid "Launch the server in your terminal and wait for it to initialize."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:-1
msgid "[ ]:"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:-1
msgid ""
"from sglang.test.doc_patch import launch_server_cmd\n"
"from sglang.utils import wait_for_server, print_highlight, "
"terminate_process\n"
"\n"
"server_process, port = launch_server_cmd(\n"
"    \"python3 -m sglang.launch_server --model-path qwen/qwen2.5-0.5b-"
"instruct --host 0.0.0.0 --log-level warning\"\n"
")\n"
"\n"
"wait_for_server(f\"http://localhost:{port}\")\n"
"print(f\"Server started on http://localhost:{port}\")"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:61
msgid "Chat Completions"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:64
#: ../../../basic_usage/openai_api_completions.ipynb:468
msgid "Usage"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:66
msgid ""
"The server fully implements the OpenAI API. It will automatically apply the "
"chat template specified in the Hugging Face tokenizer, if one is available. "
"You can also specify a custom chat template with ``--chat-template`` when "
"launching the server."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:-1
msgid ""
"import openai\n"
"\n"
"client = openai.Client(base_url=f\"http://127.0.0.1:{port}/v1\", "
"api_key=\"None\")\n"
"\n"
"response = client.chat.completions.create(\n"
"    model=\"qwen/qwen2.5-0.5b-instruct\",\n"
"    messages=[\n"
"        {\"role\": \"user\", \"content\": \"List 3 countries and their "
"capitals.\"},\n"
"    ],\n"
"    temperature=0,\n"
"    max_tokens=64,\n"
")\n"
"\n"
"print_highlight(f\"Response: {response}\")"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:100
msgid "Model Thinking/Reasoning Support"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:102
msgid ""
"Some models support internal reasoning or thinking processes that can be "
"exposed in the API response. SGLang provides unified support for various "
"reasoning models through the ``chat_template_kwargs`` parameter and "
"compatible reasoning parsers."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:105
msgid "Supported Models and Configuration"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:108
msgid "Model Family"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:108
msgid "Chat Template Parameter"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:108
msgid "Reasoning Parser"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:108
msgid "Notes"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:110
msgid "DeepSeek-R1 (R1, R1-0528, R1-Distill)"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:110
#: ../../../basic_usage/openai_api_completions.ipynb:114
msgid "``enable_thinking``"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:110
msgid "``--reasoning-parser deepseek-r1``"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:110
msgid "Standard reasoning models"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:112
msgid "DeepSeek-V3.1"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:112
msgid "``thinking``"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:112
msgid "``--reasoning-parser deepseek-v3``"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:112
#: ../../../basic_usage/openai_api_completions.ipynb:114
msgid "Hybrid model (thinking/non-thinking modes)"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:114
msgid "Qwen3 (standard)"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:114
msgid "``--reasoning-parser qwen3``"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:116
msgid "Qwen3-Thinking"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:116
#: ../../../basic_usage/openai_api_completions.ipynb:118
#: ../../../basic_usage/openai_api_completions.ipynb:120
msgid "N/A (always enabled)"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:116
msgid "``--reasoning-parser qwen3-thinking``"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:116
msgid "Always generates reasoning"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:118
msgid "Kimi"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:118
msgid "``--reasoning-parser kimi``"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:118
msgid "Kimi thinking models"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:120
msgid "Gpt-Oss"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:120
msgid "``--reasoning-parser gpt-oss``"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:120
msgid "Gpt-Oss thinking models"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:124
msgid "Basic Usage"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:126
msgid "To enable reasoning output, you need to:"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:128
msgid "Launch the server with the appropriate reasoning parser"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:129
msgid "Set the model-specific parameter in ``chat_template_kwargs``"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:130
msgid ""
"Optionally use ``separate_reasoning: False`` to not get reasoning content "
"separately (default to ``True``)"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:132
msgid ""
"**Note for Qwen3-Thinking models:** These models always generate thinking "
"content and do not support the ``enable_thinking`` parameter. Use ``--"
"reasoning-parser qwen3-thinking`` or ``--reasoning-parser qwen3`` to parse "
"the thinking content."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:144
msgid "Example: Qwen3 Models"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:146
msgid ""
"# Launch server:\n"
"# python3 -m sglang.launch_server --model Qwen/Qwen3-4B --reasoning-parser "
"qwen3\n"
"\n"
"from openai import OpenAI\n"
"\n"
"client = OpenAI(\n"
"    api_key=\"EMPTY\",\n"
"    base_url=f\"http://127.0.0.1:30000/v1\",\n"
")\n"
"\n"
"model = \"Qwen/Qwen3-4B\"\n"
"messages = [{\"role\": \"user\", \"content\": \"How many r's are in "
"'strawberry'?\"}]\n"
"\n"
"response = client.chat.completions.create(\n"
"    model=model,\n"
"    messages=messages,\n"
"    extra_body={\n"
"        \"chat_template_kwargs\": {\"enable_thinking\": True},\n"
"        \"separate_reasoning\": True\n"
"    }\n"
")\n"
"\n"
"print(\"Reasoning:\", response.choices[0].message.reasoning_content)\n"
"print(\"-\"*100)\n"
"print(\"Answer:\", response.choices[0].message.content)"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:174
msgid "**ExampleOutput:**"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:178
msgid ""
"Reasoning: Okay, so the user is asking how many 'r's are in the word "
"'strawberry'. Let me think. First, I need to make sure I have the word "
"spelled correctly. Strawberry... S-T-R-A-W-B-E-R-R-Y. Wait, is that right? "
"Let me break it down.\n"
"\n"
"Starting with 'strawberry', let's write out the letters one by one. S, T, R, "
"A, W, B, E, R, R, Y. Hmm, wait, that's 10 letters. Let me check again. S "
"(1), T (2), R (3), A (4), W (5), B (6), E (7), R (8), R (9), Y (10). So the "
"letters are S-T-R-A-W-B-E-R-R-Y.\n"
"...\n"
"Therefore, the answer should be three R's in 'strawberry'. But I need to "
"make sure I'm not counting any other letters as R. Let me check again. S, T, "
"R, A, W, B, E, R, R, Y. No other R's. So three in total. Yeah, that seems "
"right.\n"
"\n"
"----------------------------------------------------------------------------------------------------\n"
"Answer: The word \"strawberry\" contains **three** letters 'r'. Here's the "
"breakdown:\n"
"\n"
"1. **S-T-R-A-W-B-E-R-R-Y**\n"
"   - The **third letter** is 'R'.\n"
"   - The **eighth and ninth letters** are also 'R's.\n"
"\n"
"Thus, the total count is **3**.\n"
"\n"
"**Answer:** 3."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:195
msgid ""
"**Note:** Setting ``\"enable_thinking\": False`` (or omitting it) will "
"result in ``reasoning_content`` being ``None``. Qwen3-Thinking models always "
"generate reasoning content and don't support the ``enable_thinking`` "
"parameter."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:207
#: ../../../basic_usage/openai_api_completions.ipynb:411
msgid "Logit Bias Support"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:209
msgid ""
"SGLang supports the ``logit_bias`` parameter for both chat completions and "
"completions APIs. This parameter allows you to modify the likelihood of "
"specific tokens being generated by adding bias values to their logits. The "
"bias values can range from -100 to 100, where:"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:211
msgid ""
"**Positive values** (0 to 100) increase the likelihood of the token being "
"selected"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:212
msgid ""
"**Negative values** (-100 to 0) decrease the likelihood of the token being "
"selected"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:213
msgid "**-100** effectively prevents the token from being generated"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:215
msgid ""
"The ``logit_bias`` parameter accepts a dictionary where keys are token IDs "
"(as strings) and values are the bias amounts (as floats)."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:227
msgid "Getting Token IDs"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:229
msgid ""
"To use ``logit_bias`` effectively, you need to know the token IDs for the "
"words you want to bias. Here's how to get token IDs:"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:231
msgid ""
"# Get tokenizer to find token IDs\n"
"import tiktoken\n"
"\n"
"# For OpenAI models, use the appropriate encoding\n"
"tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")  # or your model\n"
"\n"
"# Get token IDs for specific words\n"
"word = \"sunny\"\n"
"token_ids = tokenizer.encode(word)\n"
"print(f\"Token IDs for '{word}': {token_ids}\")\n"
"\n"
"# For SGLang models, you can access the tokenizer through the client\n"
"# and get token IDs for bias"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:247
msgid ""
"**Important:** The ``logit_bias`` parameter uses token IDs as string keys, "
"not the actual words."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:259
msgid "Example: DeepSeek-V3 Models"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:261
msgid ""
"DeepSeek-V3 models support thinking mode through the ``thinking`` parameter:"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:263
msgid ""
"# Launch server:\n"
"# python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.1 --tp 8  "
"--reasoning-parser deepseek-v3\n"
"\n"
"from openai import OpenAI\n"
"\n"
"client = OpenAI(\n"
"    api_key=\"EMPTY\",\n"
"    base_url=f\"http://127.0.0.1:30000/v1\",\n"
")\n"
"\n"
"model = \"deepseek-ai/DeepSeek-V3.1\"\n"
"messages = [{\"role\": \"user\", \"content\": \"How many r's are in "
"'strawberry'?\"}]\n"
"\n"
"response = client.chat.completions.create(\n"
"    model=model,\n"
"    messages=messages,\n"
"    extra_body={\n"
"        \"chat_template_kwargs\": {\"thinking\": True},\n"
"        \"separate_reasoning\": True\n"
"    }\n"
")\n"
"\n"
"print(\"Reasoning:\", response.choices[0].message.reasoning_content)\n"
"print(\"-\"*100)\n"
"print(\"Answer:\", response.choices[0].message.content)"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:291
msgid "**Example Output:**"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:295
msgid ""
"Reasoning: First, the question is: \"How many r's are in 'strawberry'?\"\n"
"\n"
"I need to count the number of times the letter 'r' appears in the word "
"\"strawberry\".\n"
"\n"
"Let me write out the word: S-T-R-A-W-B-E-R-R-Y.\n"
"\n"
"Now, I'll go through each letter and count the 'r's.\n"
"...\n"
"So, I have three 'r's in \"strawberry\".\n"
"\n"
"I should double-check. The word is spelled S-T-R-A-W-B-E-R-R-Y. The letters "
"are at positions: 3, 8, and 9 are 'r's. Yes, that's correct.\n"
"\n"
"Therefore, the answer should be 3.\n"
"----------------------------------------------------------------------------------------------------\n"
"Answer: The word \"strawberry\" contains **3** instances of the letter "
"\"r\". Here's a breakdown for clarity:\n"
"\n"
"- The word is spelled: S-T-R-A-W-B-E-R-R-Y\n"
"- The \"r\" appears at the 3rd, 8th, and 9th positions."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:314
msgid ""
"**Note:** DeepSeek-V3 models use the ``thinking`` parameter (not "
"``enable_thinking``) to control reasoning output."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:-1
msgid ""
"# Example with logit_bias parameter\n"
"# Note: You need to get the actual token IDs from your tokenizer\n"
"# For demonstration, we'll use some example token IDs\n"
"response = client.chat.completions.create(\n"
"    model=\"qwen/qwen2.5-0.5b-instruct\",\n"
"    messages=[\n"
"        {\"role\": \"user\", \"content\": \"Complete this sentence: The "
"weather today is\"}\n"
"    ],\n"
"    temperature=0.7,\n"
"    max_tokens=20,\n"
"    logit_bias={\n"
"        \"12345\": 50,  # Increase likelihood of token ID 12345\n"
"        \"67890\": -50,  # Decrease likelihood of token ID 67890\n"
"        \"11111\": 25,  # Slightly increase likelihood of token ID 11111\n"
"    },\n"
")\n"
"\n"
"print_highlight(f\"Response with logit bias: {response.choices[0].message."
"content}\")"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:352
#: ../../../basic_usage/openai_api_completions.ipynb:500
msgid "Parameters"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:354
msgid ""
"The chat completions API accepts OpenAI Chat Completions API's parameters. "
"Refer to `OpenAI Chat Completions API <https://platform.openai.com/docs/api-"
"reference/chat/create>`__ for more details."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:356
msgid ""
"SGLang extends the standard API with the ``extra_body`` parameter, allowing "
"for additional customization. One key option within ``extra_body`` is "
"``chat_template_kwargs``, which can be used to pass arguments to the chat "
"template processor."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:-1
msgid ""
"response = client.chat.completions.create(\n"
"    model=\"qwen/qwen2.5-0.5b-instruct\",\n"
"    messages=[\n"
"        {\n"
"            \"role\": \"system\",\n"
"            \"content\": \"You are a knowledgeable historian who provides "
"concise responses.\",\n"
"        },\n"
"        {\"role\": \"user\", \"content\": \"Tell me about ancient Rome\"},\n"
"        {\n"
"            \"role\": \"assistant\",\n"
"            \"content\": \"Ancient Rome was a civilization centered in Italy."
"\",\n"
"        },\n"
"        {\"role\": \"user\", \"content\": \"What were their major "
"achievements?\"},\n"
"    ],\n"
"    temperature=0.3,  # Lower temperature for more focused responses\n"
"    max_tokens=128,  # Reasonable length for a concise response\n"
"    top_p=0.95,  # Slightly higher for better fluency\n"
"    presence_penalty=0.2,  # Mild penalty to avoid repetition\n"
"    frequency_penalty=0.2,  # Mild penalty for more natural language\n"
"    n=1,  # Single response is usually more stable\n"
"    seed=42,  # Keep for reproducibility\n"
")\n"
"\n"
"print_highlight(response.choices[0].message.content)"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:399
msgid "Streaming mode is also supported."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:413
msgid ""
"The completions API also supports the ``logit_bias`` parameter with the same "
"functionality as described in the chat completions section above."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:-1
msgid ""
"stream = client.chat.completions.create(\n"
"    model=\"qwen/qwen2.5-0.5b-instruct\",\n"
"    messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n"
"    stream=True,\n"
")\n"
"for chunk in stream:\n"
"    if chunk.choices[0].delta.content is not None:\n"
"        print(chunk.choices[0].delta.content, end=\"\")"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:-1
msgid ""
"# Example with logit_bias parameter for completions API\n"
"# Note: You need to get the actual token IDs from your tokenizer\n"
"# For demonstration, we'll use some example token IDs\n"
"response = client.completions.create(\n"
"    model=\"qwen/qwen2.5-0.5b-instruct\",\n"
"    prompt=\"The best programming language for AI is\",\n"
"    temperature=0.7,\n"
"    max_tokens=20,\n"
"    logit_bias={\n"
"        \"12345\": 75,  # Strongly favor token ID 12345\n"
"        \"67890\": -100,  # Completely avoid token ID 67890\n"
"        \"11111\": -25,  # Slightly discourage token ID 11111\n"
"    },\n"
")\n"
"\n"
"print_highlight(f\"Response with logit bias: {response.choices[0].text}\")"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:465
msgid "Completions"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:470
msgid ""
"Completions API is similar to Chat Completions API, but without the "
"``messages`` parameter or chat templates."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:-1
msgid ""
"response = client.completions.create(\n"
"    model=\"qwen/qwen2.5-0.5b-instruct\",\n"
"    prompt=\"List 3 countries and their capitals.\",\n"
"    temperature=0,\n"
"    max_tokens=64,\n"
"    n=1,\n"
"    stop=None,\n"
")\n"
"\n"
"print_highlight(f\"Response: {response}\")"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:502
msgid ""
"The completions API accepts OpenAI Completions API's parameters. Refer to "
"`OpenAI Completions API <https://platform.openai.com/docs/api-reference/"
"completions/create>`__ for more details."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:504
msgid "Here is an example of a detailed completions request:"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:-1
msgid ""
"response = client.completions.create(\n"
"    model=\"qwen/qwen2.5-0.5b-instruct\",\n"
"    prompt=\"Write a short story about a space explorer.\",\n"
"    temperature=0.7,  # Moderate temperature for creative writing\n"
"    max_tokens=150,  # Longer response for a story\n"
"    top_p=0.9,  # Balanced diversity in word choice\n"
"    stop=[\"\\n\\n\", \"THE END\"],  # Multiple stop sequences\n"
"    presence_penalty=0.3,  # Encourage novel elements\n"
"    frequency_penalty=0.3,  # Reduce repetitive phrases\n"
"    n=1,  # Generate one completion\n"
"    seed=123,  # For reproducible results\n"
")\n"
"\n"
"print_highlight(f\"Response: {response}\")"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:538
msgid "Structured Outputs (JSON, Regex, EBNF)"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:540
msgid ""
"For OpenAI compatible structured outputs API, refer to `Structured Outputs "
"<../advanced_features/structured_outputs.ipynb>`__ for more details."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:552
msgid "Using LoRA Adapters"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:554
msgid ""
"SGLang supports LoRA (Low-Rank Adaptation) adapters with OpenAI-compatible "
"APIs. You can specify which adapter to use directly in the ``model`` "
"parameter using the ``base-model:adapter-name`` syntax."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:556
msgid "**Server Setup:**"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:558
msgid ""
"python -m sglang.launch_server \\\n"
"    --model-path qwen/qwen2.5-0.5b-instruct \\\n"
"    --enable-lora \\\n"
"    --lora-paths adapter_a=/path/to/adapter_a adapter_b=/path/to/adapter_b"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:565
msgid ""
"For more details on LoRA serving configuration, see the `LoRA documentation "
"<../advanced_features/lora.ipynb>`__."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:567
msgid "**API Call:**"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:569
msgid ""
"(Recommended) Use the ``model:adapter`` syntax to specify which adapter to "
"use:"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:571
msgid ""
"response = client.chat.completions.create(\n"
"    model=\"qwen/qwen2.5-0.5b-instruct:adapter_a\",  # ← base-model:adapter-"
"name\n"
"    messages=[{\"role\": \"user\", \"content\": \"Convert to SQL: show all "
"users\"}],\n"
"    max_tokens=50,\n"
")"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:579
msgid "**Backward Compatible: Using ``extra_body``**"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:581
msgid ""
"The old ``extra_body`` method is still supported for backward compatibility:"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:583
msgid ""
"# Backward compatible method\n"
"response = client.chat.completions.create(\n"
"    model=\"qwen/qwen2.5-0.5b-instruct\",\n"
"    messages=[{\"role\": \"user\", \"content\": \"Convert to SQL: show all "
"users\"}],\n"
"    extra_body={\"lora_path\": \"adapter_a\"},  # ← old method\n"
"    max_tokens=50,\n"
")"
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:593
msgid ""
"**Note:** When both ``model:adapter`` and ``extra_body[\"lora_path\"]`` are "
"specified, the ``model:adapter`` syntax takes precedence."
msgstr ""

#: ../../../basic_usage/openai_api_completions.ipynb:-1
msgid "terminate_process(server_process)"
msgstr ""
