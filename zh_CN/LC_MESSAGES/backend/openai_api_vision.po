# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2024, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang 0.3\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-11-09 18:38+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../backend/openai_api_vision.ipynb:2
msgid ""
"<style>\n"
"    .output_area.stderr, .output_area.stdout {\n"
"        color: #d3d3d3 !important; /* light gray */\n"
"    }\n"
"</style>"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:9
msgid "OpenAI APIs - Vision"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:11
msgid ""
"SGLang provides OpenAI-compatible APIs to enable a smooth transition from "
"OpenAI services to self-hosted local models. A complete reference for the "
"API is available in the `OpenAI API Reference <https://platform.openai.com/"
"docs/guides/vision>`__. This tutorial covers the vision APIs for vision "
"language models."
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:13
msgid ""
"SGLang supports vision language models such as Llama 3.2, LLaVA-OneVision, "
"and QWen-VL2"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:15
msgid ""
"`meta-llama/Llama-3.2-11B-Vision-Instruct <https://huggingface.co/meta-llama/"
"Llama-3.2-11B-Vision-Instruct>`__"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:16
msgid ""
"`lmms-lab/llava-onevision-qwen2-72b-ov-chat <https://huggingface.co/lmms-lab/"
"llava-onevision-qwen2-72b-ov-chat>`__"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:17
msgid ""
"`Qwen/Qwen2-VL-7B-Instruct <https://huggingface.co/Qwen/Qwen2-VL-7B-"
"Instruct>`__"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:29
msgid "Launch A Server"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:31
msgid "This code block is equivalent to executing"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:33
msgid ""
"python3 -m sglang.launch_server --model-path meta-llama/Llama-3.2-11B-Vision-"
"Instruct \\\n"
"  --port 30000 --chat-template llama_3_vision"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:38
msgid "in your terminal and wait for the server to be ready."
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:40
msgid ""
"Remember to add ``--chat-template llama_3_vision`` to specify the vision "
"chat template, otherwise the server only supports text. We need to specify "
"``--chat-template`` for vision language models because the chat template "
"provided in Hugging Face tokenizer only supports text."
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:-1
msgid "[ ]:"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:-1
msgid ""
"from sglang.utils import (\n"
"    execute_shell_command,\n"
"    wait_for_server,\n"
"    terminate_process,\n"
"    print_highlight,\n"
")\n"
"\n"
"embedding_process = execute_shell_command(\n"
"    \"\"\"\n"
"python3 -m sglang.launch_server --model-path meta-llama/Llama-3.2-11B-Vision-"
"Instruct \\\n"
"    --port=30000 --chat-template=llama_3_vision\n"
"\"\"\"\n"
")\n"
"\n"
"wait_for_server(\"http://localhost:30000\")"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:75
msgid "Using cURL"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:77
msgid ""
"Once the server is up, you can send test requests using curl or requests."
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:-1
msgid ""
"import subprocess\n"
"\n"
"curl_command = \"\"\"\n"
"curl -s http://localhost:30000/v1/chat/completions \\\n"
"  -d '{\n"
"    \"model\": \"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n"
"    \"messages\": [\n"
"      {\n"
"        \"role\": \"user\",\n"
"        \"content\": [\n"
"          {\n"
"            \"type\": \"text\",\n"
"            \"text\": \"What’s in this image?\"\n"
"          },\n"
"          {\n"
"            \"type\": \"image_url\",\n"
"            \"image_url\": {\n"
"              \"url\": \"https://github.com/sgl-project/sglang/blob/main/"
"test/lang/example_image.png?raw=true\"\n"
"            }\n"
"          }\n"
"        ]\n"
"      }\n"
"    ],\n"
"    \"max_tokens\": 300\n"
"  }'\n"
"\"\"\"\n"
"\n"
"response = subprocess.check_output(curl_command, shell=True).decode()\n"
"print_highlight(response)"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:126
msgid "Using Python Requests"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:-1
msgid ""
"import requests\n"
"\n"
"url = \"http://localhost:30000/v1/chat/completions\"\n"
"\n"
"data = {\n"
"    \"model\": \"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n"
"    \"messages\": [\n"
"        {\n"
"            \"role\": \"user\",\n"
"            \"content\": [\n"
"                {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n"
"                {\n"
"                    \"type\": \"image_url\",\n"
"                    \"image_url\": {\n"
"                        \"url\": \"https://github.com/sgl-project/sglang/"
"blob/main/test/lang/example_image.png?raw=true\"\n"
"                    },\n"
"                },\n"
"            ],\n"
"        }\n"
"    ],\n"
"    \"max_tokens\": 300,\n"
"}\n"
"\n"
"response = requests.post(url, json=data)\n"
"print_highlight(response.text)"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:171
msgid "Using OpenAI Python Client"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:-1
msgid ""
"from openai import OpenAI\n"
"\n"
"client = OpenAI(base_url=\"http://localhost:30000/v1\", api_key=\"None\")\n"
"\n"
"response = client.chat.completions.create(\n"
"    model=\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n"
"    messages=[\n"
"        {\n"
"            \"role\": \"user\",\n"
"            \"content\": [\n"
"                {\n"
"                    \"type\": \"text\",\n"
"                    \"text\": \"What is in this image?\",\n"
"                },\n"
"                {\n"
"                    \"type\": \"image_url\",\n"
"                    \"image_url\": {\n"
"                        \"url\": \"https://github.com/sgl-project/sglang/"
"blob/main/test/lang/example_image.png?raw=true\"\n"
"                    },\n"
"                },\n"
"            ],\n"
"        }\n"
"    ],\n"
"    max_tokens=300,\n"
")\n"
"\n"
"print_highlight(response.choices[0].message.content)"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:218
msgid "Multiple-Image Inputs"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:220
msgid ""
"The server also supports multiple images and interleaved text and images if "
"the model supports it."
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:-1
msgid ""
"from openai import OpenAI\n"
"\n"
"client = OpenAI(base_url=\"http://localhost:30000/v1\", api_key=\"None\")\n"
"\n"
"response = client.chat.completions.create(\n"
"    model=\"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n"
"    messages=[\n"
"        {\n"
"            \"role\": \"user\",\n"
"            \"content\": [\n"
"                {\n"
"                    \"type\": \"image_url\",\n"
"                    \"image_url\": {\n"
"                        \"url\": \"https://github.com/sgl-project/sglang/"
"blob/main/test/lang/example_image.png?raw=true\",\n"
"                    },\n"
"                },\n"
"                {\n"
"                    \"type\": \"image_url\",\n"
"                    \"image_url\": {\n"
"                        \"url\": \"https://raw.githubusercontent.com/sgl-"
"project/sglang/main/assets/logo.png\",\n"
"                    },\n"
"                },\n"
"                {\n"
"                    \"type\": \"text\",\n"
"                    \"text\": \"I have two very different images. They are "
"not related at all. \"\n"
"                    \"Please describe the first image in one sentence, and "
"then describe the second image in another sentence.\",\n"
"                },\n"
"            ],\n"
"        }\n"
"    ],\n"
"    temperature=0,\n"
")\n"
"\n"
"print_highlight(response.choices[0].message.content)"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:-1
msgid "[6]:"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:-1
msgid "terminate_process(embedding_process)"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:284
msgid "Chat Template"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:286
msgid ""
"As mentioned before, if you do not specify a vision model's ``--chat-"
"template``, the server uses Hugging Face's default template, which only "
"supports text."
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:288
msgid "We list popular vision models with their chat templates:"
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:290
msgid ""
"`meta-llama/Llama-3.2-Vision <https://huggingface.co/meta-llama/"
"Llama-3.2-11B-Vision-Instruct>`__ uses ``llama_3_vision``."
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:291
msgid ""
"`Qwen/Qwen2-VL-7B-Instruct <https://huggingface.co/Qwen/Qwen2-VL-7B-"
"Instruct>`__ uses ``qwen2-vl``."
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:292
msgid ""
"`LlaVA-OneVision <https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-"
"ov>`__ uses ``chatml-llava``."
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:293
msgid ""
"`LLaVA-NeXT <https://huggingface.co/collections/lmms-lab/llava-"
"next-6623288e2d61edba3ddbf5ff>`__ uses ``chatml-llava``."
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:294
msgid ""
"`Llama3-LLaVA-NeXT <https://huggingface.co/lmms-lab/llama3-llava-next-8b>`__ "
"uses ``llava_llama_3``."
msgstr ""

#: ../../../backend/openai_api_vision.ipynb:295
msgid ""
"`LLaVA-v1.5 / 1.6 <https://huggingface.co/liuhaotian/llava-v1.6-34b>`__ uses "
"``vicuna_v1.1``."
msgstr ""
