# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023-2026, SGLang
# This file is distributed under the same license as the SGLang package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SGLang stable\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-06 08:37+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../../platforms/ascend_npu_support_features.md:1
msgid "Support Features on Ascend NPU"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:3
msgid ""
"This section describes the basic functions and features supported by the "
"Ascend NPU.If you encounter issues or have any questions, please [open an "
"issue](https://github.com/sgl-project/sglang/issues)."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:6
msgid "Model and tokenizer"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Argument"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Description"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Defaults"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Options"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "A2 Supported"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "A3 Supported"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--model-path`<br>`--model`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:10
#: ../../../platforms/ascend_npu_support_features.md:68
#: ../../../platforms/ascend_npu_support_features.md:69
#: ../../../platforms/ascend_npu_support_features.md:130
#: ../../../platforms/ascend_npu_support_features.md:138
#: ../../../platforms/ascend_npu_support_features.md:183
#: ../../../platforms/ascend_npu_support_features.md:211
msgid "<br>"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The path of the model weights. This can be a local folder or a Hugging Face "
"repo ID."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`None`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Type: str"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "**<span style=\"color: green;\">√</span>**"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:10
#: ../../../platforms/ascend_npu_support_features.md:11
#: ../../../platforms/ascend_npu_support_features.md:12
#: ../../../platforms/ascend_npu_support_features.md:13
#: ../../../platforms/ascend_npu_support_features.md:14
#: ../../../platforms/ascend_npu_support_features.md:15
#: ../../../platforms/ascend_npu_support_features.md:16
#: ../../../platforms/ascend_npu_support_features.md:17
#: ../../../platforms/ascend_npu_support_features.md:18
#: ../../../platforms/ascend_npu_support_features.md:20
#: ../../../platforms/ascend_npu_support_features.md:27
#: ../../../platforms/ascend_npu_support_features.md:28
#: ../../../platforms/ascend_npu_support_features.md:29
#: ../../../platforms/ascend_npu_support_features.md:30
#: ../../../platforms/ascend_npu_support_features.md:31
#: ../../../platforms/ascend_npu_support_features.md:37
#: ../../../platforms/ascend_npu_support_features.md:38
#: ../../../platforms/ascend_npu_support_features.md:40
#: ../../../platforms/ascend_npu_support_features.md:46
#: ../../../platforms/ascend_npu_support_features.md:47
#: ../../../platforms/ascend_npu_support_features.md:48
#: ../../../platforms/ascend_npu_support_features.md:49
#: ../../../platforms/ascend_npu_support_features.md:50
#: ../../../platforms/ascend_npu_support_features.md:51
#: ../../../platforms/ascend_npu_support_features.md:52
#: ../../../platforms/ascend_npu_support_features.md:53
#: ../../../platforms/ascend_npu_support_features.md:54
#: ../../../platforms/ascend_npu_support_features.md:55
#: ../../../platforms/ascend_npu_support_features.md:56
#: ../../../platforms/ascend_npu_support_features.md:57
#: ../../../platforms/ascend_npu_support_features.md:58
#: ../../../platforms/ascend_npu_support_features.md:67
#: ../../../platforms/ascend_npu_support_features.md:68
#: ../../../platforms/ascend_npu_support_features.md:71
#: ../../../platforms/ascend_npu_support_features.md:72
#: ../../../platforms/ascend_npu_support_features.md:73
#: ../../../platforms/ascend_npu_support_features.md:76
#: ../../../platforms/ascend_npu_support_features.md:77
#: ../../../platforms/ascend_npu_support_features.md:79
#: ../../../platforms/ascend_npu_support_features.md:80
#: ../../../platforms/ascend_npu_support_features.md:81
#: ../../../platforms/ascend_npu_support_features.md:88
#: ../../../platforms/ascend_npu_support_features.md:89
#: ../../../platforms/ascend_npu_support_features.md:90
#: ../../../platforms/ascend_npu_support_features.md:91
#: ../../../platforms/ascend_npu_support_features.md:105
#: ../../../platforms/ascend_npu_support_features.md:115
#: ../../../platforms/ascend_npu_support_features.md:116
#: ../../../platforms/ascend_npu_support_features.md:117
#: ../../../platforms/ascend_npu_support_features.md:118
#: ../../../platforms/ascend_npu_support_features.md:119
#: ../../../platforms/ascend_npu_support_features.md:120
#: ../../../platforms/ascend_npu_support_features.md:121
#: ../../../platforms/ascend_npu_support_features.md:122
#: ../../../platforms/ascend_npu_support_features.md:130
#: ../../../platforms/ascend_npu_support_features.md:131
#: ../../../platforms/ascend_npu_support_features.md:132
#: ../../../platforms/ascend_npu_support_features.md:138
#: ../../../platforms/ascend_npu_support_features.md:139
#: ../../../platforms/ascend_npu_support_features.md:140
#: ../../../platforms/ascend_npu_support_features.md:146
#: ../../../platforms/ascend_npu_support_features.md:147
#: ../../../platforms/ascend_npu_support_features.md:153
#: ../../../platforms/ascend_npu_support_features.md:154
#: ../../../platforms/ascend_npu_support_features.md:156
#: ../../../platforms/ascend_npu_support_features.md:167
#: ../../../platforms/ascend_npu_support_features.md:168
#: ../../../platforms/ascend_npu_support_features.md:169
#: ../../../platforms/ascend_npu_support_features.md:170
#: ../../../platforms/ascend_npu_support_features.md:171
#: ../../../platforms/ascend_npu_support_features.md:172
#: ../../../platforms/ascend_npu_support_features.md:182
#: ../../../platforms/ascend_npu_support_features.md:183
#: ../../../platforms/ascend_npu_support_features.md:185
#: ../../../platforms/ascend_npu_support_features.md:186
#: ../../../platforms/ascend_npu_support_features.md:187
#: ../../../platforms/ascend_npu_support_features.md:188
#: ../../../platforms/ascend_npu_support_features.md:189
#: ../../../platforms/ascend_npu_support_features.md:191
#: ../../../platforms/ascend_npu_support_features.md:192
#: ../../../platforms/ascend_npu_support_features.md:193
#: ../../../platforms/ascend_npu_support_features.md:211
#: ../../../platforms/ascend_npu_support_features.md:212
#: ../../../platforms/ascend_npu_support_features.md:213
#: ../../../platforms/ascend_npu_support_features.md:216
#: ../../../platforms/ascend_npu_support_features.md:228
#: ../../../platforms/ascend_npu_support_features.md:252
#: ../../../platforms/ascend_npu_support_features.md:253
#: ../../../platforms/ascend_npu_support_features.md:254
#: ../../../platforms/ascend_npu_support_features.md:255
#: ../../../platforms/ascend_npu_support_features.md:256
#: ../../../platforms/ascend_npu_support_features.md:257
#: ../../../platforms/ascend_npu_support_features.md:258
#: ../../../platforms/ascend_npu_support_features.md:259
#: ../../../platforms/ascend_npu_support_features.md:284
#: ../../../platforms/ascend_npu_support_features.md:294
#: ../../../platforms/ascend_npu_support_features.md:295
#: ../../../platforms/ascend_npu_support_features.md:296
#: ../../../platforms/ascend_npu_support_features.md:297
#: ../../../platforms/ascend_npu_support_features.md:298
#: ../../../platforms/ascend_npu_support_features.md:304
#: ../../../platforms/ascend_npu_support_features.md:305
#: ../../../platforms/ascend_npu_support_features.md:309
#: ../../../platforms/ascend_npu_support_features.md:310
#: ../../../platforms/ascend_npu_support_features.md:311
#: ../../../platforms/ascend_npu_support_features.md:312
#: ../../../platforms/ascend_npu_support_features.md:316
#: ../../../platforms/ascend_npu_support_features.md:317
#: ../../../platforms/ascend_npu_support_features.md:321
#: ../../../platforms/ascend_npu_support_features.md:339
#: ../../../platforms/ascend_npu_support_features.md:358
#: ../../../platforms/ascend_npu_support_features.md:359
#: ../../../platforms/ascend_npu_support_features.md:360
#: ../../../platforms/ascend_npu_support_features.md:366
#: ../../../platforms/ascend_npu_support_features.md:367
#: ../../../platforms/ascend_npu_support_features.md:368
#: ../../../platforms/ascend_npu_support_features.md:374
#: ../../../platforms/ascend_npu_support_features.md:375
#: ../../../platforms/ascend_npu_support_features.md:382
msgid "<span style=\"color: green;\">"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:10
#: ../../../platforms/ascend_npu_support_features.md:11
#: ../../../platforms/ascend_npu_support_features.md:12
#: ../../../platforms/ascend_npu_support_features.md:13
#: ../../../platforms/ascend_npu_support_features.md:14
#: ../../../platforms/ascend_npu_support_features.md:15
#: ../../../platforms/ascend_npu_support_features.md:16
#: ../../../platforms/ascend_npu_support_features.md:17
#: ../../../platforms/ascend_npu_support_features.md:18
#: ../../../platforms/ascend_npu_support_features.md:19
#: ../../../platforms/ascend_npu_support_features.md:20
#: ../../../platforms/ascend_npu_support_features.md:21
#: ../../../platforms/ascend_npu_support_features.md:27
#: ../../../platforms/ascend_npu_support_features.md:28
#: ../../../platforms/ascend_npu_support_features.md:29
#: ../../../platforms/ascend_npu_support_features.md:30
#: ../../../platforms/ascend_npu_support_features.md:31
#: ../../../platforms/ascend_npu_support_features.md:37
#: ../../../platforms/ascend_npu_support_features.md:38
#: ../../../platforms/ascend_npu_support_features.md:39
#: ../../../platforms/ascend_npu_support_features.md:40
#: ../../../platforms/ascend_npu_support_features.md:46
#: ../../../platforms/ascend_npu_support_features.md:47
#: ../../../platforms/ascend_npu_support_features.md:48
#: ../../../platforms/ascend_npu_support_features.md:49
#: ../../../platforms/ascend_npu_support_features.md:50
#: ../../../platforms/ascend_npu_support_features.md:51
#: ../../../platforms/ascend_npu_support_features.md:52
#: ../../../platforms/ascend_npu_support_features.md:53
#: ../../../platforms/ascend_npu_support_features.md:54
#: ../../../platforms/ascend_npu_support_features.md:55
#: ../../../platforms/ascend_npu_support_features.md:56
#: ../../../platforms/ascend_npu_support_features.md:57
#: ../../../platforms/ascend_npu_support_features.md:58
#: ../../../platforms/ascend_npu_support_features.md:59
#: ../../../platforms/ascend_npu_support_features.md:60
#: ../../../platforms/ascend_npu_support_features.md:61
#: ../../../platforms/ascend_npu_support_features.md:67
#: ../../../platforms/ascend_npu_support_features.md:68
#: ../../../platforms/ascend_npu_support_features.md:69
#: ../../../platforms/ascend_npu_support_features.md:70
#: ../../../platforms/ascend_npu_support_features.md:71
#: ../../../platforms/ascend_npu_support_features.md:72
#: ../../../platforms/ascend_npu_support_features.md:73
#: ../../../platforms/ascend_npu_support_features.md:74
#: ../../../platforms/ascend_npu_support_features.md:75
#: ../../../platforms/ascend_npu_support_features.md:76
#: ../../../platforms/ascend_npu_support_features.md:77
#: ../../../platforms/ascend_npu_support_features.md:78
#: ../../../platforms/ascend_npu_support_features.md:79
#: ../../../platforms/ascend_npu_support_features.md:80
#: ../../../platforms/ascend_npu_support_features.md:81
#: ../../../platforms/ascend_npu_support_features.md:82
#: ../../../platforms/ascend_npu_support_features.md:88
#: ../../../platforms/ascend_npu_support_features.md:89
#: ../../../platforms/ascend_npu_support_features.md:90
#: ../../../platforms/ascend_npu_support_features.md:91
#: ../../../platforms/ascend_npu_support_features.md:92
#: ../../../platforms/ascend_npu_support_features.md:93
#: ../../../platforms/ascend_npu_support_features.md:94
#: ../../../platforms/ascend_npu_support_features.md:95
#: ../../../platforms/ascend_npu_support_features.md:96
#: ../../../platforms/ascend_npu_support_features.md:97
#: ../../../platforms/ascend_npu_support_features.md:98
#: ../../../platforms/ascend_npu_support_features.md:99
#: ../../../platforms/ascend_npu_support_features.md:100
#: ../../../platforms/ascend_npu_support_features.md:101
#: ../../../platforms/ascend_npu_support_features.md:102
#: ../../../platforms/ascend_npu_support_features.md:103
#: ../../../platforms/ascend_npu_support_features.md:104
#: ../../../platforms/ascend_npu_support_features.md:105
#: ../../../platforms/ascend_npu_support_features.md:106
#: ../../../platforms/ascend_npu_support_features.md:107
#: ../../../platforms/ascend_npu_support_features.md:108
#: ../../../platforms/ascend_npu_support_features.md:109
#: ../../../platforms/ascend_npu_support_features.md:115
#: ../../../platforms/ascend_npu_support_features.md:116
#: ../../../platforms/ascend_npu_support_features.md:117
#: ../../../platforms/ascend_npu_support_features.md:118
#: ../../../platforms/ascend_npu_support_features.md:119
#: ../../../platforms/ascend_npu_support_features.md:120
#: ../../../platforms/ascend_npu_support_features.md:121
#: ../../../platforms/ascend_npu_support_features.md:122
#: ../../../platforms/ascend_npu_support_features.md:123
#: ../../../platforms/ascend_npu_support_features.md:124
#: ../../../platforms/ascend_npu_support_features.md:130
#: ../../../platforms/ascend_npu_support_features.md:131
#: ../../../platforms/ascend_npu_support_features.md:132
#: ../../../platforms/ascend_npu_support_features.md:138
#: ../../../platforms/ascend_npu_support_features.md:139
#: ../../../platforms/ascend_npu_support_features.md:140
#: ../../../platforms/ascend_npu_support_features.md:146
#: ../../../platforms/ascend_npu_support_features.md:147
#: ../../../platforms/ascend_npu_support_features.md:153
#: ../../../platforms/ascend_npu_support_features.md:154
#: ../../../platforms/ascend_npu_support_features.md:155
#: ../../../platforms/ascend_npu_support_features.md:156
#: ../../../platforms/ascend_npu_support_features.md:157
#: ../../../platforms/ascend_npu_support_features.md:158
#: ../../../platforms/ascend_npu_support_features.md:159
#: ../../../platforms/ascend_npu_support_features.md:160
#: ../../../platforms/ascend_npu_support_features.md:161
#: ../../../platforms/ascend_npu_support_features.md:167
#: ../../../platforms/ascend_npu_support_features.md:168
#: ../../../platforms/ascend_npu_support_features.md:169
#: ../../../platforms/ascend_npu_support_features.md:170
#: ../../../platforms/ascend_npu_support_features.md:171
#: ../../../platforms/ascend_npu_support_features.md:172
#: ../../../platforms/ascend_npu_support_features.md:173
#: ../../../platforms/ascend_npu_support_features.md:174
#: ../../../platforms/ascend_npu_support_features.md:175
#: ../../../platforms/ascend_npu_support_features.md:176
#: ../../../platforms/ascend_npu_support_features.md:182
#: ../../../platforms/ascend_npu_support_features.md:183
#: ../../../platforms/ascend_npu_support_features.md:184
#: ../../../platforms/ascend_npu_support_features.md:185
#: ../../../platforms/ascend_npu_support_features.md:186
#: ../../../platforms/ascend_npu_support_features.md:187
#: ../../../platforms/ascend_npu_support_features.md:188
#: ../../../platforms/ascend_npu_support_features.md:189
#: ../../../platforms/ascend_npu_support_features.md:190
#: ../../../platforms/ascend_npu_support_features.md:191
#: ../../../platforms/ascend_npu_support_features.md:192
#: ../../../platforms/ascend_npu_support_features.md:193
#: ../../../platforms/ascend_npu_support_features.md:199
#: ../../../platforms/ascend_npu_support_features.md:200
#: ../../../platforms/ascend_npu_support_features.md:201
#: ../../../platforms/ascend_npu_support_features.md:202
#: ../../../platforms/ascend_npu_support_features.md:203
#: ../../../platforms/ascend_npu_support_features.md:204
#: ../../../platforms/ascend_npu_support_features.md:205
#: ../../../platforms/ascend_npu_support_features.md:211
#: ../../../platforms/ascend_npu_support_features.md:212
#: ../../../platforms/ascend_npu_support_features.md:213
#: ../../../platforms/ascend_npu_support_features.md:214
#: ../../../platforms/ascend_npu_support_features.md:215
#: ../../../platforms/ascend_npu_support_features.md:216
#: ../../../platforms/ascend_npu_support_features.md:217
#: ../../../platforms/ascend_npu_support_features.md:218
#: ../../../platforms/ascend_npu_support_features.md:219
#: ../../../platforms/ascend_npu_support_features.md:220
#: ../../../platforms/ascend_npu_support_features.md:221
#: ../../../platforms/ascend_npu_support_features.md:222
#: ../../../platforms/ascend_npu_support_features.md:223
#: ../../../platforms/ascend_npu_support_features.md:224
#: ../../../platforms/ascend_npu_support_features.md:225
#: ../../../platforms/ascend_npu_support_features.md:226
#: ../../../platforms/ascend_npu_support_features.md:227
#: ../../../platforms/ascend_npu_support_features.md:228
#: ../../../platforms/ascend_npu_support_features.md:229
#: ../../../platforms/ascend_npu_support_features.md:230
#: ../../../platforms/ascend_npu_support_features.md:236
#: ../../../platforms/ascend_npu_support_features.md:237
#: ../../../platforms/ascend_npu_support_features.md:238
#: ../../../platforms/ascend_npu_support_features.md:239
#: ../../../platforms/ascend_npu_support_features.md:240
#: ../../../platforms/ascend_npu_support_features.md:246
#: ../../../platforms/ascend_npu_support_features.md:252
#: ../../../platforms/ascend_npu_support_features.md:253
#: ../../../platforms/ascend_npu_support_features.md:254
#: ../../../platforms/ascend_npu_support_features.md:255
#: ../../../platforms/ascend_npu_support_features.md:256
#: ../../../platforms/ascend_npu_support_features.md:257
#: ../../../platforms/ascend_npu_support_features.md:258
#: ../../../platforms/ascend_npu_support_features.md:259
#: ../../../platforms/ascend_npu_support_features.md:260
#: ../../../platforms/ascend_npu_support_features.md:261
#: ../../../platforms/ascend_npu_support_features.md:267
#: ../../../platforms/ascend_npu_support_features.md:273
#: ../../../platforms/ascend_npu_support_features.md:274
#: ../../../platforms/ascend_npu_support_features.md:275
#: ../../../platforms/ascend_npu_support_features.md:276
#: ../../../platforms/ascend_npu_support_features.md:277
#: ../../../platforms/ascend_npu_support_features.md:278
#: ../../../platforms/ascend_npu_support_features.md:284
#: ../../../platforms/ascend_npu_support_features.md:285
#: ../../../platforms/ascend_npu_support_features.md:286
#: ../../../platforms/ascend_npu_support_features.md:287
#: ../../../platforms/ascend_npu_support_features.md:288
#: ../../../platforms/ascend_npu_support_features.md:294
#: ../../../platforms/ascend_npu_support_features.md:295
#: ../../../platforms/ascend_npu_support_features.md:296
#: ../../../platforms/ascend_npu_support_features.md:297
#: ../../../platforms/ascend_npu_support_features.md:298
#: ../../../platforms/ascend_npu_support_features.md:299
#: ../../../platforms/ascend_npu_support_features.md:300
#: ../../../platforms/ascend_npu_support_features.md:301
#: ../../../platforms/ascend_npu_support_features.md:302
#: ../../../platforms/ascend_npu_support_features.md:303
#: ../../../platforms/ascend_npu_support_features.md:304
#: ../../../platforms/ascend_npu_support_features.md:305
#: ../../../platforms/ascend_npu_support_features.md:306
#: ../../../platforms/ascend_npu_support_features.md:307
#: ../../../platforms/ascend_npu_support_features.md:308
#: ../../../platforms/ascend_npu_support_features.md:309
#: ../../../platforms/ascend_npu_support_features.md:310
#: ../../../platforms/ascend_npu_support_features.md:311
#: ../../../platforms/ascend_npu_support_features.md:312
#: ../../../platforms/ascend_npu_support_features.md:313
#: ../../../platforms/ascend_npu_support_features.md:314
#: ../../../platforms/ascend_npu_support_features.md:315
#: ../../../platforms/ascend_npu_support_features.md:316
#: ../../../platforms/ascend_npu_support_features.md:317
#: ../../../platforms/ascend_npu_support_features.md:318
#: ../../../platforms/ascend_npu_support_features.md:319
#: ../../../platforms/ascend_npu_support_features.md:320
#: ../../../platforms/ascend_npu_support_features.md:321
#: ../../../platforms/ascend_npu_support_features.md:322
#: ../../../platforms/ascend_npu_support_features.md:323
#: ../../../platforms/ascend_npu_support_features.md:324
#: ../../../platforms/ascend_npu_support_features.md:325
#: ../../../platforms/ascend_npu_support_features.md:326
#: ../../../platforms/ascend_npu_support_features.md:327
#: ../../../platforms/ascend_npu_support_features.md:328
#: ../../../platforms/ascend_npu_support_features.md:329
#: ../../../platforms/ascend_npu_support_features.md:330
#: ../../../platforms/ascend_npu_support_features.md:331
#: ../../../platforms/ascend_npu_support_features.md:332
#: ../../../platforms/ascend_npu_support_features.md:333
#: ../../../platforms/ascend_npu_support_features.md:334
#: ../../../platforms/ascend_npu_support_features.md:335
#: ../../../platforms/ascend_npu_support_features.md:336
#: ../../../platforms/ascend_npu_support_features.md:337
#: ../../../platforms/ascend_npu_support_features.md:338
#: ../../../platforms/ascend_npu_support_features.md:339
#: ../../../platforms/ascend_npu_support_features.md:340
#: ../../../platforms/ascend_npu_support_features.md:341
#: ../../../platforms/ascend_npu_support_features.md:342
#: ../../../platforms/ascend_npu_support_features.md:343
#: ../../../platforms/ascend_npu_support_features.md:344
#: ../../../platforms/ascend_npu_support_features.md:350
#: ../../../platforms/ascend_npu_support_features.md:356
#: ../../../platforms/ascend_npu_support_features.md:357
#: ../../../platforms/ascend_npu_support_features.md:358
#: ../../../platforms/ascend_npu_support_features.md:359
#: ../../../platforms/ascend_npu_support_features.md:360
#: ../../../platforms/ascend_npu_support_features.md:366
#: ../../../platforms/ascend_npu_support_features.md:367
#: ../../../platforms/ascend_npu_support_features.md:368
#: ../../../platforms/ascend_npu_support_features.md:369
#: ../../../platforms/ascend_npu_support_features.md:370
#: ../../../platforms/ascend_npu_support_features.md:371
#: ../../../platforms/ascend_npu_support_features.md:372
#: ../../../platforms/ascend_npu_support_features.md:373
#: ../../../platforms/ascend_npu_support_features.md:374
#: ../../../platforms/ascend_npu_support_features.md:375
#: ../../../platforms/ascend_npu_support_features.md:381
#: ../../../platforms/ascend_npu_support_features.md:382
#: ../../../platforms/ascend_npu_support_features.md:383
#: ../../../platforms/ascend_npu_support_features.md:384
#: ../../../platforms/ascend_npu_support_features.md:385
#: ../../../platforms/ascend_npu_support_features.md:391
#: ../../../platforms/ascend_npu_support_features.md:392
#: ../../../platforms/ascend_npu_support_features.md:393
#: ../../../platforms/ascend_npu_support_features.md:399
msgid "</span>"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--tokenizer-path`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The path of the tokenizer."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--tokenizer-mode`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Tokenizer mode. 'auto' will use the fast tokenizer if available, and 'slow' "
"will always use the slow tokenizer."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`auto`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`auto`, `slow`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--tokenizer-worker-num`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The worker num of the tokenizer manager."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`1`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Type: int"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--skip-tokenizer-init`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "If set, skip init tokenizer and pass input_ids in generate request."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`False`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "bool flag (set to enable)"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--trust-remote-code`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Whether or not to allow for custom models defined on the Hub in their own "
"modeling files."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--context-length`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The model's maximum context length. Defaults to None (will use the value "
"from the model's config.json instead)."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--is-embedding`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Whether to use a CausalLM as an embedding model."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-multimodal`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Enable the multimodal functionality for the served model. If the model being "
"served is not multimodal, nothing will happen"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--revision`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The specific model version to use. It can be a branch name, a tag name, or a "
"commit id. If unspecified, will use the default version."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "**<span style=\"color: red;\">×</span>**"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:19
#: ../../../platforms/ascend_npu_support_features.md:20
#: ../../../platforms/ascend_npu_support_features.md:21
#: ../../../platforms/ascend_npu_support_features.md:39
#: ../../../platforms/ascend_npu_support_features.md:59
#: ../../../platforms/ascend_npu_support_features.md:60
#: ../../../platforms/ascend_npu_support_features.md:61
#: ../../../platforms/ascend_npu_support_features.md:69
#: ../../../platforms/ascend_npu_support_features.md:70
#: ../../../platforms/ascend_npu_support_features.md:74
#: ../../../platforms/ascend_npu_support_features.md:75
#: ../../../platforms/ascend_npu_support_features.md:78
#: ../../../platforms/ascend_npu_support_features.md:82
#: ../../../platforms/ascend_npu_support_features.md:92
#: ../../../platforms/ascend_npu_support_features.md:93
#: ../../../platforms/ascend_npu_support_features.md:94
#: ../../../platforms/ascend_npu_support_features.md:95
#: ../../../platforms/ascend_npu_support_features.md:96
#: ../../../platforms/ascend_npu_support_features.md:97
#: ../../../platforms/ascend_npu_support_features.md:98
#: ../../../platforms/ascend_npu_support_features.md:99
#: ../../../platforms/ascend_npu_support_features.md:100
#: ../../../platforms/ascend_npu_support_features.md:101
#: ../../../platforms/ascend_npu_support_features.md:102
#: ../../../platforms/ascend_npu_support_features.md:103
#: ../../../platforms/ascend_npu_support_features.md:104
#: ../../../platforms/ascend_npu_support_features.md:106
#: ../../../platforms/ascend_npu_support_features.md:107
#: ../../../platforms/ascend_npu_support_features.md:108
#: ../../../platforms/ascend_npu_support_features.md:109
#: ../../../platforms/ascend_npu_support_features.md:123
#: ../../../platforms/ascend_npu_support_features.md:124
#: ../../../platforms/ascend_npu_support_features.md:155
#: ../../../platforms/ascend_npu_support_features.md:157
#: ../../../platforms/ascend_npu_support_features.md:158
#: ../../../platforms/ascend_npu_support_features.md:159
#: ../../../platforms/ascend_npu_support_features.md:160
#: ../../../platforms/ascend_npu_support_features.md:161
#: ../../../platforms/ascend_npu_support_features.md:173
#: ../../../platforms/ascend_npu_support_features.md:174
#: ../../../platforms/ascend_npu_support_features.md:175
#: ../../../platforms/ascend_npu_support_features.md:176
#: ../../../platforms/ascend_npu_support_features.md:184
#: ../../../platforms/ascend_npu_support_features.md:190
#: ../../../platforms/ascend_npu_support_features.md:199
#: ../../../platforms/ascend_npu_support_features.md:200
#: ../../../platforms/ascend_npu_support_features.md:201
#: ../../../platforms/ascend_npu_support_features.md:202
#: ../../../platforms/ascend_npu_support_features.md:203
#: ../../../platforms/ascend_npu_support_features.md:204
#: ../../../platforms/ascend_npu_support_features.md:205
#: ../../../platforms/ascend_npu_support_features.md:211
#: ../../../platforms/ascend_npu_support_features.md:212
#: ../../../platforms/ascend_npu_support_features.md:213
#: ../../../platforms/ascend_npu_support_features.md:214
#: ../../../platforms/ascend_npu_support_features.md:215
#: ../../../platforms/ascend_npu_support_features.md:216
#: ../../../platforms/ascend_npu_support_features.md:217
#: ../../../platforms/ascend_npu_support_features.md:218
#: ../../../platforms/ascend_npu_support_features.md:219
#: ../../../platforms/ascend_npu_support_features.md:220
#: ../../../platforms/ascend_npu_support_features.md:221
#: ../../../platforms/ascend_npu_support_features.md:222
#: ../../../platforms/ascend_npu_support_features.md:223
#: ../../../platforms/ascend_npu_support_features.md:224
#: ../../../platforms/ascend_npu_support_features.md:225
#: ../../../platforms/ascend_npu_support_features.md:226
#: ../../../platforms/ascend_npu_support_features.md:227
#: ../../../platforms/ascend_npu_support_features.md:229
#: ../../../platforms/ascend_npu_support_features.md:230
#: ../../../platforms/ascend_npu_support_features.md:236
#: ../../../platforms/ascend_npu_support_features.md:237
#: ../../../platforms/ascend_npu_support_features.md:238
#: ../../../platforms/ascend_npu_support_features.md:239
#: ../../../platforms/ascend_npu_support_features.md:240
#: ../../../platforms/ascend_npu_support_features.md:246
#: ../../../platforms/ascend_npu_support_features.md:260
#: ../../../platforms/ascend_npu_support_features.md:261
#: ../../../platforms/ascend_npu_support_features.md:267
#: ../../../platforms/ascend_npu_support_features.md:273
#: ../../../platforms/ascend_npu_support_features.md:274
#: ../../../platforms/ascend_npu_support_features.md:275
#: ../../../platforms/ascend_npu_support_features.md:276
#: ../../../platforms/ascend_npu_support_features.md:277
#: ../../../platforms/ascend_npu_support_features.md:278
#: ../../../platforms/ascend_npu_support_features.md:285
#: ../../../platforms/ascend_npu_support_features.md:286
#: ../../../platforms/ascend_npu_support_features.md:287
#: ../../../platforms/ascend_npu_support_features.md:288
#: ../../../platforms/ascend_npu_support_features.md:295
#: ../../../platforms/ascend_npu_support_features.md:296
#: ../../../platforms/ascend_npu_support_features.md:298
#: ../../../platforms/ascend_npu_support_features.md:299
#: ../../../platforms/ascend_npu_support_features.md:300
#: ../../../platforms/ascend_npu_support_features.md:301
#: ../../../platforms/ascend_npu_support_features.md:302
#: ../../../platforms/ascend_npu_support_features.md:303
#: ../../../platforms/ascend_npu_support_features.md:306
#: ../../../platforms/ascend_npu_support_features.md:307
#: ../../../platforms/ascend_npu_support_features.md:308
#: ../../../platforms/ascend_npu_support_features.md:313
#: ../../../platforms/ascend_npu_support_features.md:314
#: ../../../platforms/ascend_npu_support_features.md:315
#: ../../../platforms/ascend_npu_support_features.md:318
#: ../../../platforms/ascend_npu_support_features.md:319
#: ../../../platforms/ascend_npu_support_features.md:320
#: ../../../platforms/ascend_npu_support_features.md:321
#: ../../../platforms/ascend_npu_support_features.md:322
#: ../../../platforms/ascend_npu_support_features.md:323
#: ../../../platforms/ascend_npu_support_features.md:324
#: ../../../platforms/ascend_npu_support_features.md:325
#: ../../../platforms/ascend_npu_support_features.md:326
#: ../../../platforms/ascend_npu_support_features.md:327
#: ../../../platforms/ascend_npu_support_features.md:328
#: ../../../platforms/ascend_npu_support_features.md:329
#: ../../../platforms/ascend_npu_support_features.md:330
#: ../../../platforms/ascend_npu_support_features.md:331
#: ../../../platforms/ascend_npu_support_features.md:332
#: ../../../platforms/ascend_npu_support_features.md:333
#: ../../../platforms/ascend_npu_support_features.md:334
#: ../../../platforms/ascend_npu_support_features.md:335
#: ../../../platforms/ascend_npu_support_features.md:336
#: ../../../platforms/ascend_npu_support_features.md:337
#: ../../../platforms/ascend_npu_support_features.md:338
#: ../../../platforms/ascend_npu_support_features.md:340
#: ../../../platforms/ascend_npu_support_features.md:341
#: ../../../platforms/ascend_npu_support_features.md:342
#: ../../../platforms/ascend_npu_support_features.md:343
#: ../../../platforms/ascend_npu_support_features.md:344
#: ../../../platforms/ascend_npu_support_features.md:350
#: ../../../platforms/ascend_npu_support_features.md:356
#: ../../../platforms/ascend_npu_support_features.md:357
#: ../../../platforms/ascend_npu_support_features.md:369
#: ../../../platforms/ascend_npu_support_features.md:370
#: ../../../platforms/ascend_npu_support_features.md:371
#: ../../../platforms/ascend_npu_support_features.md:372
#: ../../../platforms/ascend_npu_support_features.md:373
#: ../../../platforms/ascend_npu_support_features.md:381
#: ../../../platforms/ascend_npu_support_features.md:383
#: ../../../platforms/ascend_npu_support_features.md:384
#: ../../../platforms/ascend_npu_support_features.md:385
#: ../../../platforms/ascend_npu_support_features.md:391
#: ../../../platforms/ascend_npu_support_features.md:392
#: ../../../platforms/ascend_npu_support_features.md:393
#: ../../../platforms/ascend_npu_support_features.md:399
msgid "<span style=\"color: red;\">"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--model-impl`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Which implementation of the model to use. * “auto” will try to use the "
"SGLang implementation if it exists and fall back to the Transformers "
"implementation if no SGLang implementation is available. * “sglang” will use "
"the SGLang model implementation. * “transformers” will use the Transformers "
"model implementation.* \"mindspore\" will use the MindSpore model "
"implementation."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "'auto', 'sglang', 'transformers'"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--model-loader-extra-config`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Extra config for model loader. This will be passed to the model loader "
"corresponding to the chosen load_format."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "{}"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Type: str'"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:23
msgid "HTTP server"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--host`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The host of the HTTP server."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`127.0.0.1`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--port`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The port of the HTTP server."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`30000`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--skip-server-warmup`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "If set, skip warmup."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--warmups`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Specify custom warmup functions (csv) to run before server starts eg. --"
"warmups=warmup_name1,warmup_name2 will run the functions `warmup_name1` and "
"`warmup_name2` specified in warmup.py before the server starts listening for "
"requests"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--nccl-port`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The port for NCCL distributed environment setup. Defaults to a random port."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:33
msgid "Quantization and data type"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--dtype`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Data type for model weights and activations. * \"auto\" will use FP16 "
"precision for FP32 and FP16 models, and BF16 precision for BF16 models. * "
"\"half\" for FP16. Recommended for AWQ quantization. * \"float16\" is the "
"same as \"half\". * \"bfloat16\" for a balance between precision and range."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`auto`, `float16`, `bfloat16`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--quantization`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The quantization method."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`modelslim`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--quantization-param-path`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Path to the JSON file containing the KV cache scaling factors. This should "
"generally be supplied, when KV cache dtype is FP8. Otherwise, KV cache "
"scaling factors default to 1.0, which may cause accuracy issues."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Type: Optional[str]"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--kv-cache-dtype`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Data type for kv cache storage. \"auto\" will use model data type. "
"\"fp8_e5m2\" and \"fp8_e4m3\" is supported for CUDA 11.8+."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:42
msgid "Memory and scheduling"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--mem-fraction-static`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The fraction of the memory used for static allocation (model weights and KV "
"cache memory pool). Use a smaller value if you see out-of-memory errors."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Type: float"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--max-running-requests`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The maximum number of running requests."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--prefill-max-requests`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The maximum number of requests in a prefill batch. If not specified, there "
"is no limit.."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--max-queued-requests`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The maximum number of queued requests. This option is ignored when using "
"disaggregation-mode."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--max-total-tokens`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The maximum number of tokens in the memory pool. If not specified, it will "
"be automatically calculated based on the memory usage fraction. This option "
"is typically used for development and debugging purposes."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--chunked-prefill-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The maximum number of tokens in a chunk for the chunked prefill. Setting "
"this to -1 means disabling chunked prefill."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--max-prefill-tokens`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The maximum number of tokens in a prefill batch. The real bound will be the "
"maximum of this value and the model's maximum context length."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`16384`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--schedule-policy`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The scheduling policy of the requests."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`fcfs`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`lpm`, `fcfs`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-priority-scheduling`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Enable priority scheduling. Requests with higher priority integer values "
"will be scheduled first by default."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--schedule-low-priority-values-first`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"If specified with --enable-priority-scheduling, the scheduler will schedule "
"requests with lower priority integer values first."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--priority-scheduling-preemption-threshold`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Minimum difference in priorities for an incoming request to have to preempt "
"running request(s)."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`10`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--schedule-conservativeness`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"How conservative the schedule policy is. A larger value means more "
"conservative scheduling. Use a larger value if you see requests being "
"retracted frequently."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`1.0`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--page-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The number of tokens in a page, auto set 128 to Ascend NPU."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`128`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--hybrid-kvcache-ratio`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Mix ratio in [0,1] between uniform and hybrid kv buffers (0.0 = pure "
"uniform: swa_size / full_size = 1)(1.0 = pure hybrid: swa_size / full_size = "
"local_attention_size / context_length)"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Optional[float]"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--swa-full-tokens-ratio`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The ratio of SWA layer KV tokens / full layer KV tokens, regardless of the "
"number of swa:full layers. It should be between 0 and 1. E.g. 0.5 means if "
"each swa layer has 50 tokens, then each full layer has 100 tokens."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`0.8`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disable-hybrid-swa-memory`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Disable the hybrid SWA memory."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:63
msgid "Runtime options"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--device`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The device to use ('cuda', 'xpu', 'hpu', 'npu', 'cpu'). Defaults to auto-"
"detection if not specified."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--tensor-parallel-size`<br>`--tp-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The tensor parallelism size."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--pipeline-parallel-size`<br>`--pp-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The pipeline parallelism size."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--pp-max-micro-batch-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The maximum micro batch size in pipeline parallelism."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--stream-interval`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The interval (or buffer size) for streaming in terms of the token length. A "
"smaller value makes streaming smoother, while a larger value makes the "
"throughput higher"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--stream-output`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Whether to output as a sequence of disjoint segments."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--random-seed`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The random seed."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--constrained-json-whitespace-pattern`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"(outlines and llguidance backends only) Regex pattern for syntactic "
"whitespaces allowed in JSON constrained output. For example, to allow the "
"model to generate consecutive whitespaces, set the pattern to [\\n\\t ]*"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--constrained-json-disable-any-whitespace`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"(xgrammar and llguidance backends only) Enforce compact representation in "
"JSON constrained output."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--watchdog-timeout`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Set watchdog timeout in seconds. If a forward batch takes longer than this, "
"the server will crash to prevent hanging."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`300`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--soft-watchdog-timeout`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Set soft watchdog timeout in seconds. If a forward batch takes longer than "
"this, the server will dump information for debugging."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--dist-timeout`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Set timeout for torch.distributed initialization."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--base-gpu-id`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The base GPU ID to start allocating GPUs from. Useful when running multiple "
"instances on the same machine."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`0`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--gpu-id-step`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The delta between consecutive GPU IDs that are used. For example, setting it "
"to 2 will use GPU 0,2,4,..."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--sleep-on-idle`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Reduce CPU usage when sglang is idle."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--mm-process-config`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"A JSON string for multimodal preprocessing configuration. It can contain "
"keys: `image`, `video`, `audio`."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`{}`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:84
msgid "Logging"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--log-level`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The logging level of all loggers."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`info`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--log-level-http`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The logging level of HTTP server. If not set, reuse --log-level by default."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--log-requests`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Log metadata, inputs, outputs of all requests. The verbosity is decided by --"
"log-requests-level"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--log-requests-level`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"0: Log metadata (no sampling parameters). 1: Log metadata and sampling "
"parameters. 2: Log metadata, sampling parameters and partial input/output. "
"3: Log every input/output."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`2`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`0`, `1`, `2`, `3`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--crash-dump-folder`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Folder path to dump requests from the last 5 min before a crash (if any). If "
"not specified, crash dumping is disabled."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--crash-on-nan`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Crash the server on nan logprobs."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-metrics`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable log prometheus metrics."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-metrics-for-all-schedulers`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Enable --enable-metrics-for-all-schedulers when you want schedulers on all "
"TP ranks (not just TP 0) to record request metrics separately. This is "
"especially useful when dp_attention is enabled, as otherwise all metrics "
"appear to come from TP 0."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--tokenizer-metrics-custom-labels-header`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Specify the HTTP header for passing custom labels for tokenizer metrics."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`x-custom-labels`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--tokenizer-metrics-allowed-custom-labels`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The custom labels allowed for tokenizer metrics. The labels are specified "
"via a dict in '--tokenizer-metrics-custom-labels-header' field in HTTP "
"requests, e.g., {'label1': 'value1', 'label2': 'value2'} is allowed if '--"
"tokenizer-metrics-allowed-custom-labels label1 label2' is set."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "List[str]"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--bucket-time-to-first-token`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The buckets of time to first token, specified as a list of floats."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "List[float]"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--bucket-inter-token-latency`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The buckets of inter-token latency, specified as a list of floats."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--bucket-e2e-request-latency`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The buckets of end-to-end request latency, specified as a list of floats."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--collect-tokens-histogram`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Collect prompt/generation tokens histogram."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--prompt-tokens-buckets`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The buckets rule of prompt tokens. Supports 3 rule types: 'default' uses "
"predefined buckets; 'tse <middle> <base> <count>' generates two sides "
"exponential distributed buckets (e.g., 'tse 1000 2 8' generates buckets "
"[984.0, 992.0, 996.0, 998.0, 1000.0, 1002.0, 1004.0, 1008.0, 1016.0]).); "
"'custom <value1> <value2> ...' uses custom bucket values (e.g., 'custom 10 "
"50 100 500')."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:102
#: ../../../platforms/ascend_npu_support_features.md:103
msgid "<middle>"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:102
#: ../../../platforms/ascend_npu_support_features.md:103
msgid "<base>"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:102
#: ../../../platforms/ascend_npu_support_features.md:103
msgid "<count>"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:102
#: ../../../platforms/ascend_npu_support_features.md:103
msgid "<value1>"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:102
#: ../../../platforms/ascend_npu_support_features.md:103
msgid "<value2>"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--generation-tokens-buckets`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The buckets rule for generation tokens histogram. Supports 3 rule types: "
"'default' uses predefined buckets; 'tse <middle> <base> <count>' generates "
"two sides exponential distributed buckets (e.g., 'tse 1000 2 8' generates "
"buckets [984.0, 992.0, 996.0, 998.0, 1000.0, 1002.0, 1004.0, 1008.0, "
"1016.0]).); 'custom <value1> <value2> ...' uses custom bucket values (e.g., "
"'custom 10 50 100 500')."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--gc-warning-threshold-secs`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The threshold for long GC warning. If a GC takes longer than this, a warning "
"will be logged. Set to 0 to disable."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`0.0`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--decode-log-interval`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The log interval of decode batch."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`40`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-request-time-stats-logging`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable per request time stats logging"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--kv-events-config`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Config in json format for NVIDIA dynamo KV event publishing. Publishing will "
"be enabled if this flag is used."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-trace`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable opentelemetry trace"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--oltp-traces-endpoint`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Config opentelemetry collector endpoint if --enable-trace is set. format: "
"<ip>:<port>"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:109
msgid "<ip>"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:109
msgid "<port>"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`localhost:4317`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:111
msgid "API related"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--api-key`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Set API key of the server. It is also used in the OpenAI API compatible "
"server."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--served-model-name`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Override the model name returned by the v1/models endpoint in OpenAI API "
"server."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--weight-version`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Version identifier for the model weights. Defaults to 'default' if not "
"specified."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`default`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--chat-template`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The buliltin chat template name or the path of the chat template file. This "
"is only used for OpenAI-compatible API server."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--completion-template`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The buliltin completion template name or the path of the completion template "
"file. This is only used for OpenAI-compatible API server. only for code "
"completion currently."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-cache-report`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Return number of cached tokens in usage.prompt_tokens_details for each "
"openai request."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`True`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--reasoning-parser`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Specify the parser for reasoning models. Supported parsers: [deepseek-r1, "
"deepseek-v3, glm45, gpt-oss, kimi, qwen3, qwen3-thinking, step3]."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"`deepseek-r1`, `deepseek-v3`, `glm45`, `gpt-oss`, `kimi`, `qwen3`, `qwen3-"
"thinking`, `step3`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--tool-call-parser`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Specify the parser for handling tool-call interactions. Supported parsers: "
"[ llama3,qwen]."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`llama3`,`qwen`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--sampling-defaults`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Where to get default sampling parameters. 'openai' uses SGLang/OpenAI "
"defaults (temperature=1.0, top_p=1.0, etc.). 'model' uses the model's "
"generation_config.json to get the recommended sampling parameters if "
"available. Default is 'model'."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`model`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`openai`, `model`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--tool-server`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Either 'demo' or a comma-separated list of tool server urls to use for the "
"model. If not specified, no tool server will be used."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:126
msgid "Data parallelism"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--data-parallel-size`<br>`--dp-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The data parallelism size."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--load-balance-method`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The load balancing strategy for data parallelism. The Minimum Token "
"algorithm can only be used when DP attention is applied. This algorithm "
"performs load balancing based on the real-time token load of the DP workers."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`round_robin`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`round_robin`, `shortest_queue`, `minimum_tokens`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--prefill-round-robin-balance`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Prefill is round robin balanced. This is used to promise decode server can "
"get the correct dp rank."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:134
msgid "Multi-node distributed serving"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--dist-init-addr`<br>`--nccl-init-addr`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The host address for initializing distributed backend (e.g., "
"`192.168.0.2:25000`)."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--nnodes`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The number of nodes."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--node-rank`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The node rank."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:142
msgid "Model override args"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--json-model-override-args`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"A dictionary in JSON string format used to override default model "
"configurations."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--preferred-sampling-params`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"json-formatted sampling settings that will be returned in /get_model_info"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:149
msgid "LoRA"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-lora`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Enable LoRA support for the model. This argument is automatically set to "
"`True` if `--lora-paths` is provided for backward compatibility."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Bool flag (set to enable)"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--max-lora-rank`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The maximum LoRA rank that should be supported. If not specified, it will be "
"automatically inferred from the adapters provided in `--lora-paths`. This "
"argument is needed when you expect to dynamically load adapters of larger "
"LoRA rank after server startup."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--lora-target-modules`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The union set of all target modules where LoRA should be applied (e.g., "
"`q_proj`, `k_proj`, `gate_proj`). If not specified, it will be automatically "
"inferred from the adapters provided in `--lora-paths`. You can also set it "
"to `all` to enable LoRA for all supported modules; note this may introduce "
"minor performance overhead."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"`q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`, "
"`qkv_proj`, `gate_up_proj`, `all`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--lora-paths`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The list of LoRA adapters to load. Each adapter must be specified in one of "
"the following formats: `<PATH>` | `<NAME>=<PATH>` | JSON with schema "
"`{\"lora_name\": str, \"lora_path\": str, \"pinned\": bool}`."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Type: List[str] / JSON objects"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--max-loras-per-batch`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Maximum number of adapters for a running batch, including base-only requests."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`8`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--max-loaded-loras`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"If specified, limits the maximum number of LoRA adapters loaded in CPU "
"memory at a time. Must be ≥ `--max-loras-per-batch`."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--lora-eviction-policy`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "LoRA adapter eviction policy when the GPU memory pool is full."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`lru`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`lru`, `fifo`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--lora-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Choose the kernel backend for multi-LoRA serving."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`triton`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`triton`, `csgmv`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--max-lora-chunk-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Maximum chunk size for the ChunkedSGMV LoRA backend. Only used when `--lora-"
"backend` is `csgmv`. Larger values may improve performance."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`16`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`16`, `32`, `64`, `128`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:163
msgid "Kernel Backends (Attention, Sampling, Grammar, GEMM)"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--attention-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Choose the kernels for attention layers."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`ascend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--prefill-attention-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Choose the kernels for prefill attention layers (have priority over --"
"attention-backend)."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--decode-attention-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Choose the kernels for decode attention layers (have priority over --"
"attention-backend)."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--sampling-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Choose the kernels for sampling layers."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`pytorch`,`ascend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--grammar-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Choose the backend for grammar-guided decoding."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`xgrammar`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--mm-attention-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Set multimodal attention backend."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`ascend_attn`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--nsa-prefill-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Choose the NSA backend for the prefill stage (overrides `--attention-"
"backend` when running DeepSeek NSA-style attention)."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`flashmla_sparse`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`flashmla_sparse`, `flashmla_decode`, `fa3`, `tilelang`, `aiter`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--nsa-decode-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Choose the NSA backend for the decode stage when running DeepSeek NSA-style "
"attention. Overrides `--attention-backend` for decoding."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`flashmla_kv`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`flashmla_prefill`, `flashmla_kv`, `fa3`, `tilelang`, `aiter`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--fp8-gemm-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Choose the runner backend for Blockwise FP8 GEMM operations. Options: "
"'auto' (default, auto-selects based on hardware), 'deep_gemm' (JIT-compiled; "
"enabled by default on NVIDIA Hopper (SM90) and Blackwell (SM100) when "
"DeepGEMM is installed), 'flashinfer_trtllm' (optimal for Blackwell and low-"
"latency), 'cutlass' (optimal for Hopper/Blackwell GPUs and high-throughput), "
"'triton' (fallback, widely compatible), 'aiter' (ROCm only). **NOTE**: This "
"replaces the deprecated environment variables "
"SGLANG_ENABLE_FLASHINFER_FP8_GEMM and SGLANG_SUPPORT_CUTLASS_BLOCK_FP8."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`auto`, `deep_gemm`, `flashinfer_trtllm`, `cutlass`, `triton`, `aiter`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disable-flashinfer-autotune`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Flashinfer autotune is enabled by default. Set this flag to disable the "
"autotune."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:178
msgid "Speculative decoding"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-algorithm`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Speculative algorithm."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`EAGLE`, `EAGLE3`, `NEXTN`, `STANDALONE`, `NGRAM`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-draft-model-path`<br>`--speculative-draft-model`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The path of the draft model weights. This can be a local folder or a Hugging "
"Face repo ID."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-draft-model-revision`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The specific draft model version to use. It can be a branch name, a tag "
"name, or a commit id. If unspecified, will use the default version."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-num-steps`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The number of steps sampled from draft model in Speculative Decoding."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-eagle-topk`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The number of tokens sampled from the draft model in eagle2 each step."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-num-draft-tokens`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The number of tokens sampled from the draft model in Speculative Decoding."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-accept-threshold-single`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Accept a draft token if its probability in the target model is greater than "
"this threshold."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-accept-threshold-acc`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The accept probability of a draft token is raised from its target "
"probability p to min(1, p / threshold_acc)."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-token-map`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The path of the draft model's small vocab table."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-attention-mode`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Attention backend for speculative decoding operations (both target verify "
"and draft extend). Can be one of 'prefill' (default) or 'decode'."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`prefill`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`prefill`, `decode`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-moe-runner-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"MOE backend for EAGLE speculative decoding, see --moe-runner-backend for "
"options. Same as moe runner backend if unset."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-moe-a2a-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"MOE A2A backend for EAGLE speculative decoding, see --moe-a2a-backend for "
"options. Same as moe a2a backend if unset."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:195
msgid "Ngram speculative decoding"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-ngram-min-match-window-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The minimum window size for pattern matching in ngram speculative decoding."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-ngram-max-match-window-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The maximum window size for pattern matching in ngram speculative decoding."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`12`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-ngram-min-bfs-breadth`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The minimum breadth for BFS (Breadth-First Search) in ngram speculative "
"decoding."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-ngram-max-bfs-breadth`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The maximum breadth for BFS (Breadth-First Search) in ngram speculative "
"decoding."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-ngram-match-type`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The match type for cache tree."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`BFS`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`BFS`, `PROB`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-ngram-branch-length`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The branch length for ngram speculative decoding."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`18`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--speculative-ngram-capacity`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The cache capacity for ngram speculative decoding."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`10000000`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:207
msgid "Expert parallelism"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--expert-parallel-size`<br>`--ep-size`<br>`--ep`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The expert parallelism size. Default equal to tp size."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`tp-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--moe-a2a-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Select the backend for all-to-all communication for expert parallelism."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`none`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`none`, `deepep`, `ascend_fuseep`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--moe-runner-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Choose the runner backend for MoE."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--flashinfer-mxfp4-moe-precision`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Choose the computation precision of flashinfer mxfp4 moe"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`default`, `bf16`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-flashinfer-allreduce-fusion`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable FlashInfer allreduce fusion with Residual RMSNorm."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--deepep-mode`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Select the mode when enable DeepEP MoE, could be `normal`, `low_latency` or "
"`auto`. Default is `auto`, which means `low_latency` for decode batch and "
"`normal` for prefill batch."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`normal`, `low_latency`, `auto`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--ep-num-redundant-experts`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Allocate this number of redundant experts in expert parallel."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--ep-dispatch-algorithm`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The algorithm to choose ranks for redundant experts in expert parallel."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--init-expert-location`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Initial location of EP experts."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`trivial`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-eplb`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable EPLB algorithm"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--eplb-algorithm`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Chosen EPLB algorithm"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--eplb-rebalance-layers-per-chunk`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Number of layers to rebalance per forward pass."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--eplb-min-rebalancing-utilization-threshold`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Minimum threshold for GPU average utilization to trigger EPLB rebalancing. "
"Must be in the range [0.0, 1.0]."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--expert-distribution-recorder-mode`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Mode of expert distribution recorder."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--expert-distribution-recorder-buffer-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Circular buffer size of expert distribution recorder. Set to -1 to denote "
"infinite buffer."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-expert-distribution-metrics`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable logging metrics for expert balancedness"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--deepep-config`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Tuned DeepEP config suitable for your own cluster. It can be either a string "
"with JSON content or a file path."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--moe-dense-tp-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"TP size for MoE dense MLP layers. This flag is useful when, with large TP "
"size, there are errors caused by weights in MLP layers having dimension "
"smaller than the min dimension GEMM supports."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--elastic-ep-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Select the collective communication backend for elastic EP. Currently "
"supports 'mooncake'."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "None"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "N/A"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--mooncake-ib-device`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The InfiniBand devices for Mooncake Backend, accepts multiple comma-"
"separated devices. Default is None, which triggers automatic device "
"detection when Mooncake Backend is enabled."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:232
msgid "Mamba Cache"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--max-mamba-cache-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The maximum size of the mamba cache."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--mamba-ssm-dtype`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The data type of the SSM states in mamba cache."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`float32`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`float32`, `bfloat16`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--mamba-full-memory-ratio`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The ratio of mamba state memory to full kv cache memory."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`0.2`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--mamba-scheduler-strategy`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The strategy to use for mamba scheduler. `auto` currently defaults to "
"`no_buffer`. 1. `no_buffer` does not support overlap scheduler due to not "
"allocating extra mamba state buffers. Branching point caching support is "
"feasible but not implemented. 2. `extra_buffer` supports overlap schedule by "
"allocating extra mamba state buffers to track mamba state for caching (mamba "
"state usage per running req becomes `2x` for non-spec; `1+(1/"
"(2+speculative_num_draft_tokens))x` for spec dec (e.g. 1.16x if "
"speculative_num_draft_tokens==4)). 2a. `extra_buffer` is strictly better for "
"non-KV-cache-bound cases; for KV-cache-bound cases, the tradeoff depends on "
"whether enabling overlap outweighs reduced max running requests. 2b. mamba "
"caching at radix cache branching point is strictly better than non-branch "
"but requires kernel support (currently only FLA backend), currently only "
"extra_buffer supports branching."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`auto`, `no_buffer`, `extra_buffer`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--mamba-track-interval`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The interval (in tokens) to track the mamba state during decode. Only used "
"when `--mamba-scheduler-strategy` is `extra_buffer`. Must be divisible by "
"page_size if set, and must be >= speculative_num_draft_tokens when using "
"speculative decoding."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`256`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:242
msgid "Args for multi-item scoring"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--multi-item-scoring-delimiter`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Delimiter token ID for multi-item scoring. Used to combine Query and Items "
"into a single sequence: Query<delimiter>Item1<delimiter>Item2<delimiter>... "
"This enables efficient batch processing of multiple items against a single "
"query."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:246
msgid "<delimiter>"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:248
msgid "Hierarchical cache"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-hierarchical-cache`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable hierarchical cache"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--hicache-ratio`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The ratio of the size of host KV cache memory pool to the size of device "
"pool."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`2.0`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--hicache-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The size of host KV cache memory pool in gigabytes, which will override the "
"hicache_ratio if set."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--hicache-write-policy`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The write policy of hierarchical cache."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`write_through`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`write_back`, `write_through`, `write_through_selective`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--radix-eviction-policy`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The eviction policy of radix trees. 'lru' stands for Least Recently Used, "
"'lfu' stands for Least Frequently Used."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`lru`, `lfu`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--hicache-io-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The IO backend for KV cache transfer between CPU and GPU"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`kernel`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`kernel_ascend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--hicache-mem-layout`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The layout of host memory pool for hierarchical cache."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`layer_first`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`page_first_direct`, `page_first_kv_split`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--hicache-storage-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The storage backend for hierarchical KV cache. Built-in backends: file, "
"mooncake, hf3fs, nixl, aibrix. For dynamic backend, use --hicache-storage-"
"backend-extra-config to specify: backend_name (custom name), module_path "
"(Python module path), class_name (backend class name)."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`file`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--hicache-storage-prefetch-policy`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Control when prefetching from the storage backend should stop."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`best_effort`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`best_effort`, `wait_complete`, `timeout`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--hicache-storage-backend-extra-config`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"A dictionary in JSON string format containing extra configuration for the "
"storage backend."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:263
msgid "LMCache"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-lmcache`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Using LMCache as an alternative hierarchical cache solution"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:269
msgid "Double Sparsity"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-double-sparsity`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable double sparsity attention"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--ds-channel-config-path`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The path of the double sparsity channel config"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--ds-heavy-channel-num`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The number of heavy channels in double sparsity attention"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`32`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--ds-heavy-token-num`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The number of heavy tokens in double sparsity attention"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--ds-heavy-channel-type`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The type of heavy channels in double sparsity attention"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`qk`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--ds-sparse-decode-threshold`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The minimum decode sequence length required before the double-sparsity "
"backend switches from the dense fallback to the sparse decode kernel."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`4096`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:280
msgid "Offloading"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--cpu-offload-gb`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "How many GBs of RAM to reserve for CPU offloading."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--offload-group-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Number of layers per group in offloading."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`-1`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--offload-num-in-group`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Number of layers to be offloaded within a group."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--offload-prefetch-step`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Steps to prefetch in offloading."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--offload-mode`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Mode of offloading."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`cpu`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:290
msgid "Optimization/debug options"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disable-radix-cache`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Disable RadixAttention for prefix caching."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--cuda-graph-max-bs`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Set the maximum batch size for cuda graph. It will extend the cuda graph "
"capture batch size to this value."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--cuda-graph-bs`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Set the list of batch sizes for cuda graph."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "List[int]"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disable-cuda-graph`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Disable cuda graph."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disable-cuda-graph-padding`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Disable cuda graph when padding is needed. Still uses cuda graph when "
"padding is not needed."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-profile-cuda-graph`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable profiling of cuda graph capture."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-cudagraph-gc`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Enable garbage collection during CUDA graph capture. If disabled (default), "
"GC is frozen during capture to speed up the process."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-nccl-nvls`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable NCCL NVLS for prefill heavy requests when available."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-symm-mem`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable NCCL symmetric memory for fast collectives."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disable-flashinfer-cutlass-moe-fp4-allgather`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Disables quantize before all-gather for flashinfer cutlass moe."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-tokenizer-batch-encode`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Enable batch tokenization for improved performance when processing multiple "
"text inputs. Do not use with image inputs, pre-tokenized input_ids, or "
"input_embeds."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disable-outlines-disk-cache`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Disable disk cache of outlines to avoid possible crashes related to file "
"system or high concurrency."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disable-custom-all-reduce`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Disable the custom all-reduce kernel and fall back to NCCL."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-mscclpp`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Enable using mscclpp for small messages for all-reduce kernel and fall back "
"to NCCL."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-torch-symm-mem`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Enable using torch symm mem for all-reduce kernel and fall back to NCCL. "
"Only supports CUDA device SM90 and above. SM90 supports world size 4, 6, 8. "
"SM10 supports world size 6, 8."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disable-overlap-schedule`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Disable the overlap scheduler, which overlaps the CPU scheduler with GPU "
"model worker."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-mixed-chunk`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Enabling mixing prefill and decode in a batch when using chunked prefill."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-dp-attention`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Enabling data parallelism for attention and tensor parallelism for FFN. The "
"dp size should be equal to the tp size. Currently DeepSeek-V2 and Qwen 2/3 "
"MoE models are supported."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-dp-lm-head`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Enable vocabulary parallel across the attention TP group to avoid all-gather "
"across DP groups, optimizing performance under DP attention."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-two-batch-overlap`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enabling two micro batches to overlap."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-single-batch-overlap`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Let computation and communication overlap within one micro batch."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--tbo-token-distribution-threshold`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The threshold of token distribution between two batches in micro-batch-"
"overlap, determines whether to two-batch-overlap or two-chunk-overlap. Set "
"to 0 denote disable two-chunk-overlap."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`0.48`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-torch-compile`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Optimize the model with torch.compile. Experimental feature."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-torch-compile-debug-mode`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable debug mode for torch compile."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-piecewise-cuda-graph`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Optimize the model with piecewise cuda graph for extend/prefill only. "
"Experimental feature."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--piecewise-cuda-graph-tokens`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Set the list of tokens when using piecewise cuda graph."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Type: JSON list"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--torch-compile-max-bs`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Set the maximum batch size when using torch compile."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--piecewise-cuda-graph-max-tokens`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Set the maximum tokens when using piecewise cuda graph."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--torchao-config`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Optimize the model with torchao. Experimental feature. Current choices are: "
"int8dq, int8wo, int4wo-<group_size>, fp8wo, fp8dq-per_tensor, fp8dq-per_row"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "``"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-nan-detection`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable the NaN detection for debugging purposes."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-p2p-check`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Enable P2P check for GPU access, otherwise the p2p access is allowed by "
"default."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--triton-attention-reduce-in-fp32`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Cast the intermediate attention results to fp32 to avoid possible crashes "
"related to fp16. This only affects Triton attention kernels."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--triton-attention-num-kv-splits`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The number of KV splits in flash decoding Triton kernel. Larger value is "
"better in longer context scenarios. The default value is 8."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--triton-attention-split-tile-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The size of split KV tile in flash decoding Triton kernel. Used for "
"deterministic inference."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--num-continuous-decode-steps`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Run multiple continuous decoding steps to reduce scheduling overhead. This "
"can potentially increase throughput but may also increase time-to-first-"
"token latency. The default value is 1, meaning only run one decoding step at "
"a time."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--delete-ckpt-after-loading`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Delete the model checkpoint after loading the model."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-memory-saver`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Allow saving memory using release_memory_occupation and "
"resume_memory_occupation"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-weights-cpu-backup`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Save model weights to CPU memory during release_weights_occupation and "
"resume_weights_occupation"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--allow-auto-truncate`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Allow automatically truncating requests that exceed the maximum input length "
"instead of returning an error."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-custom-logit-processor`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Enable users to pass custom logit processors to the server (disabled by "
"default for security)"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--flashinfer-mla-disable-ragged`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Not using ragged prefill wrapper when running flashinfer mla"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disable-shared-experts-fusion`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Disable shared experts fusion optimization for deepseek v3/r1."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disable-chunked-prefix-cache`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Disable chunked prefix cache feature for deepseek, which should save "
"overhead for short sequences."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disable-fast-image-processor`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Adopt base image processor instead of fast image processor."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--keep-mm-feature-on-device`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Keep multimodal feature tensors on device after processing to save D2H copy."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-return-hidden-states`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable returning hidden states with responses."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--scheduler-recv-interval`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The interval to poll requests in scheduler. Can be set to >1 to reduce the "
"overhead of this."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--numa-node`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Sets the numa node for the subprocesses. i-th element corresponds to i-th "
"subprocess."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-layerwise-nvtx-marker`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Enable layerwise NVTX profiling annotations for the model. This adds NVTX "
"markers to every layer for detailed per-layer performance analysis with "
"Nsight Systems."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-attn-tp-input-scattered`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Allow input of attention to be scattered when only using tensor parallelism, "
"to reduce the computational load of operations such as qkv latent."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-nsa-prefill-context-parallel`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Context parallelism used in the long sequence prefill phase of DeepSeek v3.2"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:346
msgid "Forward hooks"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--forward-hooks`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"JSON-formatted list of forward hook specifications. Each element must "
"include `target_modules` (list of glob patterns matched against `model."
"named_modules()` names) and `hook_factory` (Python import path to a factory, "
"e.g. `my_package.hooks:make_hook`). An optional `name` field is used for "
"logging, and an optional `config` object is passed as a `dict` to the "
"factory."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:352
msgid "Debug tensor dumps"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--debug-tensor-dump-input-file`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The input filename for dumping tensors"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--debug-tensor-dump-inject`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Inject the outputs from jax as the input of every layer."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-dynamic-batch-tokenizer`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Enable async dynamic batch tokenizer for improved performance when multiple "
"requests arrive concurrently."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--dynamic-batch-tokenizer-batch-size`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"[Only used if --enable-dynamic-batch-tokenizer is set] Maximum batch size "
"for dynamic batch tokenizer."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--dynamic-batch-tokenizer-batch-timeout`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"[Only used if --enable-dynamic-batch-tokenizer is set] Timeout in seconds "
"for batching tokenization requests."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`0.002`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:362
msgid "PD disaggregation"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disaggregation-mode`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Only used for PD disaggregation. \"prefill\" for prefill-only server, and "
"\"decode\" for decode-only server. If not specified, it is not PD "
"disaggregated"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`null`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`null`, `prefill`, `decode`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disaggregation-transfer-backend`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The backend for disaggregation transfer. Default is mooncake."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`mooncake`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`mooncake`, `nixl`, `ascend`, `fake`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disaggregation-bootstrap-port`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Bootstrap server port on the prefill server. Default is 8998."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`8998`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disaggregation-decode-tp`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Decode tp size. If not set, it matches the tp size of the current engine. "
"This is only set on the prefill server."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disaggregation-decode-dp`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Decode dp size. If not set, it matches the dp size of the current engine. "
"This is only set on the prefill server."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disaggregation-prefill-pp`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Prefill pp size. If not set, it is default to 1. This is only set on the "
"decode server."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disaggregation-ib-device`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The InfiniBand devices for disaggregation transfer, accepts single device (e."
"g., --disaggregation-ib-device mlx5_0) or multiple comma-separated devices "
"(e.g., --disaggregation-ib-device mlx5_0,mlx5_1). Default is None, which "
"triggers automatic device detection when mooncake backend is enabled."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disaggregation-decode-enable-offload-kvcache`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable async KV cache offloading on decode server (PD mode)."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--num-reserved-decode-tokens`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"Number of decode tokens that will have memory reserved when adding new "
"request to the running batch."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`512`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--disaggregation-decode-polling-interval`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The interval to poll requests in decode server. Can be set to >1 to reduce "
"the overhead of this."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:377
msgid "Custom weight loader"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--custom-weight-loader`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The custom dataloader which used to update the model. Should be set with a "
"valid import path, such as my_package.weight_load_func"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--weight-loader-disable-mmap`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Disable mmap while loading weight using safetensors."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--remote-instance-weight-loader-seed-instance-ip`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The ip of the seed instance for loading weights from remote instance."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--remote-instance-weight-loader-seed-instance-service-port`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid ""
"The service port of the seed instance for loading weights from remote "
"instance."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--remote-instance-weight-loader-send-weights-group-ports`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The communication group ports for loading weights from remote instance."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:387
msgid "For PD-Multiplexing"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-pdmux`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable PD-Multiplexing, PD running on greenctx stream."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--pdmux-config-path`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "The path of the PD-Multiplexing config file."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--sm-group-num`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Number of sm partition groups."
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:395
msgid "For deterministic inference"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "`--enable-deterministic-inference`"
msgstr ""

#: ../../../platforms/ascend_npu_support_features.md:0
msgid "Enable deterministic inference mode with batch invariant ops."
msgstr ""
